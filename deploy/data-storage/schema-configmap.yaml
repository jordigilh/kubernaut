apiVersion: v1
data:
  001_initial_schema.sql: |-
    -- Initial schema for Action History Storage
    -- Migration: 001_initial_schema.sql

    -- Enable required extensions
    CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

    -- 1. Resource References Table
    CREATE TABLE resource_references (
        id BIGSERIAL PRIMARY KEY,
        resource_uid VARCHAR(36) UNIQUE NOT NULL, -- Kubernetes UID
        api_version VARCHAR(100) NOT NULL,
        kind VARCHAR(100) NOT NULL,
        name VARCHAR(253) NOT NULL,
        namespace VARCHAR(63),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        deleted_at TIMESTAMP WITH TIME ZONE, -- For soft deletion tracking
        last_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

        -- Constraints
        UNIQUE(namespace, kind, name)
    );

    -- Indexes for resource_references
    CREATE INDEX idx_resource_kind ON resource_references (kind);
    CREATE INDEX idx_resource_namespace ON resource_references (namespace);
    CREATE INDEX idx_resource_last_seen ON resource_references (last_seen);
    CREATE INDEX idx_resource_uid ON resource_references (resource_uid);

    -- 2. Action Histories Table
    CREATE TABLE action_histories (
        id BIGSERIAL PRIMARY KEY,
        resource_id BIGINT NOT NULL REFERENCES resource_references(id) ON DELETE CASCADE,

        -- Retention configuration
        max_actions INTEGER DEFAULT 1000,
        max_age_days INTEGER DEFAULT 30,
        compaction_strategy VARCHAR(20) DEFAULT 'pattern-aware', -- oldest-first, effectiveness-based, pattern-aware

        -- Analysis configuration
        oscillation_window_minutes INTEGER DEFAULT 120,
        effectiveness_threshold DECIMAL(3,2) DEFAULT 0.70,
        pattern_min_occurrences INTEGER DEFAULT 3,

        -- Status tracking
        total_actions INTEGER DEFAULT 0,
        last_action_at TIMESTAMP WITH TIME ZONE,
        last_analysis_at TIMESTAMP WITH TIME ZONE,
        next_analysis_at TIMESTAMP WITH TIME ZONE,

        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

        -- Constraints
        UNIQUE(resource_id)
    );

    -- Indexes for action_histories
    CREATE INDEX idx_ah_last_action ON action_histories (last_action_at);
    CREATE INDEX idx_ah_next_analysis ON action_histories (next_analysis_at);
    CREATE INDEX idx_ah_resource_id ON action_histories (resource_id);

    -- 3. Resource Action Traces Table (will be partitioned)
    CREATE TABLE resource_action_traces (
        id BIGSERIAL,
        action_history_id BIGINT NOT NULL REFERENCES action_histories(id) ON DELETE CASCADE,

        -- Action identification
        action_id VARCHAR(64) NOT NULL, -- UUID for this specific action
        correlation_id VARCHAR(64), -- For tracing across systems

        -- Timing information
        action_timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
        execution_start_time TIMESTAMP WITH TIME ZONE,
        execution_end_time TIMESTAMP WITH TIME ZONE,
        execution_duration_ms INTEGER,

        -- Alert context
        alert_name VARCHAR(200) NOT NULL,
        alert_severity VARCHAR(20) NOT NULL, -- info, warning, critical
        alert_labels JSONB,
        alert_annotations JSONB,
        alert_firing_time TIMESTAMP WITH TIME ZONE,

        -- AI model information
        model_used VARCHAR(100) NOT NULL,
        routing_tier VARCHAR(20), -- route1, route2, route3
        model_confidence DECIMAL(4,3) NOT NULL, -- 0.000-1.000
        model_reasoning TEXT,
        alternative_actions JSONB, -- [{"action": "scale_deployment", "confidence": 0.85}]

        -- Action details
        action_type VARCHAR(50) NOT NULL,
        action_parameters JSONB, -- {"replicas": 5, "memory": "2Gi"}

        -- Resource state capture
        resource_state_before JSONB,
        resource_state_after JSONB,

        -- Execution tracking
        execution_status VARCHAR(20) DEFAULT 'pending', -- pending, executing, completed, failed, rolled-back
        execution_error TEXT,
        kubernetes_operations JSONB, -- [{"operation": "patch", "resource": "deployment/webapp", "result": "success"}]

        -- Effectiveness assessment
        effectiveness_score DECIMAL(4,3), -- 0.000-1.000, calculated after execution
        effectiveness_criteria JSONB, -- {"alert_resolved": true, "target_metric_improved": true}
        effectiveness_assessed_at TIMESTAMP WITH TIME ZONE,
        effectiveness_assessment_method VARCHAR(20), -- automated, manual, ml-derived
        effectiveness_notes TEXT,

        -- Follow-up tracking
        follow_up_actions JSONB, -- [{"action_id": "uuid", "relation": "correction"}]

        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

        -- Primary key includes timestamp for partitioning
        PRIMARY KEY (id, action_timestamp)
    ) PARTITION BY RANGE (action_timestamp);

    -- Indexes for resource_action_traces (will be inherited by partitions)
    CREATE INDEX idx_rat_action_history ON resource_action_traces (action_history_id);
    CREATE INDEX idx_rat_action_type ON resource_action_traces (action_type);
    CREATE INDEX idx_rat_model_used ON resource_action_traces (model_used);
    CREATE INDEX idx_rat_alert_name ON resource_action_traces (alert_name);
    CREATE INDEX idx_rat_execution_status ON resource_action_traces (execution_status);
    CREATE INDEX idx_rat_effectiveness_score ON resource_action_traces (effectiveness_score);
    CREATE INDEX idx_rat_correlation_id ON resource_action_traces (correlation_id);

    -- Composite indexes for common queries
    CREATE INDEX idx_rat_history_timestamp ON resource_action_traces (action_history_id, action_timestamp);
    CREATE INDEX idx_rat_type_timestamp ON resource_action_traces (action_type, action_timestamp);
    CREATE INDEX idx_rat_model_effectiveness ON resource_action_traces (model_used, effectiveness_score);

    -- GIN indexes for JSONB queries
    CREATE INDEX idx_rat_alert_labels_gin ON resource_action_traces USING GIN (alert_labels);
    CREATE INDEX idx_rat_action_parameters_gin ON resource_action_traces USING GIN (action_parameters);
    CREATE INDEX idx_rat_resource_state_gin ON resource_action_traces USING GIN (resource_state_before);

    -- Partial indexes for active data
    CREATE INDEX idx_rat_pending_actions ON resource_action_traces (action_timestamp)
        WHERE execution_status IN ('pending', 'executing');

    -- 4. Partitions for resource_action_traces are created by migration 002
    -- (002 drops and recreates this table with partitions starting March 2026)

    -- 5. Oscillation Patterns Table
    CREATE TABLE oscillation_patterns (
        id BIGSERIAL PRIMARY KEY,

        -- Pattern definition
        pattern_type VARCHAR(50) NOT NULL, -- scale-oscillation, resource-thrashing, ineffective-loop, cascading-failure
        pattern_name VARCHAR(200) NOT NULL,
        description TEXT,

        -- Detection criteria
        min_occurrences INTEGER NOT NULL DEFAULT 3,
        time_window_minutes INTEGER NOT NULL DEFAULT 120,
        action_sequence JSONB, -- ["scale_deployment", "scale_deployment", "scale_deployment"]
        threshold_config JSONB, -- {"confidence_drop": 0.2, "effectiveness_threshold": 0.3}

        -- Resource scope
        resource_types TEXT[], -- ["Deployment", "StatefulSet"]
        namespaces TEXT[], -- ["production", "staging"]
        label_selectors JSONB, -- {"app": "webapp", "tier": "frontend"}

        -- Prevention strategy
        prevention_strategy VARCHAR(50) NOT NULL, -- block-action, escalate-human, alternative-action, cooling-period
        prevention_parameters JSONB, -- {"cooling_period_minutes": 30, "escalation_webhook": "..."}

        -- Alerting configuration
        alerting_enabled BOOLEAN DEFAULT true,
        alert_severity VARCHAR(20) DEFAULT 'warning',
        alert_channels TEXT[], -- ["slack", "pagerduty"]

        -- Pattern statistics
        total_detections INTEGER DEFAULT 0,
        prevention_success_rate DECIMAL(4,3),
        false_positive_rate DECIMAL(4,3),
        last_detection_at TIMESTAMP WITH TIME ZONE,

        -- Lifecycle
        active BOOLEAN DEFAULT true,
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Indexes for oscillation_patterns
    CREATE INDEX idx_op_pattern_type ON oscillation_patterns (pattern_type);
    CREATE INDEX idx_op_active_patterns ON oscillation_patterns (active);
    CREATE INDEX idx_op_last_detection ON oscillation_patterns (last_detection_at);

    -- 6. Oscillation Detections Table
    CREATE TABLE oscillation_detections (
        id BIGSERIAL PRIMARY KEY,
        pattern_id BIGINT NOT NULL REFERENCES oscillation_patterns(id) ON DELETE CASCADE,
        resource_id BIGINT NOT NULL REFERENCES resource_references(id) ON DELETE CASCADE,

        detected_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
        confidence DECIMAL(4,3) NOT NULL, -- 0.000-1.000
        action_count INTEGER NOT NULL,
        time_span_minutes INTEGER NOT NULL,

        -- Pattern evidence
        matching_actions BIGINT[], -- Array of action_trace IDs that matched the pattern
        pattern_evidence JSONB, -- Detailed evidence for the detection

        -- Prevention outcome
        prevention_applied BOOLEAN DEFAULT false,
        prevention_action VARCHAR(50), -- blocked, escalated, alternative-suggested
        prevention_details JSONB,
        prevention_successful BOOLEAN,

        -- Resolution tracking
        resolved BOOLEAN DEFAULT false,
        resolved_at TIMESTAMP WITH TIME ZONE,
        resolution_method VARCHAR(50), -- timeout, manual-intervention, automatic
        resolution_notes TEXT,

        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Indexes for oscillation_detections
    CREATE INDEX idx_od_pattern_resource ON oscillation_detections (pattern_id, resource_id);
    CREATE INDEX idx_od_detected_at ON oscillation_detections (detected_at);
    CREATE INDEX idx_od_unresolved ON oscillation_detections (resolved) WHERE resolved = false;

    -- 7. Action Effectiveness Metrics Table
    CREATE TABLE action_effectiveness_metrics (
        id BIGSERIAL PRIMARY KEY,

        -- Scope definition
        scope_type VARCHAR(50) NOT NULL, -- global, namespace, resource-type, alert-type, model
        scope_value VARCHAR(200), -- specific value for the scope
        metric_period VARCHAR(20) NOT NULL, -- 1h, 24h, 7d, 30d

        -- Time range for this metric
        period_start TIMESTAMP WITH TIME ZONE NOT NULL,
        period_end TIMESTAMP WITH TIME ZONE NOT NULL,

        -- Effectiveness by action type
        action_type VARCHAR(50) NOT NULL,
        sample_size INTEGER NOT NULL,
        average_score DECIMAL(4,3) NOT NULL,
        median_score DECIMAL(4,3),
        std_deviation DECIMAL(4,3),
        confidence_interval_lower DECIMAL(4,3),
        confidence_interval_upper DECIMAL(4,3),

        -- Trend analysis
        trend_direction VARCHAR(20), -- improving, stable, declining
        trend_confidence DECIMAL(4,3),

        -- Statistical significance
        min_sample_size_met BOOLEAN,
        statistical_significance DECIMAL(4,3),

        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

        -- Ensure uniqueness and enable efficient queries
        UNIQUE(scope_type, scope_value, metric_period, period_start, action_type)
    );

    -- Indexes for action_effectiveness_metrics
    CREATE INDEX idx_aem_scope_period ON action_effectiveness_metrics (scope_type, scope_value, metric_period);
    CREATE INDEX idx_aem_period_range ON action_effectiveness_metrics (period_start, period_end);
    CREATE INDEX idx_aem_action_effectiveness ON action_effectiveness_metrics (action_type, average_score);

    -- 8. Retention Operations Table
    CREATE TABLE retention_operations (
        id BIGSERIAL PRIMARY KEY,
        action_history_id BIGINT NOT NULL REFERENCES action_histories(id) ON DELETE CASCADE,

        operation_type VARCHAR(30) NOT NULL, -- cleanup, archive, compact
        strategy_used VARCHAR(30) NOT NULL, -- oldest-first, effectiveness-based, pattern-aware

        -- Operation details
        records_before INTEGER NOT NULL,
        records_after INTEGER NOT NULL,
        records_deleted INTEGER NOT NULL,
        records_archived INTEGER,

        -- Criteria used
        retention_criteria JSONB, -- {"max_age_days": 30, "min_effectiveness": 0.1}
        preserved_criteria JSONB, -- {"pattern_examples": 5, "high_effectiveness": 10}

        operation_start TIMESTAMP WITH TIME ZONE NOT NULL,
        operation_end TIMESTAMP WITH TIME ZONE,
        operation_duration_ms INTEGER,
        operation_status VARCHAR(20) DEFAULT 'running', -- running, completed, failed
        error_message TEXT,

        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );

    -- Indexes for retention_operations
    CREATE INDEX idx_ro_action_history_ops ON retention_operations (action_history_id);
    CREATE INDEX idx_ro_operation_time ON retention_operations (operation_start);

    -- 9. Insert default oscillation patterns
    INSERT INTO oscillation_patterns (
        pattern_type, pattern_name, description, min_occurrences, time_window_minutes,
        threshold_config, prevention_strategy, prevention_parameters
    ) VALUES
    (
        'scale-oscillation',
        'Scale Up/Down Oscillation',
        'Rapid alternating scale up and scale down operations within a short time window',
        3,
        120,
        '{"min_direction_changes": 2, "max_time_between_actions": 30, "effectiveness_threshold": 0.5}',
        'cooling-period',
        '{"cooling_period_minutes": 30, "escalate_after": 3}'
    ),
    (
        'resource-thrashing',
        'Resource/Scale Thrashing',
        'Alternating between resource adjustments and scaling decisions',
        2,
        90,
        '{"action_types": ["increase_resources", "scale_deployment"], "effectiveness_threshold": 0.6}',
        'alternative-action',
        '{"suggest_alternatives": true, "block_conflicting": true}'
    ),
    (
        'ineffective-loop',
        'Ineffective Action Loop',
        'Repeated actions with consistently low effectiveness scores',
        4,
        180,
        '{"effectiveness_threshold": 0.3, "min_repetitions": 3}',
        'escalate-human',
        '{"escalation_webhook": null, "require_approval": true}'
    ),
    (
        'cascading-failure',
        'Cascading Failure Pattern',
        'Actions that trigger more alerts than they resolve',
        2,
        60,
        '{"new_alerts_threshold": 1.5, "recurrence_rate_threshold": 0.4}',
        'block-action',
        '{"block_duration_minutes": 60, "require_manual_override": true}'
    );

    -- 10. Create function for automatic partition creation
    CREATE OR REPLACE FUNCTION create_monthly_partitions()
    RETURNS void AS $$
    DECLARE
        start_date date;
        end_date date;
        table_name text;
    BEGIN
        -- Create partition for next month
        start_date := date_trunc('month', CURRENT_DATE + interval '1 month');
        end_date := start_date + interval '1 month';
        table_name := 'resource_action_traces_' ||
                      to_char(start_date, 'YYYY') || '_' ||
                      to_char(start_date, 'MM');

        EXECUTE format('CREATE TABLE IF NOT EXISTS %I
                       PARTITION OF resource_action_traces
                       FOR VALUES FROM (%L) TO (%L)',
                       table_name, start_date, end_date);

        RAISE NOTICE 'Created partition: %', table_name;
    END;
    $$ LANGUAGE plpgsql;

    -- 11. Create trigger function to update updated_at columns
    CREATE OR REPLACE FUNCTION update_updated_at()
    RETURNS TRIGGER AS $$
    BEGIN
        NEW.updated_at = NOW();
        RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;

    -- Create triggers for updated_at
    CREATE TRIGGER update_action_histories_updated_at
        BEFORE UPDATE ON action_histories
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at();

    CREATE TRIGGER update_resource_action_traces_updated_at
        BEFORE UPDATE ON resource_action_traces
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at();

    CREATE TRIGGER update_oscillation_patterns_updated_at
        BEFORE UPDATE ON oscillation_patterns
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at();

    -- 12. Create views for common queries
    CREATE VIEW action_history_summary AS
    SELECT
        rr.namespace,
        rr.kind,
        rr.name,
        ah.total_actions,
        ah.last_action_at,
        COUNT(rat.id) as recent_actions_24h,
        AVG(rat.effectiveness_score) as avg_effectiveness_24h,
        COUNT(DISTINCT rat.action_type) as action_types_used
    FROM resource_references rr
    JOIN action_histories ah ON ah.resource_id = rr.id
    LEFT JOIN resource_action_traces rat ON rat.action_history_id = ah.id
        AND rat.action_timestamp > NOW() - INTERVAL '24 hours'
    GROUP BY rr.id, rr.namespace, rr.kind, rr.name, ah.total_actions, ah.last_action_at;

    -- Summary statistics
    CREATE VIEW oscillation_detection_summary AS
    SELECT
        pattern_type,
        COUNT(*) as total_detections,
        COUNT(*) FILTER (WHERE prevention_applied = true) as preventions_applied,
        COUNT(*) FILTER (WHERE prevention_successful = true) as successful_preventions,
        AVG(confidence) as avg_confidence,
        MAX(detected_at) as last_detection
    FROM oscillation_detections od
    JOIN oscillation_patterns op ON od.pattern_id = op.id
    GROUP BY pattern_type;

    -- Grant permissions (for development)
    GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO slm_user;
    GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO slm_user;
    GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO slm_user;
  002_fix_partitioning.sql: |-
    -- Fix partitioning issues in resource_action_traces table
    -- Migration: 002_fix_partitioning.sql

    -- Drop the problematic partitioned table and recreate it properly
    DROP TABLE IF EXISTS resource_action_traces CASCADE;

    -- 3. Resource Action Traces Table (partitioned by timestamp)
    CREATE TABLE resource_action_traces (
        id BIGSERIAL,
        action_history_id BIGINT NOT NULL REFERENCES action_histories(id) ON DELETE CASCADE,

        -- Action identification
        action_id VARCHAR(64) NOT NULL, -- UUID for this specific action
        correlation_id VARCHAR(64), -- For tracing across systems

        -- Timing information
        action_timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
        execution_start_time TIMESTAMP WITH TIME ZONE,
        execution_end_time TIMESTAMP WITH TIME ZONE,
        execution_duration_ms INTEGER,

        -- Alert context
        alert_name VARCHAR(200) NOT NULL,
        alert_severity VARCHAR(20) NOT NULL, -- info, warning, critical
        alert_labels JSONB,
        alert_annotations JSONB,
        alert_firing_time TIMESTAMP WITH TIME ZONE,

        -- AI model information
        model_used VARCHAR(100) NOT NULL,
        routing_tier VARCHAR(20), -- route1, route2, route3
        model_confidence DECIMAL(4,3) NOT NULL, -- 0.000-1.000
        model_reasoning TEXT,
        alternative_actions JSONB, -- [{"action": "scale_deployment", "confidence": 0.85}]

        -- Action details
        action_type VARCHAR(50) NOT NULL,
        action_parameters JSONB, -- {"replicas": 5, "memory": "2Gi"}

        -- Resource state capture
        resource_state_before JSONB,
        resource_state_after JSONB,

        -- Execution tracking
        execution_status VARCHAR(20) DEFAULT 'pending', -- pending, executing, completed, failed, rolled-back
        execution_error TEXT,
        kubernetes_operations JSONB, -- [{"operation": "patch", "resource": "deployment/webapp", "result": "success"}]

        -- Effectiveness assessment
        effectiveness_score DECIMAL(4,3), -- 0.000-1.000, calculated after execution
        effectiveness_criteria JSONB, -- {"alert_resolved": true, "target_metric_improved": true}
        effectiveness_assessed_at TIMESTAMP WITH TIME ZONE,
        effectiveness_assessment_method VARCHAR(20), -- automated, manual, ml-derived
        effectiveness_notes TEXT,

        -- Follow-up tracking
        follow_up_actions JSONB, -- [{"action_id": "uuid", "relation": "correction"}]

        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

        -- Primary key includes timestamp for partitioning
        PRIMARY KEY (id, action_timestamp),
        -- Unique constraint must include partition key
        UNIQUE (action_id, action_timestamp)
    ) PARTITION BY RANGE (action_timestamp);

    -- Default partition catches any row outside defined monthly ranges (e.g., backdated test data)
    CREATE TABLE resource_action_traces_default
        PARTITION OF resource_action_traces
        DEFAULT;

    -- Create partitions for resource_action_traces
    -- Range: March 2026 - December 2028 (Issue #234: first release month through 3-year horizon)
    CREATE TABLE resource_action_traces_2026_03
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');

    CREATE TABLE resource_action_traces_2026_04
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-04-01') TO ('2026-05-01');

    CREATE TABLE resource_action_traces_2026_05
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-05-01') TO ('2026-06-01');

    CREATE TABLE resource_action_traces_2026_06
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-06-01') TO ('2026-07-01');

    CREATE TABLE resource_action_traces_2026_07
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-07-01') TO ('2026-08-01');

    CREATE TABLE resource_action_traces_2026_08
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-08-01') TO ('2026-09-01');

    CREATE TABLE resource_action_traces_2026_09
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-09-01') TO ('2026-10-01');

    CREATE TABLE resource_action_traces_2026_10
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-10-01') TO ('2026-11-01');

    CREATE TABLE resource_action_traces_2026_11
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-11-01') TO ('2026-12-01');

    CREATE TABLE resource_action_traces_2026_12
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-12-01') TO ('2027-01-01');

    CREATE TABLE resource_action_traces_2027_01
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-01-01') TO ('2027-02-01');

    CREATE TABLE resource_action_traces_2027_02
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-02-01') TO ('2027-03-01');

    CREATE TABLE resource_action_traces_2027_03
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-03-01') TO ('2027-04-01');

    CREATE TABLE resource_action_traces_2027_04
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-04-01') TO ('2027-05-01');

    CREATE TABLE resource_action_traces_2027_05
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-05-01') TO ('2027-06-01');

    CREATE TABLE resource_action_traces_2027_06
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-06-01') TO ('2027-07-01');

    CREATE TABLE resource_action_traces_2027_07
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-07-01') TO ('2027-08-01');

    CREATE TABLE resource_action_traces_2027_08
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-08-01') TO ('2027-09-01');

    CREATE TABLE resource_action_traces_2027_09
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-09-01') TO ('2027-10-01');

    CREATE TABLE resource_action_traces_2027_10
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-10-01') TO ('2027-11-01');

    CREATE TABLE resource_action_traces_2027_11
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-11-01') TO ('2027-12-01');

    CREATE TABLE resource_action_traces_2027_12
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-12-01') TO ('2028-01-01');

    CREATE TABLE resource_action_traces_2028_01
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-01-01') TO ('2028-02-01');

    CREATE TABLE resource_action_traces_2028_02
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-02-01') TO ('2028-03-01');

    CREATE TABLE resource_action_traces_2028_03
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-03-01') TO ('2028-04-01');

    CREATE TABLE resource_action_traces_2028_04
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-04-01') TO ('2028-05-01');

    CREATE TABLE resource_action_traces_2028_05
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-05-01') TO ('2028-06-01');

    CREATE TABLE resource_action_traces_2028_06
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-06-01') TO ('2028-07-01');

    CREATE TABLE resource_action_traces_2028_07
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-07-01') TO ('2028-08-01');

    CREATE TABLE resource_action_traces_2028_08
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-08-01') TO ('2028-09-01');

    CREATE TABLE resource_action_traces_2028_09
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-09-01') TO ('2028-10-01');

    CREATE TABLE resource_action_traces_2028_10
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-10-01') TO ('2028-11-01');

    CREATE TABLE resource_action_traces_2028_11
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-11-01') TO ('2028-12-01');

    CREATE TABLE resource_action_traces_2028_12
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-12-01') TO ('2029-01-01');

    -- Create indexes on the partitioned table (will be inherited by partitions)
    CREATE INDEX idx_rat_action_history ON resource_action_traces (action_history_id, action_timestamp);
    CREATE INDEX idx_rat_action_type ON resource_action_traces (action_type, action_timestamp);
    CREATE INDEX idx_rat_model_used ON resource_action_traces (model_used, action_timestamp);
    CREATE INDEX idx_rat_alert_name ON resource_action_traces (alert_name, action_timestamp);
    CREATE INDEX idx_rat_execution_status ON resource_action_traces (execution_status) WHERE execution_status IN ('pending', 'executing');
    CREATE INDEX idx_rat_effectiveness_score ON resource_action_traces (effectiveness_score) WHERE effectiveness_score IS NOT NULL;
    CREATE INDEX idx_rat_correlation_id ON resource_action_traces (correlation_id) WHERE correlation_id IS NOT NULL;

    -- GIN indexes for JSONB queries
    CREATE INDEX idx_rat_alert_labels_gin ON resource_action_traces USING GIN (alert_labels);
    CREATE INDEX idx_rat_action_parameters_gin ON resource_action_traces USING GIN (action_parameters);
    CREATE INDEX idx_rat_resource_state_gin ON resource_action_traces USING GIN (resource_state_before);

    -- Recreate the views that depend on resource_action_traces
    DROP VIEW IF EXISTS action_history_summary;
    CREATE VIEW action_history_summary AS
    SELECT
        rr.namespace,
        rr.kind,
        rr.name,
        ah.total_actions,
        ah.last_action_at,
        COUNT(rat.id) as recent_actions_24h,
        AVG(rat.effectiveness_score) as avg_effectiveness_24h,
        COUNT(DISTINCT rat.action_type) as action_types_used
    FROM resource_references rr
    JOIN action_histories ah ON ah.resource_id = rr.id
    LEFT JOIN resource_action_traces rat ON rat.action_history_id = ah.id
        AND rat.action_timestamp > NOW() - INTERVAL '24 hours'
    GROUP BY rr.id, rr.namespace, rr.kind, rr.name, ah.total_actions, ah.last_action_at;

    -- Create trigger for updated_at on the new table
    CREATE TRIGGER update_resource_action_traces_updated_at
        BEFORE UPDATE ON resource_action_traces
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at();
  003_stored_procedures.sql: "-- Migration: 003_stored_procedures.sql\n-- Replace
    hardcoded queries with PostgreSQL stored procedures for better performance and
    maintainability\n\n-- =============================================================================\n--
    OSCILLATION DETECTION PROCEDURES\n-- =============================================================================\n\n--
    1. Scale Oscillation Detection\nCREATE OR REPLACE FUNCTION detect_scale_oscillation(\n
    \   p_namespace VARCHAR(63),\n    p_kind VARCHAR(100), \n    p_name VARCHAR(253),\n
    \   p_window_minutes INTEGER DEFAULT 120\n)\nRETURNS TABLE (\n    direction_changes
    INTEGER,\n    first_change TIMESTAMP WITH TIME ZONE,\n    last_change TIMESTAMP
    WITH TIME ZONE,\n    avg_effectiveness DECIMAL(4,3),\n    duration_minutes DECIMAL(10,2),\n
    \   severity VARCHAR(20),\n    action_sequence JSONB\n) AS $$\nBEGIN\n    RETURN
    QUERY\n    WITH scale_actions AS (\n        SELECT \n            rat.id,\n            rat.action_timestamp,\n
    \           rat.action_parameters->>'replicas' as replica_count,\n            LAG(rat.action_parameters->>'replicas')
    OVER (\n                PARTITION BY ah.resource_id \n                ORDER BY
    rat.action_timestamp\n            ) as prev_replica_count,\n            LAG(rat.action_timestamp)
    OVER (\n                PARTITION BY ah.resource_id \n                ORDER BY
    rat.action_timestamp  \n            ) as prev_timestamp,\n            COALESCE(rat.effectiveness_score,
    0.0) as effectiveness_score\n        FROM resource_action_traces rat\n        JOIN
    action_histories ah ON rat.action_history_id = ah.id\n        JOIN resource_references
    rr ON ah.resource_id = rr.id\n        WHERE rat.action_type = 'scale_deployment'\n
    \       AND rr.namespace = p_namespace \n        AND rr.kind = p_kind \n        AND
    rr.name = p_name\n        AND rat.action_timestamp > NOW() - INTERVAL '1 minute'
    * p_window_minutes\n    ),\n    direction_changes AS (\n        SELECT \n            id,\n
    \           action_timestamp,\n            replica_count::int,\n            prev_replica_count::int,\n
    \           prev_timestamp,\n            effectiveness_score,\n            CASE
    \n                WHEN replica_count::int > prev_replica_count::int THEN 'up'\n
    \               WHEN replica_count::int < prev_replica_count::int THEN 'down'\n
    \               ELSE 'none'\n            END as direction,\n            LAG(CASE
    \n                WHEN replica_count::int > prev_replica_count::int THEN 'up'\n
    \               WHEN replica_count::int < prev_replica_count::int THEN 'down'\n
    \               ELSE 'none'\n            END) OVER (ORDER BY action_timestamp)
    as prev_direction\n        FROM scale_actions\n        WHERE prev_replica_count
    IS NOT NULL\n    ),\n    oscillation_analysis AS (\n        SELECT \n            COUNT(*)
    FILTER (WHERE direction != prev_direction AND direction != 'none' AND prev_direction
    != 'none') as direction_changes,\n            MIN(action_timestamp) as first_change,\n
    \           MAX(action_timestamp) as last_change,\n            AVG(effectiveness_score)
    as avg_effectiveness,\n            EXTRACT(EPOCH FROM (MAX(action_timestamp) -
    MIN(action_timestamp)))/60 as duration_minutes,\n            array_agg(\n                json_build_object(\n
    \                   'timestamp', action_timestamp,\n                    'replica_count',
    replica_count,\n                    'direction', direction,\n                    'effectiveness',
    effectiveness_score\n                ) ORDER BY action_timestamp\n            )
    as action_sequence\n        FROM direction_changes\n    )\n    SELECT \n        oa.direction_changes::INTEGER,\n
    \       oa.first_change,\n        oa.last_change,\n        oa.avg_effectiveness::DECIMAL(4,3),\n
    \       oa.duration_minutes::DECIMAL(10,2),\n        CASE \n            WHEN oa.direction_changes
    >= 4 AND oa.duration_minutes <= 60 AND oa.avg_effectiveness < 0.5 THEN 'critical'\n
    \           WHEN oa.direction_changes >= 3 AND oa.duration_minutes <= 120 AND
    oa.avg_effectiveness < 0.7 THEN 'high'\n            WHEN oa.direction_changes
    >= 2 AND oa.duration_minutes <= 180 THEN 'medium'\n            ELSE 'low'\n        END::VARCHAR(20)
    as severity,\n        to_jsonb(oa.action_sequence) as action_sequence\n    FROM
    oscillation_analysis oa\n    WHERE oa.direction_changes >= 2;\nEND;\n$$ LANGUAGE
    plpgsql STABLE SECURITY DEFINER;\n\n-- 2. Resource Thrashing Detection\nCREATE
    OR REPLACE FUNCTION detect_resource_thrashing(\n    p_namespace VARCHAR(63),\n
    \   p_kind VARCHAR(100),\n    p_name VARCHAR(253), \n    p_window_minutes INTEGER
    DEFAULT 120\n)\nRETURNS TABLE (\n    thrashing_transitions INTEGER,\n    total_actions
    INTEGER,\n    first_action TIMESTAMP WITH TIME ZONE,\n    last_action TIMESTAMP
    WITH TIME ZONE,\n    avg_effectiveness DECIMAL(4,3),\n    avg_time_gap_minutes
    DECIMAL(10,2),\n    severity VARCHAR(20)\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH
    resource_actions AS (\n        SELECT \n            rat.action_timestamp,\n            rat.action_type,\n
    \           rat.action_parameters,\n            rat.effectiveness_score,\n            LAG(rat.action_type)
    OVER (\n                PARTITION BY ah.resource_id \n                ORDER BY
    rat.action_timestamp\n            ) as prev_action_type,\n            LAG(rat.action_timestamp)
    OVER (\n                PARTITION BY ah.resource_id \n                ORDER BY
    rat.action_timestamp\n            ) as prev_timestamp\n        FROM resource_action_traces
    rat\n        JOIN action_histories ah ON rat.action_history_id = ah.id\n        JOIN
    resource_references rr ON ah.resource_id = rr.id\n        WHERE rat.action_type
    IN ('increase_resources', 'scale_deployment')\n        AND rr.namespace = p_namespace
    \n        AND rr.kind = p_kind \n        AND rr.name = p_name\n        AND rat.action_timestamp
    > NOW() - INTERVAL '1 minute' * p_window_minutes\n    ),\n    thrashing_patterns
    AS (\n        SELECT \n            action_timestamp,\n            action_type,\n
    \           prev_action_type,\n            COALESCE(effectiveness_score, 0.0)
    as effectiveness_score,\n            EXTRACT(EPOCH FROM (action_timestamp - prev_timestamp))/60
    as time_gap_minutes,\n            CASE \n                WHEN (action_type = 'increase_resources'
    AND prev_action_type = 'scale_deployment') OR\n                     (action_type
    = 'scale_deployment' AND prev_action_type = 'increase_resources')\n                THEN
    1 ELSE 0\n            END as is_thrashing_transition\n        FROM resource_actions\n
    \       WHERE prev_action_type IS NOT NULL\n        AND action_timestamp - prev_timestamp
    < INTERVAL '45 minutes'\n    ),\n    thrashing_analysis AS (\n        SELECT \n
    \           COUNT(*) FILTER (WHERE is_thrashing_transition = 1) as thrashing_transitions,\n
    \           COUNT(*) as total_actions,\n            MIN(action_timestamp) as first_action,\n
    \           MAX(action_timestamp) as last_action,\n            AVG(effectiveness_score)
    as avg_effectiveness,\n            AVG(time_gap_minutes) as avg_time_gap_minutes\n
    \       FROM thrashing_patterns\n    )\n    SELECT \n        ta.thrashing_transitions::INTEGER,\n
    \       ta.total_actions::INTEGER,\n        ta.first_action,\n        ta.last_action,\n
    \       ta.avg_effectiveness::DECIMAL(4,3),\n        ta.avg_time_gap_minutes::DECIMAL(10,2),\n
    \       CASE \n            WHEN ta.thrashing_transitions >= 3 AND ta.avg_effectiveness
    < 0.6 THEN 'critical'\n            WHEN ta.thrashing_transitions >= 2 AND ta.avg_effectiveness
    < 0.7 THEN 'high'\n            WHEN ta.thrashing_transitions >= 1 AND ta.avg_time_gap_minutes
    < 15 THEN 'medium'\n            ELSE 'low'\n        END::VARCHAR(20) as severity\n
    \   FROM thrashing_analysis ta\n    WHERE ta.thrashing_transitions >= 1;\nEND;\n$$
    LANGUAGE plpgsql STABLE SECURITY DEFINER;\n\n-- 3. Ineffective Loop Detection\nCREATE
    OR REPLACE FUNCTION detect_ineffective_loops(\n    p_namespace VARCHAR(63),\n
    \   p_kind VARCHAR(100),\n    p_name VARCHAR(253),\n    p_window_minutes INTEGER
    DEFAULT 120\n)\nRETURNS TABLE (\n    action_type VARCHAR(50),\n    repetition_count
    INTEGER,\n    avg_effectiveness DECIMAL(4,3),\n    effectiveness_stddev DECIMAL(4,3),\n
    \   first_occurrence TIMESTAMP WITH TIME ZONE,\n    last_occurrence TIMESTAMP
    WITH TIME ZONE,\n    span_minutes DECIMAL(10,2),\n    severity VARCHAR(20),\n
    \   effectiveness_trend DECIMAL(6,3),\n    effectiveness_scores DECIMAL(4,3)[],\n
    \   timestamps TIMESTAMP WITH TIME ZONE[]\n) AS $$\nBEGIN\n    RETURN QUERY\n
    \   WITH repeated_actions AS (\n        SELECT \n            rat.action_type,\n
    \           COUNT(*) as repetition_count,\n            AVG(COALESCE(rat.effectiveness_score,
    0.0)) as avg_effectiveness,\n            STDDEV(COALESCE(rat.effectiveness_score,
    0.0)) as effectiveness_stddev,\n            MIN(rat.action_timestamp) as first_occurrence,\n
    \           MAX(rat.action_timestamp) as last_occurrence,\n            EXTRACT(EPOCH
    FROM (MAX(rat.action_timestamp) - MIN(rat.action_timestamp)))/60 as span_minutes,\n
    \           array_agg(COALESCE(rat.effectiveness_score, 0.0) ORDER BY rat.action_timestamp)
    as effectiveness_scores,\n            array_agg(rat.action_timestamp ORDER BY
    rat.action_timestamp) as timestamps\n        FROM resource_action_traces rat\n
    \       JOIN action_histories ah ON rat.action_history_id = ah.id\n        JOIN
    resource_references rr ON ah.resource_id = rr.id\n        WHERE rr.namespace =
    p_namespace \n        AND rr.kind = p_kind \n        AND rr.name = p_name\n        AND
    rat.action_timestamp > NOW() - INTERVAL '1 minute' * p_window_minutes\n        GROUP
    BY rat.action_type\n    ),\n    ineffective_patterns AS (\n        SELECT \n            ra.action_type,\n
    \           ra.repetition_count,\n            ra.avg_effectiveness,\n            COALESCE(ra.effectiveness_stddev,
    0.0) as effectiveness_stddev,\n            ra.first_occurrence,\n            ra.last_occurrence,\n
    \           ra.span_minutes,\n            ra.effectiveness_scores,\n            ra.timestamps,\n
    \           CASE \n                WHEN ra.repetition_count >= 5 AND ra.avg_effectiveness
    < 0.3 THEN 'critical'\n                WHEN ra.repetition_count >= 4 AND ra.avg_effectiveness
    < 0.5 THEN 'high'\n                WHEN ra.repetition_count >= 3 AND ra.avg_effectiveness
    < 0.6 THEN 'medium'\n                WHEN ra.repetition_count >= 2 AND ra.avg_effectiveness
    < 0.4 THEN 'low'\n                ELSE 'none'\n            END as severity,\n
    \           CASE \n                WHEN ra.repetition_count >= 3 THEN\n                    (ra.effectiveness_scores[array_length(ra.effectiveness_scores,
    1)] - ra.effectiveness_scores[1]) / \n                    GREATEST(ra.effectiveness_scores[1],
    0.1)\n                ELSE 0\n            END as effectiveness_trend\n        FROM
    repeated_actions ra\n        WHERE ra.repetition_count >= 2\n    )\n    SELECT
    \n        ip.action_type,\n        ip.repetition_count::INTEGER,\n        ip.avg_effectiveness::DECIMAL(4,3),\n
    \       ip.effectiveness_stddev::DECIMAL(4,3),\n        ip.first_occurrence,\n
    \       ip.last_occurrence,\n        ip.span_minutes::DECIMAL(10,2),\n        ip.severity::VARCHAR(20),\n
    \       ip.effectiveness_trend::DECIMAL(6,3),\n        ip.effectiveness_scores::DECIMAL(4,3)[],\n
    \       ip.timestamps\n    FROM ineffective_patterns ip\n    WHERE ip.severity
    != 'none'\n    ORDER BY \n        CASE ip.severity \n            WHEN 'critical'
    THEN 1 \n            WHEN 'high' THEN 2 \n            WHEN 'medium' THEN 3 \n
    \           ELSE 4 \n        END,\n        ip.avg_effectiveness ASC;\nEND;\n$$
    LANGUAGE plpgsql STABLE SECURITY DEFINER;\n\n-- 4. Cascading Failure Detection
    \ \nCREATE OR REPLACE FUNCTION detect_cascading_failures(\n    p_namespace VARCHAR(63),\n
    \   p_kind VARCHAR(100),\n    p_name VARCHAR(253),\n    p_window_minutes INTEGER
    DEFAULT 120\n)\nRETURNS TABLE (\n    action_type VARCHAR(50),\n    total_actions
    INTEGER,\n    avg_new_alerts DECIMAL(6,2),\n    recurrence_rate DECIMAL(4,3),\n
    \   avg_effectiveness DECIMAL(4,3),\n    actions_causing_cascades INTEGER,\n    max_alerts_triggered
    INTEGER,\n    severity VARCHAR(20)\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH
    action_outcomes AS (\n        SELECT \n            rat.id,\n            rat.action_timestamp,\n
    \           rat.action_type,\n            rat.alert_name as original_alert,\n
    \           COALESCE(rat.effectiveness_score, 0.0) as effectiveness_score,\n            (\n
    \               SELECT COUNT(DISTINCT rat2.alert_name)\n                FROM resource_action_traces
    rat2\n                JOIN action_histories ah2 ON rat2.action_history_id = ah2.id\n
    \               WHERE ah2.resource_id = ah.resource_id\n                AND rat2.action_timestamp
    BETWEEN rat.action_timestamp AND rat.action_timestamp + INTERVAL '30 minutes'\n
    \               AND rat2.alert_name != rat.alert_name\n            ) as new_alerts_triggered,\n
    \           (\n                SELECT COUNT(*)\n                FROM resource_action_traces
    rat3\n                JOIN action_histories ah3 ON rat3.action_history_id = ah3.id\n
    \               WHERE ah3.resource_id = ah.resource_id\n                AND rat3.action_timestamp
    > rat.action_timestamp\n                AND rat3.alert_name = rat.alert_name\n
    \               LIMIT 1\n            ) as original_alert_recurred\n        FROM
    resource_action_traces rat\n        JOIN action_histories ah ON rat.action_history_id
    = ah.id\n        JOIN resource_references rr ON ah.resource_id = rr.id\n        WHERE
    rr.namespace = p_namespace \n        AND rr.kind = p_kind \n        AND rr.name
    = p_name\n        AND rat.action_timestamp > NOW() - INTERVAL '1 minute' * p_window_minutes\n
    \   ),\n    cascading_analysis AS (\n        SELECT \n            ao.action_type,\n
    \           COUNT(*) as total_actions,\n            AVG(ao.new_alerts_triggered::float)
    as avg_new_alerts,\n            AVG(CASE WHEN ao.original_alert_recurred > 0 THEN
    1.0 ELSE 0.0 END) as recurrence_rate,\n            AVG(ao.effectiveness_score)
    as avg_effectiveness,\n            SUM(CASE WHEN ao.new_alerts_triggered > 0 THEN
    1 ELSE 0 END) as actions_causing_cascades,\n            MAX(ao.new_alerts_triggered)
    as max_alerts_triggered\n        FROM action_outcomes ao\n        GROUP BY ao.action_type\n
    \   )\n    SELECT \n        ca.action_type,\n        ca.total_actions::INTEGER,\n
    \       ca.avg_new_alerts::DECIMAL(6,2),\n        ca.recurrence_rate::DECIMAL(4,3),\n
    \       ca.avg_effectiveness::DECIMAL(4,3),\n        ca.actions_causing_cascades::INTEGER,\n
    \       ca.max_alerts_triggered::INTEGER,\n        CASE \n            WHEN ca.avg_new_alerts
    > 2.0 AND ca.recurrence_rate > 0.5 THEN 'critical'\n            WHEN ca.avg_new_alerts
    > 1.5 OR ca.recurrence_rate > 0.7 THEN 'high'\n            WHEN ca.avg_new_alerts
    > 1.0 OR ca.recurrence_rate > 0.4 THEN 'medium'\n            WHEN ca.actions_causing_cascades
    > 0 THEN 'low'\n            ELSE 'none'\n        END::VARCHAR(20) as severity\n
    \   FROM cascading_analysis ca\n    WHERE ca.actions_causing_cascades > 0\n    ORDER
    BY ca.avg_new_alerts DESC, ca.recurrence_rate DESC;\nEND;\n$$ LANGUAGE plpgsql
    STABLE SECURITY DEFINER;\n\n-- =============================================================================\n--
    ACTION HISTORY MANAGEMENT PROCEDURES\n-- =============================================================================\n\n--
    5. Get Action History with Filters\nCREATE OR REPLACE FUNCTION get_action_traces(\n
    \   p_namespace VARCHAR(63),\n    p_kind VARCHAR(100),\n    p_name VARCHAR(253),\n
    \   p_action_type VARCHAR(50) DEFAULT NULL,\n    p_model_used VARCHAR(100) DEFAULT
    NULL,\n    p_time_start TIMESTAMP WITH TIME ZONE DEFAULT NULL,\n    p_time_end
    TIMESTAMP WITH TIME ZONE DEFAULT NULL,\n    p_limit INTEGER DEFAULT 50,\n    p_offset
    INTEGER DEFAULT 0\n)\nRETURNS TABLE (\n    action_id VARCHAR(64),\n    action_timestamp
    TIMESTAMP WITH TIME ZONE,\n    action_type VARCHAR(50),\n    model_used VARCHAR(100),\n
    \   model_confidence DECIMAL(4,3),\n    execution_status VARCHAR(20),\n    effectiveness_score
    DECIMAL(4,3),\n    model_reasoning TEXT,\n    action_parameters JSONB,\n    alert_name
    VARCHAR(200),\n    alert_severity VARCHAR(20)\n) AS $$\nBEGIN\n    RETURN QUERY\n
    \   SELECT \n        rat.action_id,\n        rat.action_timestamp,\n        rat.action_type,\n
    \       rat.model_used,\n        rat.model_confidence,\n        rat.execution_status,\n
    \       rat.effectiveness_score,\n        rat.model_reasoning,\n        rat.action_parameters,\n
    \       rat.alert_name,\n        rat.alert_severity\n    FROM resource_action_traces
    rat\n    JOIN action_histories ah ON rat.action_history_id = ah.id\n    JOIN resource_references
    rr ON ah.resource_id = rr.id\n    WHERE rr.namespace = p_namespace \n    AND rr.kind
    = p_kind \n    AND rr.name = p_name\n    AND (p_action_type IS NULL OR rat.action_type
    = p_action_type)\n    AND (p_model_used IS NULL OR rat.model_used = p_model_used)\n
    \   AND (p_time_start IS NULL OR rat.action_timestamp >= p_time_start)\n    AND
    (p_time_end IS NULL OR rat.action_timestamp <= p_time_end)\n    ORDER BY rat.action_timestamp
    DESC\n    LIMIT p_limit\n    OFFSET p_offset;\nEND;\n$$ LANGUAGE plpgsql STABLE
    SECURITY DEFINER;\n\n-- 6. Get Action Effectiveness Metrics\nCREATE OR REPLACE
    FUNCTION get_action_effectiveness(\n    p_namespace VARCHAR(63),\n    p_kind VARCHAR(100),\n
    \   p_name VARCHAR(253),\n    p_action_type VARCHAR(50) DEFAULT NULL,\n    p_time_start
    TIMESTAMP WITH TIME ZONE DEFAULT NOW() - INTERVAL '7 days',\n    p_time_end TIMESTAMP
    WITH TIME ZONE DEFAULT NOW()\n)\nRETURNS TABLE (\n    action_type VARCHAR(50),\n
    \   sample_size INTEGER,\n    avg_effectiveness DECIMAL(4,3),\n    stddev_effectiveness
    DECIMAL(4,3),\n    min_effectiveness DECIMAL(4,3),\n    max_effectiveness DECIMAL(4,3),\n
    \   success_rate DECIMAL(4,3)\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT \n
    \       rat.action_type,\n        COUNT(*)::INTEGER as sample_size,\n        AVG(rat.effectiveness_score)::DECIMAL(4,3)
    as avg_effectiveness,\n        STDDEV(rat.effectiveness_score)::DECIMAL(4,3) as
    stddev_effectiveness,\n        MIN(rat.effectiveness_score)::DECIMAL(4,3) as min_effectiveness,\n
    \       MAX(rat.effectiveness_score)::DECIMAL(4,3) as max_effectiveness,\n        AVG(CASE
    WHEN rat.execution_status = 'completed' THEN 1.0 ELSE 0.0 END)::DECIMAL(4,3) as
    success_rate\n    FROM resource_action_traces rat\n    JOIN action_histories ah
    ON rat.action_history_id = ah.id\n    JOIN resource_references rr ON ah.resource_id
    = rr.id\n    WHERE rr.namespace = p_namespace \n    AND rr.kind = p_kind \n    AND
    rr.name = p_name\n    AND rat.effectiveness_score IS NOT NULL\n    AND rat.action_timestamp
    BETWEEN p_time_start AND p_time_end\n    AND (p_action_type IS NULL OR rat.action_type
    = p_action_type)\n    GROUP BY rat.action_type\n    HAVING COUNT(*) >= 1\n    ORDER
    BY avg_effectiveness DESC;\nEND;\n$$ LANGUAGE plpgsql STABLE SECURITY DEFINER;\n\n--
    7. Store Oscillation Detection Result\nCREATE OR REPLACE FUNCTION store_oscillation_detection(\n
    \   p_pattern_id INTEGER,\n    p_namespace VARCHAR(63),\n    p_kind VARCHAR(100),\n
    \   p_name VARCHAR(253),\n    p_confidence DECIMAL(4,3),\n    p_action_count INTEGER,\n
    \   p_time_span_minutes INTEGER,\n    p_pattern_evidence JSONB,\n    p_prevention_action
    VARCHAR(50) DEFAULT NULL\n)\nRETURNS INTEGER AS $$\nDECLARE\n    v_resource_id
    INTEGER;\n    v_detection_id INTEGER;\nBEGIN\n    -- Get or create resource reference\n
    \   SELECT id INTO v_resource_id\n    FROM resource_references \n    WHERE namespace
    = p_namespace AND kind = p_kind AND name = p_name;\n    \n    IF v_resource_id
    IS NULL THEN\n        INSERT INTO resource_references (resource_uid, api_version,
    kind, name, namespace, last_seen)\n        VALUES (gen_random_uuid()::text, 'apps/v1',
    p_kind, p_name, p_namespace, NOW())\n        RETURNING id INTO v_resource_id;\n
    \   END IF;\n    \n    -- Insert oscillation detection\n    INSERT INTO oscillation_detections
    (\n        pattern_id, resource_id, detected_at, confidence, action_count,\n        time_span_minutes,
    pattern_evidence, prevention_applied,\n        prevention_action\n    ) VALUES
    (\n        p_pattern_id, v_resource_id, NOW(), p_confidence, p_action_count,\n
    \       p_time_span_minutes, p_pattern_evidence, \n        p_prevention_action
    IS NOT NULL,\n        p_prevention_action\n    ) RETURNING id INTO v_detection_id;\n
    \   \n    RETURN v_detection_id;\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n\n--
    =============================================================================\n--
    SECURITY AND PERFORMANCE OPTIMIZATIONS\n-- =============================================================================\n\n--
    Create indexes for procedure performance\nCREATE INDEX IF NOT EXISTS idx_rat_resource_action_time
    \nON resource_action_traces (action_history_id, action_type, action_timestamp
    DESC);\n\nCREATE INDEX IF NOT EXISTS idx_rat_effectiveness_analysis\nON resource_action_traces
    (action_type, effectiveness_score, action_timestamp)\nWHERE effectiveness_score
    IS NOT NULL;\n\n-- Grant execute permissions (adjust as needed for your environment)\nGRANT
    EXECUTE ON FUNCTION detect_scale_oscillation(VARCHAR, VARCHAR, VARCHAR, INTEGER)
    TO slm_user;\nGRANT EXECUTE ON FUNCTION detect_resource_thrashing(VARCHAR, VARCHAR,
    VARCHAR, INTEGER) TO slm_user;\nGRANT EXECUTE ON FUNCTION detect_ineffective_loops(VARCHAR,
    VARCHAR, VARCHAR, INTEGER) TO slm_user;\nGRANT EXECUTE ON FUNCTION detect_cascading_failures(VARCHAR,
    VARCHAR, VARCHAR, INTEGER) TO slm_user;\nGRANT EXECUTE ON FUNCTION get_action_traces(VARCHAR,
    VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME
    ZONE, INTEGER, INTEGER) TO slm_user;\nGRANT EXECUTE ON FUNCTION get_action_effectiveness(VARCHAR,
    VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE)
    TO slm_user;\nGRANT EXECUTE ON FUNCTION store_oscillation_detection(INTEGER, VARCHAR,
    VARCHAR, VARCHAR, DECIMAL, INTEGER, INTEGER, JSONB, VARCHAR) TO slm_user;\n\n--
    Add helpful comments\nCOMMENT ON FUNCTION detect_scale_oscillation IS 'Detects
    scale oscillation patterns for a resource within a time window';\nCOMMENT ON FUNCTION
    detect_resource_thrashing IS 'Detects resource thrashing between scale and resource
    adjustment actions';\nCOMMENT ON FUNCTION detect_ineffective_loops IS 'Identifies
    repeated actions with low effectiveness scores';\nCOMMENT ON FUNCTION detect_cascading_failures
    IS 'Detects actions that trigger more alerts than they resolve';\nCOMMENT ON FUNCTION
    get_action_traces IS 'Retrieves filtered action history for a resource';\nCOMMENT
    ON FUNCTION get_action_effectiveness IS 'Calculates effectiveness metrics for
    actions on a resource';\nCOMMENT ON FUNCTION store_oscillation_detection IS 'Stores
    oscillation detection results with proper resource management';\n\n-- =============================================================================\n--
    DETECTOR BASE PROCEDURES\n-- =============================================================================\n\n--
    8. Get Resource Actions Base\nCREATE OR REPLACE FUNCTION get_resource_actions_base(\n
    \   p_namespace VARCHAR(63),\n    p_kind VARCHAR(100),\n    p_name VARCHAR(253),\n
    \   p_window_minutes INTEGER DEFAULT NULL\n)\nRETURNS TABLE (\n    trace_id BIGINT,\n
    \   action_timestamp TIMESTAMP WITH TIME ZONE,\n    action_type VARCHAR(50),\n
    \   action_parameters JSONB,\n    effectiveness_score DECIMAL(4,3),\n    model_confidence
    DECIMAL(4,3),\n    execution_status VARCHAR(20)\n) AS $$\nBEGIN\n    RETURN QUERY\n
    \   SELECT \n        rat.id as trace_id,\n        rat.action_timestamp,\n        rat.action_type,\n
    \       rat.action_parameters,\n        rat.effectiveness_score,\n        rat.model_confidence,\n
    \       rat.execution_status\n    FROM resource_action_traces rat\n    JOIN action_histories
    ah ON rat.action_history_id = ah.id\n    JOIN resource_references rr ON ah.resource_id
    = rr.id\n    WHERE rr.namespace = p_namespace \n    AND rr.kind = p_kind \n    AND
    rr.name = p_name\n    AND (p_window_minutes IS NULL OR rat.action_timestamp >
    NOW() - INTERVAL '1 minute' * p_window_minutes)\n    ORDER BY rat.action_timestamp
    DESC;\nEND;\n$$ LANGUAGE plpgsql STABLE SECURITY DEFINER;\n\n-- 9. Get Resource
    ID\nCREATE OR REPLACE FUNCTION get_resource_id(\n    p_namespace VARCHAR(63),\n
    \   p_kind VARCHAR(100),\n    p_name VARCHAR(253)\n)\nRETURNS INTEGER AS $$\nDECLARE\n
    \   v_resource_id INTEGER;\nBEGIN\n    SELECT id INTO v_resource_id\n    FROM
    resource_references \n    WHERE namespace = p_namespace AND kind = p_kind AND
    name = p_name;\n    \n    IF v_resource_id IS NULL THEN\n        RAISE EXCEPTION
    'Resource not found: namespace=%, kind=%, name=%', p_namespace, p_kind, p_name;\n
    \   END IF;\n    \n    RETURN v_resource_id;\nEND;\n$$ LANGUAGE plpgsql STABLE
    SECURITY DEFINER;\n\n-- 10. Action Oscillation Analysis\nCREATE OR REPLACE FUNCTION
    analyze_action_oscillation(\n    p_namespace VARCHAR(63),\n    p_kind VARCHAR(100),\n
    \   p_name VARCHAR(253),\n    p_window_minutes INTEGER DEFAULT 120\n)\nRETURNS
    TABLE (\n    action_timestamp TIMESTAMP WITH TIME ZONE,\n    action_type VARCHAR(50),\n
    \   effectiveness_score DECIMAL(4,3),\n    prev_timestamp TIMESTAMP WITH TIME
    ZONE,\n    prev_action_type VARCHAR(50),\n    time_gap_minutes DECIMAL(10,2),\n
    \   action_sequence_position INTEGER\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH
    action_analysis AS (\n        SELECT \n            rat.action_timestamp,\n            rat.action_type,\n
    \           rat.effectiveness_score,\n            LAG(rat.action_timestamp) OVER
    (ORDER BY rat.action_timestamp) as prev_timestamp,\n            LAG(rat.action_type)
    OVER (ORDER BY rat.action_timestamp) as prev_action_type,\n            ROW_NUMBER()
    OVER (ORDER BY rat.action_timestamp) as sequence_position\n        FROM resource_action_traces
    rat\n        JOIN action_histories ah ON rat.action_history_id = ah.id\n        JOIN
    resource_references rr ON ah.resource_id = rr.id\n        WHERE rr.namespace =
    p_namespace \n        AND rr.kind = p_kind \n        AND rr.name = p_name\n        AND
    rat.action_timestamp > NOW() - INTERVAL '1 minute' * p_window_minutes\n    )\n
    \   SELECT \n        aa.action_timestamp,\n        aa.action_type,\n        aa.effectiveness_score,\n
    \       aa.prev_timestamp,\n        aa.prev_action_type,\n        CASE \n            WHEN
    aa.prev_timestamp IS NOT NULL THEN\n                EXTRACT(EPOCH FROM (aa.action_timestamp
    - aa.prev_timestamp))/60\n            ELSE 0\n        END::DECIMAL(10,2) as time_gap_minutes,\n
    \       aa.sequence_position::INTEGER as action_sequence_position\n    FROM action_analysis
    aa\n    ORDER BY aa.action_timestamp;\nEND;\n$$ LANGUAGE plpgsql STABLE SECURITY
    DEFINER;\n\n-- Grant execute permissions\nGRANT EXECUTE ON FUNCTION get_resource_actions_base(VARCHAR,
    VARCHAR, VARCHAR, INTEGER) TO slm_user;\nGRANT EXECUTE ON FUNCTION get_resource_id(VARCHAR,
    VARCHAR, VARCHAR) TO slm_user;\nGRANT EXECUTE ON FUNCTION analyze_action_oscillation(VARCHAR,
    VARCHAR, VARCHAR, INTEGER) TO slm_user;\n\n-- Add helpful comments\nCOMMENT ON
    FUNCTION get_resource_actions_base IS 'Retrieves base resource action data with
    optional time window filtering';\nCOMMENT ON FUNCTION get_resource_id IS 'Gets
    the database ID for a resource reference with error handling';\nCOMMENT ON FUNCTION
    analyze_action_oscillation IS 'Analyzes action sequences for oscillation patterns
    with timing gaps';"
  004_add_effectiveness_assessment_due.sql: |
    -- Add missing effectiveness_assessment_due column
    -- Migration: 004_add_effectiveness_assessment_due.sql

    -- Add effectiveness_assessment_due column to resource_action_traces table
    ALTER TABLE resource_action_traces
    ADD COLUMN effectiveness_assessment_due TIMESTAMP WITH TIME ZONE;

    -- Create index for the new column for efficient queries
    CREATE INDEX idx_rat_effectiveness_due ON resource_action_traces (effectiveness_assessment_due);

    -- Update the partition tables as well (they inherit from the parent)
    -- The column will be automatically added to existing partitions since they inherit the schema
  011_rename_alert_to_signal.sql: |
    -- Migration: 011_rename_alert_to_signal.sql
    -- Purpose: Rename all "alert" terminology to "signal" for domain model consistency
    -- Rationale: The project uses "signal" as the abstract term for events (alerts, logs, traces, metrics)
    -- Related: BR-STORAGE-001, BR-STORAGE-030 (architectural consistency)
    -- NOTE: V1.0 version - only handles existing tables (vector tables removed)

    -- ============================================================================
    -- STEP 1: Drop dependent views
    -- ============================================================================

    DROP VIEW IF EXISTS incident_summary_view;

    -- ============================================================================
    -- STEP 2: Drop indexes that will be renamed
    -- ============================================================================

    DROP INDEX IF EXISTS idx_rat_alert_name;
    DROP INDEX IF EXISTS idx_rat_alert_labels_gin;

    -- ============================================================================
    -- STEP 3: Rename columns in resource_action_traces
    -- ============================================================================

    ALTER TABLE resource_action_traces
        RENAME COLUMN alert_name TO signal_name;

    ALTER TABLE resource_action_traces
        RENAME COLUMN alert_severity TO signal_severity;

    ALTER TABLE resource_action_traces
        RENAME COLUMN alert_labels TO signal_labels;

    ALTER TABLE resource_action_traces
        RENAME COLUMN alert_annotations TO signal_annotations;

    ALTER TABLE resource_action_traces
        RENAME COLUMN alert_firing_time TO signal_firing_time;

    -- ============================================================================
    -- STEP 4: Rename columns in action_assessments (if exists)
    -- ============================================================================

    DO $$
    BEGIN
        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'action_assessments') THEN
            ALTER TABLE action_assessments RENAME COLUMN alert_name TO signal_name;
        END IF;
    END $$;

    -- ============================================================================
    -- STEP 5: Rename columns in effectiveness_results (if exists)
    -- ============================================================================

    DO $$
    BEGIN
        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'effectiveness_results') THEN
            ALTER TABLE effectiveness_results RENAME COLUMN alert_resolved TO signal_resolved;
        END IF;
    END $$;

    -- ============================================================================
    -- STEP 6: Rename columns in action_outcomes (if exists)
    -- ============================================================================

    DO $$
    BEGIN
        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'action_outcomes') THEN
            ALTER TABLE action_outcomes RENAME COLUMN alert_resolved TO signal_resolved;
        END IF;
    END $$;

    -- ============================================================================
    -- STEP 7: Rename columns in cascade_analysis (if exists)
    -- ============================================================================

    DO $$
    BEGIN
        IF EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'cascade_analysis') THEN
            ALTER TABLE cascade_analysis RENAME COLUMN alert_resolved TO signal_resolved;
        END IF;
    END $$;

    -- ============================================================================
    -- STEP 8: Recreate indexes with new names
    -- ============================================================================

    CREATE INDEX idx_rat_signal_name ON resource_action_traces (signal_name, action_timestamp);
    CREATE INDEX idx_rat_signal_labels_gin ON resource_action_traces USING GIN (signal_labels);

    -- ============================================================================
    -- STEP 9: Recreate incident_summary_view
    -- ============================================================================

    CREATE OR REPLACE VIEW incident_summary_view AS
    SELECT
        signal_severity as severity,
        COUNT(*) as incident_count
    FROM resource_action_traces
    GROUP BY signal_severity
    ORDER BY
        CASE signal_severity
            WHEN 'critical' THEN 1
            WHEN 'high' THEN 2
            WHEN 'medium' THEN 3
            WHEN 'low' THEN 4
            ELSE 5
        END;

    -- ============================================================================
    -- STEP 10: Update stored procedures/functions
    -- ============================================================================

    -- Drop and recreate analyze_cascade_effects function
    DROP FUNCTION IF EXISTS analyze_cascade_effects(INTEGER, INTERVAL, INTEGER);

    CREATE OR REPLACE FUNCTION analyze_cascade_effects(
        p_days_back INTEGER DEFAULT 7,
        p_time_window INTERVAL DEFAULT '1 hour'::interval,
        p_max_signals INTEGER DEFAULT NULL
    )
    RETURNS TABLE (
        action_type VARCHAR,
        avg_new_signals NUMERIC,
        max_signals_triggered INTEGER,
        actions_causing_cascades INTEGER,
        total_actions INTEGER,
        cascade_rate NUMERIC
    ) AS $$
    BEGIN
        RETURN QUERY
        WITH action_outcomes AS (
            SELECT
                rat.action_type,
                rat.action_id,
                rat.action_timestamp,
                rat.signal_name as original_signal,
                rat.execution_status,
                (
                    SELECT COUNT(DISTINCT rat2.signal_name)
                    FROM resource_action_traces rat2
                    WHERE rat2.action_timestamp BETWEEN
                        rat.action_timestamp AND
                        rat.action_timestamp + p_time_window
                    AND rat2.signal_name != rat.signal_name
                ) as new_signals_triggered,
                (
                    SELECT COUNT(*)
                    FROM resource_action_traces rat3
                    WHERE rat3.action_timestamp > rat.action_timestamp
                    AND rat3.action_timestamp <= rat.action_timestamp + INTERVAL '24 hours'
                    AND rat3.signal_name = rat.signal_name
                ) as recurrence_count
            FROM resource_action_traces rat
            WHERE rat.action_timestamp >= NOW() - (p_days_back || ' days')::INTERVAL
            AND rat.execution_status = 'completed'
        )
        SELECT
            ao.action_type::VARCHAR,
            ROUND(AVG(ao.new_signals_triggered::float), 2) as avg_new_signals,
            MAX(ao.new_signals_triggered)::INTEGER as max_signals_triggered,
            SUM(CASE WHEN ao.new_signals_triggered > 0 THEN 1 ELSE 0 END)::INTEGER as actions_causing_cascades,
            COUNT(*)::INTEGER as total_actions,
            ROUND((SUM(CASE WHEN ao.new_signals_triggered > 0 THEN 1 ELSE 0 END)::float / COUNT(*)) * 100, 2) as cascade_rate
        FROM action_outcomes ao
        GROUP BY ao.action_type
        HAVING p_max_signals IS NULL OR MAX(ao.new_signals_triggered) <= p_max_signals
        ORDER BY cascade_rate DESC;
    END;
    $$ LANGUAGE plpgsql;

    -- Drop and recreate get_recent_actions function
    DROP FUNCTION IF EXISTS get_recent_actions(INTEGER, VARCHAR, VARCHAR);

    CREATE OR REPLACE FUNCTION get_recent_actions(
        p_limit INTEGER DEFAULT 100,
        p_signal_name VARCHAR(200) DEFAULT NULL,
        p_signal_severity VARCHAR(20) DEFAULT NULL
    )
    RETURNS TABLE (
        action_id VARCHAR,
        action_timestamp TIMESTAMP WITH TIME ZONE,
        signal_name VARCHAR,
        signal_severity VARCHAR,
        execution_status VARCHAR
    ) AS $$
    BEGIN
        RETURN QUERY
        SELECT
            rat.action_id::VARCHAR,
            rat.action_timestamp,
            rat.signal_name::VARCHAR,
            rat.signal_severity::VARCHAR,
            rat.execution_status::VARCHAR
        FROM resource_action_traces rat
        WHERE (p_signal_name IS NULL OR rat.signal_name = p_signal_name)
        AND (p_signal_severity IS NULL OR rat.signal_severity = p_signal_severity)
        ORDER BY rat.action_timestamp DESC
        LIMIT p_limit;
    END;
    $$ LANGUAGE plpgsql;
  012_adr033_multidimensional_tracking.sql: |+
    -- +goose Up
    -- +goose StatementBegin
    -- ========================================
    -- ADR-033: Multi-Dimensional Success Tracking
    -- Migration: Add columns for incident-type, workflow, and AI execution mode tracking
    -- Date: November 4, 2025
    -- ========================================

    -- ========================================
    -- DIMENSION 1: INCIDENT TYPE (PRIMARY)
    -- ========================================
    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        incident_type VARCHAR(100);

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        alert_name VARCHAR(255);

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        incident_severity VARCHAR(20);

    COMMENT ON COLUMN resource_action_traces.incident_type IS
    'ADR-033: PRIMARY dimension for success tracking. Examples: pod-oom-killer, high-cpu, disk-pressure';

    COMMENT ON COLUMN resource_action_traces.alert_name IS
    'ADR-033: Prometheus alert name. Can be used as proxy for incident_type';

    COMMENT ON COLUMN resource_action_traces.incident_severity IS
    'ADR-033: Incident severity level. Values: critical, warning, info';

    -- ========================================
    -- DIMENSION 2: WORKFLOW (SECONDARY)
    -- ========================================
    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        workflow_id VARCHAR(64);

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        workflow_version VARCHAR(20);

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        workflow_step_number INT;

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        workflow_execution_id VARCHAR(64);

    COMMENT ON COLUMN resource_action_traces.workflow_id IS
    'ADR-033: SECONDARY dimension. Workflow identifier from catalog. Examples: pod-oom-recovery, disk-cleanup';

    COMMENT ON COLUMN resource_action_traces.workflow_version IS
    'ADR-033: Semantic version of workflow. Examples: v1.0, v1.2, v2.0';

    COMMENT ON COLUMN resource_action_traces.workflow_step_number IS
    'ADR-033: Step position within workflow execution (1, 2, 3, ...). NULL for non-workflow actions';

    COMMENT ON COLUMN resource_action_traces.workflow_execution_id IS
    'ADR-033: Groups all actions in a single workflow execution. Same ID across all steps';

    -- ========================================
    -- AI EXECUTION MODE (HYBRID MODEL)
    -- ========================================
    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        ai_selected_workflow BOOLEAN DEFAULT false;

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        ai_chained_workflows BOOLEAN DEFAULT false;

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        ai_manual_escalation BOOLEAN DEFAULT false;

    ALTER TABLE resource_action_traces ADD COLUMN IF NOT EXISTS
        ai_workflow_customization JSONB;

    COMMENT ON COLUMN resource_action_traces.ai_selected_workflow IS
    'ADR-033: TRUE if AI selected single workflow from catalog (90-95% of cases)';

    COMMENT ON COLUMN resource_action_traces.ai_chained_workflows IS
    'ADR-033: TRUE if AI chained multiple catalog workflows (4-9% of cases)';

    COMMENT ON COLUMN resource_action_traces.ai_manual_escalation IS
    'ADR-033: TRUE if AI escalated to human operator (<1% of cases)';

    COMMENT ON COLUMN resource_action_traces.ai_workflow_customization IS
    'ADR-033: Parameters customized by AI for incident-specific needs. Format: {"param": "value"}';

    -- ========================================
    -- INDEXES FOR MULTI-DIMENSIONAL QUERIES
    -- ========================================

    -- Incident-Type Success Rate (PRIMARY dimension)
    CREATE INDEX IF NOT EXISTS idx_incident_type_success
    ON resource_action_traces(incident_type, execution_status, action_timestamp DESC)
    WHERE incident_type IS NOT NULL;

    -- Workflow Success Rate (SECONDARY dimension)
    CREATE INDEX IF NOT EXISTS idx_workflow_success
    ON resource_action_traces(workflow_id, workflow_version, execution_status, action_timestamp DESC)
    WHERE workflow_id IS NOT NULL;

    -- Action-Type Success Rate (TERTIARY dimension - already have index on action_type)
    -- No new index needed, existing indexes suffice

    -- Multi-dimensional composite index (incident_type + workflow_id + action_type)
    CREATE INDEX IF NOT EXISTS idx_multidimensional_success
    ON resource_action_traces(incident_type, workflow_id, action_type, execution_status, action_timestamp DESC)
    WHERE incident_type IS NOT NULL AND workflow_id IS NOT NULL;

    -- Workflow execution grouping (for chained workflow tracking)
    CREATE INDEX IF NOT EXISTS idx_workflow_execution
    ON resource_action_traces(workflow_execution_id, workflow_step_number, action_timestamp DESC)
    WHERE workflow_execution_id IS NOT NULL;

    -- AI execution mode filtering
    CREATE INDEX IF NOT EXISTS idx_ai_execution_mode
    ON resource_action_traces(incident_type, ai_selected_workflow, ai_chained_workflows, ai_manual_escalation, action_timestamp DESC)
    WHERE incident_type IS NOT NULL;

    -- Alert name lookup (for incident-type proxy)
    CREATE INDEX IF NOT EXISTS idx_alert_name_lookup
    ON resource_action_traces(alert_name, execution_status, action_timestamp DESC)
    WHERE alert_name IS NOT NULL;

    -- ========================================
    -- BACKWARD COMPATIBILITY VALIDATION
    -- ========================================

    -- All new columns are nullable - existing rows remain valid
    -- Existing queries continue to work without modification
    -- New queries can filter WHERE incident_type IS NOT NULL

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    -- ========================================
    -- ROLLBACK: Remove ADR-033 columns and indexes
    -- ========================================

    -- Drop indexes
    DROP INDEX IF EXISTS idx_alert_name_lookup;
    DROP INDEX IF EXISTS idx_ai_execution_mode;
    DROP INDEX IF EXISTS idx_workflow_execution;
    DROP INDEX IF EXISTS idx_multidimensional_success;
    DROP INDEX IF EXISTS idx_workflow_success;
    DROP INDEX IF EXISTS idx_incident_type_success;

    -- Drop AI execution mode columns
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS ai_workflow_customization;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS ai_manual_escalation;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS ai_chained_workflows;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS ai_selected_workflow;

    -- Drop workflow columns
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS workflow_execution_id;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS workflow_step_number;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS workflow_version;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS workflow_id;

    -- Drop incident-type columns
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS incident_severity;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS alert_name;
    ALTER TABLE resource_action_traces DROP COLUMN IF EXISTS incident_type;

    -- +goose StatementEnd

  013_create_audit_events_table.sql: |+
    -- +goose Up
    -- +goose StatementBegin
    -- ========================================
    -- ADR-034: Unified Audit Table Design - Core Schema
    -- Migration: Create audit_events table with event sourcing pattern
    -- BR-STORAGE-032: Unified audit trail for compliance and cross-service correlation
    -- Date: November 18, 2025
    -- Version: 5.7 (Phase 1 of Day 21)
    -- ========================================
    --
    -- ARCHITECTURE PATTERNS:
    -- 1. Event Sourcing: Immutable, append-only audit trail
    -- 2. Monthly Range Partitioning: Partition key event_date (generated column)
    -- 3. JSONB Hybrid Storage: 27 structured columns + flexible JSONB
    -- 4. GIN Index: Fast JSONB path queries
    -- 5. UUID Primary Keys: Distributed system compatibility
    -- 6. Parent-Child Relationships: FK constraint with ON DELETE RESTRICT (requires parent_event_date)
    --
    -- COMPLIANCE:
    -- - SOC 2: Immutable audit trail, 7-year retention support
    -- - ISO 27001: Long-term audit storage infrastructure
    -- - GDPR: Sensitive data tracking via is_sensitive flag
    --
    -- ========================================

    -- Create partitioned audit_events table
    -- AUTHORITATIVE SOURCE: ADR-034 (Unified Audit Table Design)
    -- This migration implements the exact schema from ADR-034
    CREATE TABLE IF NOT EXISTS audit_events (
        -- ========================================
        -- EVENT IDENTITY
        -- ========================================
        event_id UUID NOT NULL DEFAULT gen_random_uuid(),
        event_version VARCHAR(10) NOT NULL DEFAULT '1.0',

        -- ========================================
        -- TEMPORAL INFORMATION
        -- ========================================
        event_timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
        event_date DATE NOT NULL, -- For partitioning (generated from event_timestamp)

        -- Primary key must include partition key for partitioned tables
        PRIMARY KEY (event_id, event_date),

        -- ========================================
        -- EVENT CLASSIFICATION (ADR-034)
        -- ========================================
        event_type VARCHAR(100) NOT NULL,        -- 'gateway.signal.received'
        event_category VARCHAR(50) NOT NULL,     -- 'signal', 'remediation', 'workflow'
        event_action VARCHAR(50) NOT NULL,       -- 'received', 'processed', 'executed'
        event_outcome VARCHAR(20) NOT NULL,      -- 'success', 'failure', 'pending'

        -- ========================================
        -- ACTOR INFORMATION (Who)
        -- ========================================
        actor_type VARCHAR(50) NOT NULL,         -- 'service', 'external', 'user'
        actor_id VARCHAR(255) NOT NULL,          -- 'gateway-service', 'aws-cloudwatch'
        actor_ip INET,                           -- IP address of actor (optional)

        -- ========================================
        -- RESOURCE INFORMATION (What)
        -- ========================================
        resource_type VARCHAR(100) NOT NULL,     -- 'Signal', 'RemediationRequest'
        resource_id VARCHAR(255) NOT NULL,       -- 'fp-abc123', 'rr-2025-001'
        resource_name VARCHAR(255),              -- Human-readable resource name (optional)

        -- ========================================
        -- CONTEXT INFORMATION (Where/Why)
        -- ========================================
        correlation_id VARCHAR(255) NOT NULL,    -- remediation_id (groups related events)
        parent_event_id UUID,                    -- Links to parent event (optional)
        parent_event_date DATE,                  -- Parent event date (required for FK on partitioned table)
        trace_id VARCHAR(255),                   -- OpenTelemetry trace ID (optional)
        span_id VARCHAR(255),                    -- OpenTelemetry span ID (optional)

        -- ========================================
        -- KUBERNETES CONTEXT
        -- ========================================
        namespace VARCHAR(253),                  -- Kubernetes namespace (optional)
        cluster_name VARCHAR(255),               -- Cluster identifier (optional)

        -- ========================================
        -- EVENT PAYLOAD (JSONB - flexible, queryable)
        -- ========================================
        event_data JSONB NOT NULL,               -- Service-specific payload (common envelope format)
        event_metadata JSONB,                    -- Additional metadata (optional)

        -- ========================================
        -- AUDIT METADATA
        -- ========================================
        severity VARCHAR(20),                    -- 'info', 'warning', 'error', 'critical'
        duration_ms INTEGER,                     -- Operation duration in milliseconds
        error_code VARCHAR(50),                  -- Error code if outcome is failure
        error_message TEXT,                      -- Detailed error message

        -- ========================================
        -- COMPLIANCE
        -- ========================================
        retention_days INTEGER DEFAULT 2555,     -- 7 years (SOC 2 / ISO 27001)
        is_sensitive BOOLEAN DEFAULT FALSE       -- Flag for sensitive data (GDPR, PII)

    ) PARTITION BY RANGE (event_date);

    -- ========================================
    -- INDEXES (ADR-034 Standard Indexes)
    -- ========================================

    -- Index 1: Event timestamp (for time-range queries)
    CREATE INDEX IF NOT EXISTS idx_audit_events_event_timestamp
        ON audit_events (event_timestamp DESC);

    -- Index 2: Correlation ID (for cross-service correlation - most common query)
    CREATE INDEX IF NOT EXISTS idx_audit_events_correlation_id
        ON audit_events (correlation_id, event_timestamp DESC);

    -- Index 3: Event type (for filtering by event type)
    CREATE INDEX IF NOT EXISTS idx_audit_events_event_type
        ON audit_events (event_type, event_timestamp DESC);

    -- Index 4: Resource composite index (for resource-specific queries)
    CREATE INDEX IF NOT EXISTS idx_audit_events_resource
        ON audit_events (resource_type, resource_id, event_timestamp DESC);

    -- Index 5: Actor composite index (for actor-specific audit trails)
    CREATE INDEX IF NOT EXISTS idx_audit_events_actor
        ON audit_events (actor_type, actor_id, event_timestamp DESC);

    -- Index 6: Outcome (for success/failure analytics)
    CREATE INDEX IF NOT EXISTS idx_audit_events_outcome
        ON audit_events (event_outcome, event_timestamp DESC);

    -- Index 7: Event date (for partition pruning optimization)
    CREATE INDEX IF NOT EXISTS idx_audit_events_event_date
        ON audit_events (event_date);

    -- Index 8: GIN index for JSONB queries (1% of query volume, <500ms target)
    CREATE INDEX IF NOT EXISTS idx_audit_events_event_data_gin
        ON audit_events USING GIN (event_data);

    -- +goose StatementEnd

    -- +goose StatementBegin
    -- ========================================
    -- TRIGGER: Auto-populate event_date from event_timestamp
    -- ========================================

    -- Trigger function to set event_date from event_timestamp
    CREATE OR REPLACE FUNCTION set_audit_event_date()
    RETURNS TRIGGER AS $$
    BEGIN
        NEW.event_date := NEW.event_timestamp::DATE;
        RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;
    -- +goose StatementEnd

    -- +goose StatementBegin
    -- Trigger to auto-populate event_date before INSERT
    CREATE TRIGGER trg_set_audit_event_date
        BEFORE INSERT ON audit_events
        FOR EACH ROW
        EXECUTE FUNCTION set_audit_event_date();
    -- +goose StatementEnd

    -- ========================================
    -- FOREIGN KEY CONSTRAINT (Parent-Child Relationships)
    -- ========================================
    -- +goose StatementBegin

    -- Self-referencing FK with ON DELETE RESTRICT (enforces event sourcing immutability)
    -- Rationale: Prevents accidental deletion of parent events with children
    -- Requires: parent_event_date column (added 2025-11-18, see ADR-034 update)
    -- PostgreSQL Requirement: FK constraints on partitioned tables must include partition key
    ALTER TABLE audit_events
        ADD CONSTRAINT fk_audit_events_parent
        FOREIGN KEY (parent_event_id, parent_event_date)
        REFERENCES audit_events(event_id, event_date)
        ON DELETE RESTRICT;
    -- +goose StatementEnd

    -- ========================================
    -- INITIAL PARTITIONS (Current month + 3 future months)
    -- ========================================

    -- Create partitions dynamically based on current date
    -- Note: In production, use create_audit_events_partitions.sh for automation

    DO $$
    DECLARE
        current_month DATE := DATE_TRUNC('month', CURRENT_DATE);
        partition_start DATE;
        partition_end DATE;
        partition_name TEXT;
        i INT;
    BEGIN
        -- Create 4 partitions: current month + 3 future months
        FOR i IN 0..3 LOOP
            partition_start := current_month + (i || ' months')::INTERVAL;
            partition_end := current_month + ((i + 1) || ' months')::INTERVAL;
            partition_name := 'audit_events_' || TO_CHAR(partition_start, 'YYYY_MM');

            -- Create partition if it doesn't exist
            EXECUTE format(
                'CREATE TABLE IF NOT EXISTS %I PARTITION OF audit_events FOR VALUES FROM (%L) TO (%L)',
                partition_name,
                partition_start,
                partition_end
            );

            RAISE NOTICE 'Created partition: % for range [%, %)', partition_name, partition_start, partition_end;
        END LOOP;
    END $$;

    -- ========================================
    -- COMMENTS FOR DOCUMENTATION
    -- ========================================

    COMMENT ON TABLE audit_events IS
    'ADR-034: Unified audit trail for all Kubernaut services. Event sourcing pattern (immutable, append-only).';

    COMMENT ON COLUMN audit_events.event_date IS
    'Generated column from event_timestamp. Used for monthly range partitioning.';

    COMMENT ON COLUMN audit_events.correlation_id IS
    'Links events across services for complete remediation timeline (e.g., rr-2025-001).';

    COMMENT ON COLUMN audit_events.parent_event_id IS
    'Parent event for causality tracking. FK constraint enforces immutability with ON DELETE RESTRICT.';

    COMMENT ON COLUMN audit_events.parent_event_date IS
    'Parent event date (partition key). Required for FK constraint on partitioned tables (PostgreSQL requirement).';

    COMMENT ON COLUMN audit_events.event_data IS
    'JSONB payload with common envelope + service-specific data. GIN index for path queries.';

    COMMENT ON COLUMN audit_events.is_sensitive IS
    'GDPR compliance: Flag for sensitive data requiring special retention handling.';

    -- +goose StatementBegin
    -- ========================================
    -- DEFAULT PARTITION
    -- Catches any row outside defined monthly ranges (e.g., backdated test data).
    -- Monthly partitions for Mar 2026Dec 2028 are created by migrations 014/1000/030.
    -- ========================================

    CREATE TABLE IF NOT EXISTS audit_events_default
        PARTITION OF audit_events
        DEFAULT;
    -- +goose StatementEnd

    -- +goose StatementBegin
    -- ========================================
    -- GRANT PERMISSIONS (Event Sourcing: SELECT + INSERT only)
    -- Rationale: No UPDATE or DELETE to enforce immutability
    -- ========================================

    -- Grant SELECT and INSERT to datastorage application user
    -- Note: Assumes 'datastorage_app' role exists (created in earlier migrations)
    DO $$
    BEGIN
        IF EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'datastorage_app') THEN
            GRANT SELECT, INSERT ON audit_events TO datastorage_app;
            REVOKE UPDATE, DELETE ON audit_events FROM datastorage_app;
            RAISE NOTICE 'Granted SELECT/INSERT, revoked UPDATE/DELETE on audit_events for datastorage_app';
        ELSE
            RAISE NOTICE 'Role datastorage_app does not exist, skipping grants';
        END IF;
    END $$;
    -- +goose StatementEnd

    -- ========================================
    -- MIGRATION COMPLETE
    -- ========================================

    -- +goose Down
    -- +goose StatementBegin
    -- ========================================
    -- ROLLBACK: Drop audit_events table and all partitions
    -- ========================================

    DROP TABLE IF EXISTS audit_events CASCADE;

    -- Note: CASCADE will automatically drop all partitions
    -- Note: This is a destructive operation - audit data will be lost

    -- +goose StatementEnd

  014_create_audit_events_partitions.sql: |
    -- +goose Up
    -- Issue #234: Partitions starting from March 2026 (first release month) through December 2028

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-04-01') TO ('2026-05-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-05-01') TO ('2026-06-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-06-01') TO ('2026-07-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-07-01') TO ('2026-08-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-08-01') TO ('2026-09-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-09-01') TO ('2026-10-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-10-01') TO ('2026-11-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-11-01') TO ('2026-12-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-12-01') TO ('2027-01-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_01
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-01-01') TO ('2027-02-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_02
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-02-01') TO ('2027-03-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-03-01') TO ('2027-04-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-04-01') TO ('2027-05-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-05-01') TO ('2027-06-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-06-01') TO ('2027-07-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-07-01') TO ('2027-08-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-08-01') TO ('2027-09-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-09-01') TO ('2027-10-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-10-01') TO ('2027-11-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-11-01') TO ('2027-12-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-12-01') TO ('2028-01-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_01
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-01-01') TO ('2028-02-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_02
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-02-01') TO ('2028-03-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-03-01') TO ('2028-04-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-04-01') TO ('2028-05-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-05-01') TO ('2028-06-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-06-01') TO ('2028-07-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-07-01') TO ('2028-08-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-08-01') TO ('2028-09-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-09-01') TO ('2028-10-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-10-01') TO ('2028-11-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-11-01') TO ('2028-12-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-12-01') TO ('2029-01-01');
    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    DROP TABLE IF EXISTS audit_events_2026_03;
    DROP TABLE IF EXISTS audit_events_2026_04;
    DROP TABLE IF EXISTS audit_events_2026_05;
    DROP TABLE IF EXISTS audit_events_2026_06;
    DROP TABLE IF EXISTS audit_events_2026_07;
    DROP TABLE IF EXISTS audit_events_2026_08;
    DROP TABLE IF EXISTS audit_events_2026_09;
    DROP TABLE IF EXISTS audit_events_2026_10;
    DROP TABLE IF EXISTS audit_events_2026_11;
    DROP TABLE IF EXISTS audit_events_2026_12;
    DROP TABLE IF EXISTS audit_events_2027_01;
    DROP TABLE IF EXISTS audit_events_2027_02;
    DROP TABLE IF EXISTS audit_events_2027_03;
    DROP TABLE IF EXISTS audit_events_2027_04;
    DROP TABLE IF EXISTS audit_events_2027_05;
    DROP TABLE IF EXISTS audit_events_2027_06;
    DROP TABLE IF EXISTS audit_events_2027_07;
    DROP TABLE IF EXISTS audit_events_2027_08;
    DROP TABLE IF EXISTS audit_events_2027_09;
    DROP TABLE IF EXISTS audit_events_2027_10;
    DROP TABLE IF EXISTS audit_events_2027_11;
    DROP TABLE IF EXISTS audit_events_2027_12;
    DROP TABLE IF EXISTS audit_events_2028_01;
    DROP TABLE IF EXISTS audit_events_2028_02;
    DROP TABLE IF EXISTS audit_events_2028_03;
    DROP TABLE IF EXISTS audit_events_2028_04;
    DROP TABLE IF EXISTS audit_events_2028_05;
    DROP TABLE IF EXISTS audit_events_2028_06;
    DROP TABLE IF EXISTS audit_events_2028_07;
    DROP TABLE IF EXISTS audit_events_2028_08;
    DROP TABLE IF EXISTS audit_events_2028_09;
    DROP TABLE IF EXISTS audit_events_2028_10;
    DROP TABLE IF EXISTS audit_events_2028_11;
    DROP TABLE IF EXISTS audit_events_2028_12;
    -- +goose StatementEnd
  015_create_workflow_catalog_table.sql: |
    -- +goose Up
    -- +goose StatementBegin

    -- ========================================
    -- MIGRATION 015: Remediation Workflow Catalog (V1.0 - Label-Only)
    -- ========================================
    -- Authority: DD-STORAGE-008 v2.0 (Workflow Catalog Schema)
    -- Business Requirement: BR-STORAGE-012 (Workflow Catalog - Label-Only Search in V1.0)
    -- Design Decision: DD-NAMING-001 (Remediation Workflow Terminology)
    -- Immutability: DD-WORKFLOW-012 (Workflow Immutability Constraints)
    -- NOTE: V1.0 uses label-only search (deterministic), semantic search removed
    -- ========================================
    --
    -- Purpose: Create remediation_workflow_catalog table for label-based workflow matching
    --
    -- Key Features:
    -- 1. Composite primary key (workflow_id, version) for immutability (DD-WORKFLOW-012)
    -- 2. JSONB labels for flexible filtering (mandatory + detected labels)
    -- 3. Lifecycle management (active/disabled/deprecated/archived)
    -- 4. Version management with history tracking
    -- 5. Success metrics tracking
    --
    -- IMMUTABILITY (DD-WORKFLOW-012):
    -- - PRIMARY KEY (workflow_id, version) enforces immutability
    -- - Content fields (description, content, labels) CANNOT be updated
    -- - Lifecycle fields (status, metrics) CAN be updated
    -- - To change content, create a new version
    --
    -- ========================================

    -- Create remediation_workflow_catalog table
    CREATE TABLE remediation_workflow_catalog (
        -- ========================================
        -- IDENTITY (Composite Primary Key)
        -- ========================================
        workflow_id VARCHAR(255) NOT NULL,
        version VARCHAR(50) NOT NULL,           -- MUST be semantic version (e.g., v1.0.0, v1.2.3)

        -- ========================================
        -- METADATA
        -- ========================================
        name VARCHAR(255) NOT NULL,
        description TEXT NOT NULL,
        owner VARCHAR(255),                      -- Team or user responsible
        maintainer VARCHAR(255),                 -- Contact email

        -- ========================================
        -- CONTENT
        -- ========================================
        content TEXT NOT NULL,                   -- Full workflow YAML/JSON (Tekton Pipeline, Ansible Playbook, etc.)
        content_hash VARCHAR(64) NOT NULL,       -- SHA-256 hash for integrity

        -- ========================================
        -- LABELS (JSONB for flexible filtering)
        -- ========================================
        -- DD-CONTEXT-005: Filter Before LLM pattern
        -- V1.0: Label-only matching (deterministic, no semantic search)
        -- Examples:
        -- {
        --   "signal_types": ["MemoryLeak", "OOMKilled"],
        --   "business_category": "payments",
        --   "risk_tolerance": "low",
        --   "environment": "production",
        --   "detected_labels": {
        --     "gitOpsTool": "argocd",
        --     "pdbProtected": "true"
        --   }
        -- }
        labels JSONB NOT NULL DEFAULT '{}'::jsonb,

        -- ========================================
        -- LIFECYCLE MANAGEMENT
        -- ========================================
        -- User Requirement: "disable workflows and keep historical versions"
        status VARCHAR(20) NOT NULL DEFAULT 'active',  -- 'active', 'disabled', 'deprecated', 'archived'
        disabled_at TIMESTAMP WITH TIME ZONE,
        disabled_by VARCHAR(255),
        disabled_reason TEXT,

        -- ========================================
        -- VERSION MANAGEMENT
        -- ========================================
        -- User Requirement: traceability + immutability
        is_latest_version BOOLEAN NOT NULL DEFAULT false,
        previous_version VARCHAR(50),            -- Link to previous version
        deprecation_notice TEXT,                 -- Reason for deprecation

        -- ========================================
        -- VERSION CHANGE METADATA
        -- ========================================
        -- DD-STORAGE-008: Version validation & traceability
        version_notes TEXT,                      -- Release notes / changelog
        change_summary TEXT,                     -- Auto-generated summary of changes
        approved_by VARCHAR(255),                -- Who approved this version
        approved_at TIMESTAMP WITH TIME ZONE,    -- When was this version approved

        -- ========================================
        -- SUCCESS METRICS (ADR-033)
        -- ========================================
        expected_success_rate DECIMAL(4,3),      -- Expected success rate (0.000-1.000)
        expected_duration_seconds INTEGER,       -- Expected execution time
        actual_success_rate DECIMAL(4,3),        -- Calculated from execution history
        total_executions INTEGER DEFAULT 0,      -- Number of times executed
        successful_executions INTEGER DEFAULT 0, -- Number of successful executions

        -- ========================================
        -- AUDIT TRAIL
        -- ========================================
        created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
        created_by VARCHAR(255),
        updated_by VARCHAR(255),

        -- ========================================
        -- CONSTRAINTS
        -- ========================================
        -- IMMUTABILITY ENFORCEMENT (DD-WORKFLOW-012)
        -- This PRIMARY KEY constraint is the database-level enforcement mechanism
        -- for workflow immutability. Once a (workflow_id, version) pair is created,
        -- it cannot be overwritten. To change workflow content, create a new version.
        PRIMARY KEY (workflow_id, version),      -- IMMUTABILITY: Cannot overwrite existing version (DD-WORKFLOW-012)
        CHECK (status IN ('active', 'disabled', 'deprecated', 'archived')),
        CHECK (expected_success_rate IS NULL OR (expected_success_rate >= 0 AND expected_success_rate <= 1)),
        CHECK (actual_success_rate IS NULL OR (actual_success_rate >= 0 AND actual_success_rate <= 1)),
        CHECK (total_executions >= 0),
        CHECK (successful_executions >= 0 AND successful_executions <= total_executions)
    );

    -- ========================================
    -- INDEXES FOR QUERY PERFORMANCE
    -- ========================================

    -- Index for status filtering (most common query)
    CREATE INDEX idx_workflow_catalog_status
        ON remediation_workflow_catalog(status);

    -- Index for latest version queries (critical for workflow search)
    CREATE INDEX idx_workflow_catalog_latest
        ON remediation_workflow_catalog(workflow_id, is_latest_version)
        WHERE is_latest_version = true;

    -- GIN index for JSONB label filtering (DD-CONTEXT-005)
    -- V1.0: Primary search mechanism (label matching with wildcard weighting)
    CREATE INDEX idx_workflow_catalog_labels
        ON remediation_workflow_catalog USING GIN (labels);

    -- Index for created_at (for sorting by newest)
    CREATE INDEX idx_workflow_catalog_created_at
        ON remediation_workflow_catalog(created_at DESC);

    -- Index for success rate (for filtering high-performing workflows)
    CREATE INDEX idx_workflow_catalog_success_rate
        ON remediation_workflow_catalog(actual_success_rate DESC)
        WHERE status = 'active';

    -- ========================================
    -- TRIGGER FOR updated_at
    -- ========================================
    CREATE OR REPLACE FUNCTION update_workflow_catalog_updated_at()
    RETURNS TRIGGER AS $$
    BEGIN
        NEW.updated_at = NOW();
        RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;

    CREATE TRIGGER trigger_workflow_catalog_updated_at
        BEFORE UPDATE ON remediation_workflow_catalog
        FOR EACH ROW
        EXECUTE FUNCTION update_workflow_catalog_updated_at();

    -- ========================================
    -- COMMENTS FOR DOCUMENTATION
    -- ========================================
    COMMENT ON TABLE remediation_workflow_catalog IS 'Remediation workflow catalog for label-based search and version management (DD-STORAGE-008 v2.0, V1.0 label-only)';
    COMMENT ON COLUMN remediation_workflow_catalog.workflow_id IS 'Unique workflow identifier (e.g., "pod-oom-recovery")';
    COMMENT ON COLUMN remediation_workflow_catalog.version IS 'Semantic version (e.g., "v1.0.0", "v1.2.3")';
    COMMENT ON COLUMN remediation_workflow_catalog.labels IS 'JSONB labels for filtering (mandatory + detected labels with wildcard support)';
    COMMENT ON COLUMN remediation_workflow_catalog.status IS 'Lifecycle status: active, disabled, deprecated, archived';
    COMMENT ON COLUMN remediation_workflow_catalog.is_latest_version IS 'Flag indicating if this is the latest version of the workflow';

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    -- Drop table and all dependent objects
    DROP TABLE IF EXISTS remediation_workflow_catalog CASCADE;

    -- Drop trigger function
    DROP FUNCTION IF EXISTS update_workflow_catalog_updated_at() CASCADE;

    -- +goose StatementEnd
  017_add_workflow_schema_fields.sql: |+
    -- +goose Up
    -- +goose StatementBegin

    -- ========================================
    -- MIGRATION 017: Add ADR-043 Workflow Schema Fields
    -- ========================================
    -- Authority: ADR-043 (Workflow Schema Definition Standard)
    -- Business Requirement: BR-STORAGE-012 (Workflow Semantic Search)
    -- Design Decision: DD-WORKFLOW-005 (Automated Schema Extraction)
    -- ========================================
    --
    -- Purpose: Add columns for ADR-043 workflow schema support
    --
    -- New Columns:
    -- 1. parameters (JSONB) - Rich parameter definitions for LLM guidance
    -- 2. execution_engine (VARCHAR) - Execution engine type (default: 'tekton')
    -- 3. execution_bundle (TEXT) - OCI bundle reference (V1.1+)
    --
    -- V1.0 Behavior:
    -- - parameters: Extracted from workflow-schema.yaml content at creation time
    -- - execution_engine: Always 'tekton' (default)
    -- - execution_bundle: NULL (not used in V1.0)
    --
    -- V1.1+ Behavior:
    -- - WorkflowRegistration CRD controller extracts schema from container
    -- - execution_bundle: OCI bundle URL from container image
    --
    -- ========================================

    -- Add parameters column (JSONB for rich parameter definitions)
    -- ADR-043: Parameters include name, type, required, description, enum, pattern, min/max
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN parameters JSONB;

    -- Add execution_engine column (default 'tekton' for V1.0)
    -- ADR-043: V1 values: "tekton", V2 values: "tekton", "ansible", "lambda", "shell"
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN execution_engine VARCHAR(50) NOT NULL DEFAULT 'tekton';

    -- Add execution_bundle column (OCI bundle reference for V1.1+)
    -- ADR-043: Container image or bundle reference
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN execution_bundle TEXT;

    -- ========================================
    -- COMMENTS FOR DOCUMENTATION
    -- ========================================
    COMMENT ON COLUMN remediation_workflow_catalog.parameters IS 'ADR-043: Rich parameter definitions for LLM guidance (JSONB array of {name, type, required, description, enum, pattern, min, max, default})';
    COMMENT ON COLUMN remediation_workflow_catalog.execution_engine IS 'ADR-043: Execution engine type (tekton, ansible, lambda, shell). Default: tekton';
    COMMENT ON COLUMN remediation_workflow_catalog.execution_bundle IS 'ADR-043: OCI bundle or execution reference (V1.1+). NULL for V1.0';

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    -- Remove ADR-043 columns
    ALTER TABLE remediation_workflow_catalog
    DROP COLUMN IF EXISTS parameters;

    ALTER TABLE remediation_workflow_catalog
    DROP COLUMN IF EXISTS execution_engine;

    ALTER TABLE remediation_workflow_catalog
    DROP COLUMN IF EXISTS execution_bundle;

    -- +goose StatementEnd

  018_rename_execution_bundle_to_container_image.sql: |+
    -- +goose Up
    -- +goose StatementBegin

    -- ========================================
    -- MIGRATION 018: Rename execution_bundle to container_image, Add container_digest
    -- ========================================
    -- Authority: DD-WORKFLOW-002 v2.4 (MCP Workflow Catalog Architecture)
    -- Authority: DD-CONTRACT-001 v1.2 (AIAnalysis  WorkflowExecution Alignment)
    -- Business Requirement: BR-STORAGE-012 (Workflow Semantic Search)
    -- ========================================
    --
    -- Purpose: Align database schema with API contract terminology
    --
    -- Changes:
    -- 1. Rename execution_bundle  container_image (align with DD-WORKFLOW-002)
    -- 2. Add container_digest column for audit trail
    --
    -- V1.0 Behavior:
    -- - container_image: Base image with optional tag (e.g., quay.io/org/img:v1.0.0)
    -- - container_digest: SHA256 digest (e.g., sha256:abc123...)
    -- - Full pullspec = container_image + "@" + container_digest
    -- - Both fields MANDATORY for V1.0 (digest must be provided in input pullspec)
    --
    -- V1.1+ Behavior:
    -- - Accept tag-only pullspec, system resolves digest from registry
    --
    -- ========================================

    -- Rename execution_bundle to container_image
    -- This aligns with DD-WORKFLOW-002 v2.4 and DD-CONTRACT-001 v1.2 terminology
    ALTER TABLE remediation_workflow_catalog
    RENAME COLUMN execution_bundle TO container_image;

    -- Add container_digest column for audit trail and immutability
    -- Format: sha256:64_hex_characters (71 chars total)
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN container_digest VARCHAR(71);

    -- Create index for digest lookups (audit trail queries)
    CREATE INDEX idx_workflow_catalog_container_digest
        ON remediation_workflow_catalog(container_digest)
        WHERE container_digest IS NOT NULL;

    -- ========================================
    -- COMMENTS FOR DOCUMENTATION
    -- ========================================
    COMMENT ON COLUMN remediation_workflow_catalog.container_image IS 'DD-WORKFLOW-002 v2.4: OCI image reference (base + optional tag). Full pullspec = container_image@container_digest';
    COMMENT ON COLUMN remediation_workflow_catalog.container_digest IS 'DD-WORKFLOW-002 v2.4: SHA256 digest for audit trail and immutability (e.g., sha256:abc123...)';

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    -- Drop the digest index
    DROP INDEX IF EXISTS idx_workflow_catalog_container_digest;

    -- Remove container_digest column
    ALTER TABLE remediation_workflow_catalog
    DROP COLUMN IF EXISTS container_digest;

    -- Rename back to execution_bundle
    ALTER TABLE remediation_workflow_catalog
    RENAME COLUMN container_image TO execution_bundle;

    -- +goose StatementEnd



  019_uuid_primary_key.sql: |+
    -- +goose Up
    -- +goose StatementBegin

    -- ========================================
    -- MIGRATION 019: UUID Primary Key for Workflow Catalog
    -- ========================================
    -- Authority: DD-WORKFLOW-002 v3.0 (UUID Primary Key)
    -- Authority: DD-WORKFLOW-012 v2.0 (Workflow Immutability - UUID)
    -- Business Requirement: BR-STORAGE-012 (Workflow Semantic Search)
    -- ========================================
    --
    -- Purpose: Change workflow_id from VARCHAR to UUID (auto-generated)
    --
    -- Key Changes:
    -- 1. workflow_id becomes UUID PRIMARY KEY (auto-generated)
    -- 2. Add workflow_name column (human-readable identifier)
    -- 3. UNIQUE constraint on (workflow_name, version) to prevent duplicates
    -- 4. Update indexes for new schema
    --
    -- IMMUTABILITY (DD-WORKFLOW-012 v2.0):
    -- - workflow_id (UUID) is the single immutable identifier
    -- - workflow_name + version provides human-readable uniqueness
    -- - Content fields remain immutable
    --
    -- ========================================

    -- Enable uuid-ossp extension for UUID generation
    CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

    -- Step 1: Add new workflow_name column
    ALTER TABLE remediation_workflow_catalog
        ADD COLUMN workflow_name VARCHAR(255);

    -- Step 2: Copy existing workflow_id to workflow_name (for existing data)
    UPDATE remediation_workflow_catalog
    SET workflow_name = workflow_id;

    -- Step 3: Make workflow_name NOT NULL
    ALTER TABLE remediation_workflow_catalog
        ALTER COLUMN workflow_name SET NOT NULL;

    -- Step 4: Drop existing primary key constraint
    ALTER TABLE remediation_workflow_catalog
        DROP CONSTRAINT remediation_workflow_catalog_pkey;

    -- Step 5: Drop existing indexes that reference workflow_id
    DROP INDEX IF EXISTS idx_workflow_catalog_latest;

    -- Step 6: Create temporary column for new UUID
    ALTER TABLE remediation_workflow_catalog
        ADD COLUMN new_workflow_id UUID DEFAULT uuid_generate_v4();

    -- Step 7: Generate UUIDs for existing rows
    UPDATE remediation_workflow_catalog
    SET new_workflow_id = uuid_generate_v4();

    -- Step 8: Drop old workflow_id column
    ALTER TABLE remediation_workflow_catalog
        DROP COLUMN workflow_id;

    -- Step 9: Rename new_workflow_id to workflow_id
    ALTER TABLE remediation_workflow_catalog
        RENAME COLUMN new_workflow_id TO workflow_id;

    -- Step 10: Set workflow_id as NOT NULL
    ALTER TABLE remediation_workflow_catalog
        ALTER COLUMN workflow_id SET NOT NULL;

    -- Step 11: Remove default (UUIDs should be generated by application or trigger)
    ALTER TABLE remediation_workflow_catalog
        ALTER COLUMN workflow_id SET DEFAULT uuid_generate_v4();

    -- Step 12: Add new primary key on workflow_id (UUID)
    ALTER TABLE remediation_workflow_catalog
        ADD PRIMARY KEY (workflow_id);

    -- Step 13: Add unique constraint on (workflow_name, version)
    ALTER TABLE remediation_workflow_catalog
        ADD CONSTRAINT uq_workflow_name_version UNIQUE (workflow_name, version);

    -- ========================================
    -- RECREATE INDEXES FOR NEW SCHEMA
    -- ========================================

    -- Index for latest version queries (by workflow_name)
    CREATE INDEX idx_workflow_catalog_latest_by_name
        ON remediation_workflow_catalog(workflow_name, is_latest_version)
        WHERE is_latest_version = true;

    -- Index for workflow_name lookups
    CREATE INDEX idx_workflow_catalog_workflow_name
        ON remediation_workflow_catalog(workflow_name);

    -- ========================================
    -- UPDATE COMMENTS
    -- ========================================
    COMMENT ON COLUMN remediation_workflow_catalog.workflow_id IS 'UUID primary key (auto-generated, DD-WORKFLOW-002 v3.0)';
    COMMENT ON COLUMN remediation_workflow_catalog.workflow_name IS 'Human-readable workflow identifier (e.g., "pod-oom-recovery")';

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    -- Reverse the migration (restore composite primary key)

    -- Step 1: Drop new indexes
    DROP INDEX IF EXISTS idx_workflow_catalog_latest_by_name;
    DROP INDEX IF EXISTS idx_workflow_catalog_workflow_name;

    -- Step 2: Drop unique constraint
    ALTER TABLE remediation_workflow_catalog
        DROP CONSTRAINT IF EXISTS uq_workflow_name_version;

    -- Step 3: Drop primary key
    ALTER TABLE remediation_workflow_catalog
        DROP CONSTRAINT remediation_workflow_catalog_pkey;

    -- Step 4: Create temporary column for old workflow_id
    ALTER TABLE remediation_workflow_catalog
        ADD COLUMN old_workflow_id VARCHAR(255);

    -- Step 5: Copy workflow_name to old_workflow_id
    UPDATE remediation_workflow_catalog
    SET old_workflow_id = workflow_name;

    -- Step 6: Drop UUID workflow_id column
    ALTER TABLE remediation_workflow_catalog
        DROP COLUMN workflow_id;

    -- Step 7: Rename old_workflow_id to workflow_id
    ALTER TABLE remediation_workflow_catalog
        RENAME COLUMN old_workflow_id TO workflow_id;

    -- Step 8: Set workflow_id as NOT NULL
    ALTER TABLE remediation_workflow_catalog
        ALTER COLUMN workflow_id SET NOT NULL;

    -- Step 9: Drop workflow_name column
    ALTER TABLE remediation_workflow_catalog
        DROP COLUMN workflow_name;

    -- Step 10: Restore composite primary key
    ALTER TABLE remediation_workflow_catalog
        ADD PRIMARY KEY (workflow_id, version);

    -- Step 11: Restore original index
    CREATE INDEX idx_workflow_catalog_latest
        ON remediation_workflow_catalog(workflow_id, is_latest_version)
        WHERE is_latest_version = true;

    -- +goose StatementEnd

  020_add_workflow_label_columns.sql: |+
    -- +goose Up
    -- +goose StatementBegin

    -- ========================================
    -- MIGRATION 020: Add DD-WORKFLOW-001 v1.6 Label Columns
    -- ========================================
    -- Authority: DD-WORKFLOW-001 v1.6 (Mandatory Workflow Label Schema)
    -- Business Requirement: BR-STORAGE-012 (Workflow Semantic Search)
    -- ========================================
    --
    -- Purpose: Add columns for DD-WORKFLOW-001 v1.6 label schema
    --
    -- New Columns:
    -- 1. custom_labels (JSONB) - Customer-defined labels for hard filtering
    -- 2. detected_labels (JSONB) - Auto-detected labels from Kubernetes resources
    --
    -- DD-WORKFLOW-001 v1.6 Schema:
    -- - 5 Mandatory labels: signal_type, severity, component, environment, priority
    --   (These are stored in the existing 'labels' JSONB column)
    -- - 9 Detected labels: git_ops_managed, pdb_protected, hpa_enabled, stateful,
    --   helm_managed, network_isolated, git_ops_tool, pod_security_level, service_mesh
    -- - Custom labels: map[subdomain][]string format
    --
    -- ========================================

    -- Add custom_labels column (JSONB for customer-defined labels)
    -- DD-WORKFLOW-001 v1.5: Subdomain-based extraction design
    -- Format: {"constraint": ["cost-constrained"], "team": ["name=payments"]}
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN custom_labels JSONB NOT NULL DEFAULT '{}'::jsonb;

    -- Add detected_labels column (JSONB for auto-detected labels)
    -- DD-WORKFLOW-001 v1.6: 9 auto-detected fields
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN detected_labels JSONB NOT NULL DEFAULT '{}'::jsonb;

    -- ========================================
    -- INDEXES FOR QUERY PERFORMANCE
    -- ========================================

    -- GIN index for custom_labels filtering
    CREATE INDEX idx_workflow_catalog_custom_labels
        ON remediation_workflow_catalog USING GIN (custom_labels);

    -- GIN index for detected_labels filtering
    CREATE INDEX idx_workflow_catalog_detected_labels
        ON remediation_workflow_catalog USING GIN (detected_labels);

    -- ========================================
    -- COMMENTS FOR DOCUMENTATION
    -- ========================================
    COMMENT ON COLUMN remediation_workflow_catalog.custom_labels IS 'DD-WORKFLOW-001 v1.5: Customer-defined labels for hard filtering. Format: map[subdomain][]string';
    COMMENT ON COLUMN remediation_workflow_catalog.detected_labels IS 'DD-WORKFLOW-001 v1.6: Auto-detected labels from Kubernetes resources. 9 fields with wildcard support';

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    -- Remove DD-WORKFLOW-001 v1.6 columns
    DROP INDEX IF EXISTS idx_workflow_catalog_custom_labels;
    DROP INDEX IF EXISTS idx_workflow_catalog_detected_labels;

    ALTER TABLE remediation_workflow_catalog
    DROP COLUMN IF EXISTS custom_labels;

    ALTER TABLE remediation_workflow_catalog
    DROP COLUMN IF EXISTS detected_labels;

    -- +goose StatementEnd

  021_create_notification_audit_table.sql: |
    -- +goose Up
    -- +goose StatementBegin

    -- 
    -- Migration 021: Create notification_audit Table
    -- 
    --
    -- Authority:
    --   - docs/services/crd-controllers/06-notification/database-integration.md
    --   - pkg/datastorage/models/notification_audit.go
    --   - pkg/datastorage/repository/notification_audit_repository.go
    --
    -- Business Requirements:
    --   - BR-NOT-062: Notification Audit Persistence
    --   - BR-NOT-063: Notification Delivery Tracking
    --   - BR-NOT-064: Notification Escalation Tracking
    --
    -- Purpose:
    --   Creates the notification_audit table for tracking notification delivery
    --   attempts, failures, and escalations. Used by the Notification Controller
    --   via Data Storage HTTP API.
    --
    -- Design Decisions:
    --   - notification_id has UNIQUE constraint (one audit per notification)
    --   - Status uses CHECK constraint for valid values only
    --   - Channel uses CHECK constraint for supported channels
    --   - Indexes on common query patterns (by notification_id, remediation_id)
    --   - Timestamps use TIMESTAMP WITH TIME ZONE for timezone awareness
    --
    -- 

    CREATE TABLE IF NOT EXISTS notification_audit (
        -- Primary Key
        id BIGSERIAL PRIMARY KEY,

        -- Identity and Relationships
        remediation_id VARCHAR(255) NOT NULL,      -- Links to RemediationRequest CRD
        notification_id VARCHAR(255) NOT NULL UNIQUE,  -- NotificationRequest CRD name (must be unique)

        -- Notification Details
        recipient VARCHAR(255) NOT NULL,           -- Email, Slack user ID, PagerDuty service, etc.
        channel VARCHAR(50) NOT NULL CHECK (channel IN ('email', 'slack', 'pagerduty', 'sms')),
        message_summary TEXT NOT NULL,             -- Short summary of notification content

        -- Delivery Status
        status VARCHAR(50) NOT NULL CHECK (status IN ('sent', 'failed', 'acknowledged', 'escalated')),
        sent_at TIMESTAMP WITH TIME ZONE NOT NULL,

        -- Delivery Details (Optional)
        delivery_status TEXT,                      -- Provider response (e.g., "200 OK", "rate_limited")
        error_message TEXT,                        -- Error details if delivery failed

        -- Escalation Tracking
        escalation_level INTEGER NOT NULL DEFAULT 0,  -- 0=initial, 1=first escalation, etc.

        -- Audit Timestamps
        created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
    );

    -- 
    -- Indexes for Common Query Patterns
    -- 

    -- Query by notification_id (most common: lookup single notification)
    CREATE INDEX IF NOT EXISTS idx_notification_audit_notification_id
    ON notification_audit(notification_id);

    -- Query by remediation_id (common: get all notifications for a remediation)
    CREATE INDEX IF NOT EXISTS idx_notification_audit_remediation_id
    ON notification_audit(remediation_id);

    -- Query by channel (analytics: delivery rates per channel)
    CREATE INDEX IF NOT EXISTS idx_notification_audit_channel
    ON notification_audit(channel);

    -- Query by status (monitoring: failed notifications)
    CREATE INDEX IF NOT EXISTS idx_notification_audit_status
    ON notification_audit(status);

    -- Query by timestamp (timeline reconstruction: DESC order for recent first)
    CREATE INDEX IF NOT EXISTS idx_notification_audit_created_at
    ON notification_audit(created_at DESC);

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    -- 
    -- Rollback: Drop notification_audit Table and Indexes
    -- 

    DROP INDEX IF EXISTS idx_notification_audit_created_at;
    DROP INDEX IF EXISTS idx_notification_audit_status;
    DROP INDEX IF EXISTS idx_notification_audit_channel;
    DROP INDEX IF EXISTS idx_notification_audit_remediation_id;
    DROP INDEX IF EXISTS idx_notification_audit_notification_id;

    DROP TABLE IF EXISTS notification_audit;

    -- +goose StatementEnd
  022_add_status_reason_column.sql: |+
    -- +goose Up
    -- +goose StatementBegin
    -- Migration: Add status_reason column to remediation_workflow_catalog
    -- Purpose: Support workflow status updates with reason tracking
    -- BR-STORAGE-016: Workflow status management
    -- Issue: Integration test failure - column "status_reason" does not exist

    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN IF NOT EXISTS status_reason TEXT;

    COMMENT ON COLUMN remediation_workflow_catalog.status_reason IS 'Reason for status change (e.g., why workflow was disabled, activated, etc.)';
    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    ALTER TABLE remediation_workflow_catalog
    DROP COLUMN IF EXISTS status_reason;
    -- +goose StatementEnd

  023_add_event_hashing.sql: |+
    -- +goose Up
    -- +goose StatementBegin
    -- ========================================
    -- GAP #9: Event Hashing (Tamper-Evidence)
    -- Migration: Add blockchain-style hash chain to audit_events
    -- SOC2 Requirement: Tamper-evident audit logs (SOC 2 Type II, NIST 800-53, Sarbanes-Oxley)
    -- Date: January 6, 2026
    -- Authority: AUDIT_V1_0_ENTERPRISE_COMPLIANCE_PLAN_DEC_18_2025.md - Day 7
    -- ========================================
    --
    -- ARCHITECTURE:
    -- 1. Blockchain-style hash chain: event_hash = SHA256(previous_event_hash + event_json)
    -- 2. Each event links to previous event in same correlation_id
    -- 3. First event in chain has previous_event_hash = '' (empty string)
    -- 4. Tampering with ANY event breaks the chain (detectable via verification API)
    --
    -- COMPLIANCE:
    -- - SOC 2 Type II: Tamper-evident audit logs (Trust Services Criteria CC8.1)
    -- - NIST 800-53: AU-9 (Protection of Audit Information)
    -- - Sarbanes-Oxley: Section 404 (Internal Controls)
    --
    -- BACKWARDS COMPATIBILITY:
    -- - Existing events: event_hash = NULL, previous_event_hash = NULL
    -- - New events: Hash calculated on INSERT
    -- - Chain starts from implementation date (2026-01-06)
    -- - No backfill required (pragmatic approach for pre-release product)
    --
    -- ========================================

    -- Step 0: Enable pgcrypto extension for digest() function
    CREATE EXTENSION IF NOT EXISTS pgcrypto;

    -- Step 1: Add event_hash column (stores SHA256 hash of this event)
    ALTER TABLE audit_events ADD COLUMN event_hash TEXT;

    -- Step 2: Add previous_event_hash column (links to previous event in chain)
    ALTER TABLE audit_events ADD COLUMN previous_event_hash TEXT;

    -- Step 3: Add comment for documentation
    COMMENT ON COLUMN audit_events.event_hash IS
    'SHA256 hash of (previous_event_hash + event_json). Blockchain-style tamper detection per Gap #9.';

    COMMENT ON COLUMN audit_events.previous_event_hash IS
    'Hash of previous event in same correlation_id. Empty string for first event. Enables chain verification.';

    -- +goose StatementEnd

    -- +goose StatementBegin
    -- ========================================
    -- INDEXES FOR HASH CHAIN VERIFICATION
    -- ========================================

    -- Index 1: event_hash lookup (for chain verification)
    -- Used by: Verification API to detect tampered events
    -- Note: CONCURRENTLY removed for E2E test compatibility (transaction-based migrations)
    --       E2E tests apply migrations in transactions for atomicity
    --       CONCURRENTLY cannot run inside transaction blocks (PostgreSQL restriction)
    --       E2E tests have empty databases, so no locking/downtime concerns
    CREATE INDEX IF NOT EXISTS idx_audit_events_hash
        ON audit_events(event_hash)
        WHERE event_hash IS NOT NULL;

    -- Index 2: previous_event_hash lookup (for chain traversal)
    -- Used by: getPreviousEventHash() to find last event in chain
    -- Note: Composite index (correlation_id, event_timestamp) already exists
    --       so we don't need a separate index on previous_event_hash alone

    -- +goose StatementEnd

    -- +goose StatementBegin
    -- ========================================
    -- ADVISORY LOCK HELPER FUNCTION
    -- ========================================

    -- Function to convert correlation_id to consistent lock ID
    -- Ensures same correlation_id always gets same lock ID
    -- Range: PostgreSQL advisory locks use bigint (8 bytes)
    CREATE OR REPLACE FUNCTION audit_event_lock_id(correlation_id_param TEXT)
    RETURNS BIGINT AS $$
    DECLARE
        hash_bytes BYTEA;
        lock_id BIGINT;
    BEGIN
        -- Hash the correlation_id to get consistent lock ID
        hash_bytes := digest(correlation_id_param, 'sha256');

        -- Convert first 8 bytes to BIGINT
        -- Note: PostgreSQL advisory locks use BIGINT (signed 64-bit)
        lock_id := ('x' || encode(substring(hash_bytes, 1, 8), 'hex'))::bit(64)::bigint;

        RETURN lock_id;
    END;
    $$ LANGUAGE plpgsql IMMUTABLE;

    COMMENT ON FUNCTION audit_event_lock_id IS
    'Gap #9: Converts correlation_id to consistent advisory lock ID. Prevents race conditions during hash chain inserts.';

    -- +goose StatementEnd

    -- ========================================
    -- MIGRATION VERIFICATION
    -- ========================================

    -- Verify columns exist
    DO $$
    BEGIN
        IF NOT EXISTS (
            SELECT 1 FROM information_schema.columns
            WHERE table_name = 'audit_events' AND column_name = 'event_hash'
        ) THEN
            RAISE EXCEPTION 'Migration failed: event_hash column not created';
        END IF;

        IF NOT EXISTS (
            SELECT 1 FROM information_schema.columns
            WHERE table_name = 'audit_events' AND column_name = 'previous_event_hash'
        ) THEN
            RAISE EXCEPTION 'Migration failed: previous_event_hash column not created';
        END IF;

        RAISE NOTICE 'Gap #9 migration successful: Hash chain columns added';
    END $$;

    -- ========================================
    -- MIGRATION COMPLETE
    -- ========================================

    -- +goose Down
    -- +goose StatementBegin
    -- ========================================
    -- ROLLBACK: Remove hash chain infrastructure
    -- ========================================

    -- Drop indexes
    DROP INDEX IF EXISTS idx_audit_events_hash;

    -- Drop helper function
    DROP FUNCTION IF EXISTS audit_event_lock_id(TEXT);

    -- Drop columns
    ALTER TABLE audit_events DROP COLUMN IF EXISTS event_hash;
    ALTER TABLE audit_events DROP COLUMN IF EXISTS previous_event_hash;

    -- Note: We do NOT drop pgcrypto extension as other migrations may use it
    -- If needed, drop manually: DROP EXTENSION IF EXISTS pgcrypto CASCADE;

    RAISE NOTICE 'Gap #9 rollback complete: Hash chain removed';

    -- +goose StatementEnd

  024_add_legal_hold.sql: "-- +goose Up\n-- +goose StatementBegin\n-- ========================================\n--
    GAP #8: Legal Hold & Retention Policies\n-- Migration: Add legal hold capability
    to audit_events\n-- SOC2/SOX Requirement: Prevent deletion of events during litigation/investigation\n--
    Date: January 6, 2026\n-- Authority: AUDIT_V1_0_ENTERPRISE_COMPLIANCE_PLAN_DEC_18_2025.md
    - Day 8\n-- ========================================\n--\n-- ARCHITECTURE:\n--
    1. Legal hold flag prevents event deletion (database-level enforcement)\n-- 2.
    Retention policies define lifecycle per event_category (7 years for SOX)\n-- 3.
    Database trigger enforces legal hold (cannot be bypassed)\n-- 4. Legal hold metadata
    captured (who, when, why)\n--\n-- COMPLIANCE:\n-- - Sarbanes-Oxley: 7-year retention
    requirement (2555 days)\n-- - HIPAA: Legal hold capability for litigation\n--
    - SOC 2 Type II: Retention policy management\n--\n-- APPROVED DECISIONS (Q1-Q4):\n--
    - Q1: correlation_id-based legal holds (entire incident flow)\n-- - Q2: legal_hold
    column in audit_events table (simple boolean)\n-- - Q3: DataStorage service cron
    for retention (deferred to v1.1)\n-- - Q4: X-User-ID header authorization\n--\n--
    ========================================\n\n-- Step 1: Add legal_hold flag to
    audit_events\nALTER TABLE audit_events ADD COLUMN legal_hold BOOLEAN DEFAULT FALSE;\n\n--
    Step 2: Add legal hold metadata columns\nALTER TABLE audit_events ADD COLUMN legal_hold_reason
    TEXT;\nALTER TABLE audit_events ADD COLUMN legal_hold_placed_by TEXT;\nALTER TABLE
    audit_events ADD COLUMN legal_hold_placed_at TIMESTAMP;\n\n-- Step 3: Create partial
    index (only TRUE values for performance)\n-- Rationale: Very few events have legal
    holds, so partial index is more efficient\nCREATE INDEX idx_audit_events_legal_hold
    ON audit_events(legal_hold) \n  WHERE legal_hold = TRUE;\n\nCOMMENT ON COLUMN
    audit_events.legal_hold IS\n'Gap #8: Legal hold flag prevents deletion during
    litigation. Enforced by database trigger.';\n\nCOMMENT ON COLUMN audit_events.legal_hold_reason
    IS\n'Gap #8: Reason for legal hold (e.g., \"Litigation: Case #2026-ABC-123\")';\n\nCOMMENT
    ON COLUMN audit_events.legal_hold_placed_by IS\n'Gap #8: User who placed legal
    hold (from X-User-ID header)';\n\nCOMMENT ON COLUMN audit_events.legal_hold_placed_at
    IS\n'Gap #8: Timestamp when legal hold was placed';\n\n-- +goose StatementEnd\n\n--
    +goose StatementBegin\n-- ========================================\n-- RETENTION
    POLICIES TABLE\n-- ========================================\n\nCREATE TABLE audit_retention_policies
    (\n    policy_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    event_category
    TEXT NOT NULL UNIQUE,  -- e.g., 'gateway', 'workflow', 'remediation'\n    retention_days
    INTEGER NOT NULL,      -- e.g., 2555 (7 years for SOX)\n    legal_hold_override
    BOOLEAN DEFAULT FALSE,  -- If TRUE, never delete even after retention\n    created_at
    TIMESTAMP NOT NULL DEFAULT NOW(),\n    updated_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\nCOMMENT
    ON TABLE audit_retention_policies IS\n'Gap #8: Retention policies per event_category.
    SOX requires 7 years (2555 days).';\n\nCOMMENT ON COLUMN audit_retention_policies.retention_days
    IS\n'Retention period in days. Default: 2555 days = 7 years (Sarbanes-Oxley requirement)';\n\nCOMMENT
    ON COLUMN audit_retention_policies.legal_hold_override IS\n'If TRUE, events in
    this category are never deleted (permanent retention)';\n\n-- Insert default SOX-compliant
    retention policies (7 years = 2555 days)\nINSERT INTO audit_retention_policies
    (event_category, retention_days) VALUES\n    ('gateway', 2555),        -- Gateway
    signal events\n    ('workflow', 2555),       -- Workflow execution events\n    ('remediation',
    2555),    -- Remediation orchestration events\n    ('analysis', 2555),       --
    AI analysis events\n    ('notification', 2555);   -- Notification events\n\n--
    +goose StatementEnd\n\n-- +goose StatementBegin\n-- ========================================\n--
    LEGAL HOLD ENFORCEMENT TRIGGER\n-- ========================================\n\n--
    Function to prevent deletion of events with legal hold\nCREATE OR REPLACE FUNCTION
    prevent_legal_hold_deletion()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF OLD.legal_hold
    = TRUE THEN\n        -- Raise exception with detailed error message for compliance
    audits\n        RAISE EXCEPTION 'Cannot delete audit event with legal hold: event_id=%,
    correlation_id=%', \n            OLD.event_id, OLD.correlation_id\n            USING
    HINT = 'Release legal hold before deletion via DELETE /api/v1/audit/legal-hold/{correlation_id}',\n
    \                 ERRCODE = '23503';  -- foreign_key_violation (closest match
    for constraint)\n    END IF;\n    RETURN OLD;\nEND;\n$$ LANGUAGE plpgsql;\n\nCOMMENT
    ON FUNCTION prevent_legal_hold_deletion IS\n'Gap #8: Database-level enforcement
    of legal hold. Cannot be bypassed by application.';\n\n-- Create trigger to enforce
    legal hold\nCREATE TRIGGER enforce_legal_hold\n    BEFORE DELETE ON audit_events\n
    \   FOR EACH ROW EXECUTE FUNCTION prevent_legal_hold_deletion();\n\n-- +goose
    StatementEnd\n\n-- +goose StatementBegin\n-- ========================================\n--
    MIGRATION VERIFICATION\n-- ========================================\n\n-- Verify
    legal_hold column exists\nDO $$\nBEGIN\n    IF NOT EXISTS (\n        SELECT 1
    FROM information_schema.columns\n        WHERE table_name = 'audit_events' AND
    column_name = 'legal_hold'\n    ) THEN\n        RAISE EXCEPTION 'Migration failed:
    legal_hold column not created';\n    END IF;\n    \n    IF NOT EXISTS (\n        SELECT
    1 FROM information_schema.tables\n        WHERE table_name = 'audit_retention_policies'\n
    \   ) THEN\n        RAISE EXCEPTION 'Migration failed: audit_retention_policies
    table not created';\n    END IF;\n    \n    IF NOT EXISTS (\n        SELECT 1
    FROM pg_trigger\n        WHERE tgname = 'enforce_legal_hold'\n    ) THEN\n        RAISE
    EXCEPTION 'Migration failed: enforce_legal_hold trigger not created';\n    END
    IF;\n    \n    RAISE NOTICE 'Gap #8 migration successful: Legal hold capability
    added';\nEND $$;\n\n-- ========================================\n-- MIGRATION
    COMPLETE\n-- ========================================\n\n-- +goose Down\n-- +goose
    StatementBegin\n-- ========================================\n-- ROLLBACK: Remove
    legal hold infrastructure\n-- ========================================\n\n-- Drop
    trigger first\nDROP TRIGGER IF EXISTS enforce_legal_hold ON audit_events;\n\n--
    Drop function\nDROP FUNCTION IF EXISTS prevent_legal_hold_deletion();\n\n-- Drop
    retention policies table\nDROP TABLE IF EXISTS audit_retention_policies;\n\n--
    Drop index\nDROP INDEX IF EXISTS idx_audit_events_legal_hold;\n\n-- Drop columns
    from audit_events\nALTER TABLE audit_events DROP COLUMN IF EXISTS legal_hold;\nALTER
    TABLE audit_events DROP COLUMN IF EXISTS legal_hold_reason;\nALTER TABLE audit_events
    DROP COLUMN IF EXISTS legal_hold_placed_by;\nALTER TABLE audit_events DROP COLUMN
    IF EXISTS legal_hold_placed_at;\n\nRAISE NOTICE 'Gap #8 rollback complete: Legal
    hold removed';\n\n-- +goose StatementEnd\n\n"
  025_action_type_taxonomy.sql: |
    -- +goose Up
    -- +goose StatementBegin
    -- Migration: Create action_type_taxonomy table and add action_type FK to workflow catalog
    -- Authority: DD-WORKFLOW-016 (Action-Type Workflow Catalog Indexing)
    -- Purpose: Enable three-step discovery protocol (list actions -> list workflows -> get workflow)
    -- BR-HAPI-017-001: Three-step tool implementation

    -- 1. Create the action_type_taxonomy table
    CREATE TABLE IF NOT EXISTS action_type_taxonomy (
        action_type TEXT PRIMARY KEY,
        description JSONB NOT NULL,
        created_at  TIMESTAMP NOT NULL DEFAULT NOW(),
        updated_at  TIMESTAMP NOT NULL DEFAULT NOW()
    );

    COMMENT ON TABLE action_type_taxonomy IS 'Curated taxonomy of remediation action types (DD-WORKFLOW-016)';
    COMMENT ON COLUMN action_type_taxonomy.action_type IS 'Action type identifier (e.g., ScaleReplicas, RestartPod)';
    COMMENT ON COLUMN action_type_taxonomy.description IS 'JSONB with fields: what, when_to_use, when_not_to_use, preconditions';

    -- 2. Seed initial taxonomy values (DD-WORKFLOW-016 V1.0 - 10 action types)
    INSERT INTO action_type_taxonomy (action_type, description) VALUES
        ('ScaleReplicas', '{"what": "Horizontally scale a workload by adjusting the replica count.", "when_to_use": "Root cause is insufficient capacity to handle current load and the workload supports horizontal scaling.", "preconditions": "Evidence of increased incoming traffic or load correlating with the resource exhaustion."}'),
        ('RestartPod', '{"what": "Kill and recreate one or more pods.", "when_to_use": "Root cause is a transient runtime state issue (corrupted cache, leaked connections, stuck threads) that a fresh process would resolve.", "preconditions": "Evidence that the issue is transient (e.g., pod was healthy before, no recent code deployment)."}'),
        ('IncreaseCPULimits', '{"what": "Increase CPU resource limits on containers.", "when_to_use": "CPU throttling is caused by resource limits being too low relative to the workload actual requirements, not by a code-level issue.", "preconditions": "Container is actively CPU-throttled (not just using high CPU), and CPU usage pattern is consistent with legitimate workload."}'),
        ('IncreaseMemoryLimits', '{"what": "Increase memory resource limits on containers.", "when_to_use": "OOM kills are caused by memory limits being too low relative to the workload actual requirements.", "preconditions": "Memory usage shows a stable pattern consistent with legitimate workload, not unbounded growth over time."}'),
        ('RollbackDeployment', '{"what": "Revert a deployment to its previous stable revision.", "when_to_use": "Root cause is a recent deployment that introduced a regression, and the previous revision was healthy.", "preconditions": "A previous healthy revision exists (verify via rollout history) and the issue started after the most recent deployment."}'),
        ('DrainNode', '{"what": "Drain and cordon a Kubernetes node, evicting all pods and preventing new scheduling.", "when_to_use": "Root cause is a node-level issue (hardware degradation, kernel problems, disk pressure) affecting multiple workloads on the node, and pods must be moved to healthy nodes.", "when_not_to_use": "Only a single pod is affected on the node. This indicates a pod-level issue, not node-level -- use a pod-targeted action instead. If pods don''t need to be evicted yet, use CordonNode instead.", "preconditions": "Confirmed that multiple workloads on the same node are affected, indicating node-scoped impact."}'),
        ('CordonNode', '{"what": "Cordon a Kubernetes node to prevent new pod scheduling without evicting existing pods.", "when_to_use": "Root cause is an emerging node-level issue that warrants preventing new pods from being scheduled, but existing pods are still running and do not need immediate eviction.", "when_not_to_use": "If existing pods on the node are already failing or need to be moved to healthy nodes, use DrainNode instead.", "preconditions": "Evidence of degrading node health (intermittent errors, rising resource pressure) but existing workloads still functional."}'),
        ('RestartDeployment', '{"what": "Perform a rolling restart of all pods in a workload (Deployment or StatefulSet).", "when_to_use": "Root cause is a workload-wide state issue affecting all or most pods, such as stale configuration, expired certificates, or corrupted shared state that requires all pods to be refreshed.", "preconditions": "Evidence that the issue affects multiple pods in the same workload (not just a single pod), and a fresh set of pods would resolve the issue."}'),
        ('CleanupNode', '{"what": "Reclaim disk space on a node by purging temporary files, old logs, and unused container images.", "when_to_use": "Node disk pressure is caused by accumulated ephemeral data (temp files, old container logs, unused images), not by legitimate workload storage growth.", "when_not_to_use": "If disk usage is from legitimate workload data (persistent volumes, application databases). Cleanup would not help and could cause data loss. Use DrainNode instead if the node needs to be decommissioned.", "preconditions": "Evidence that disk usage is dominated by ephemeral/reclaimable data (container image cache, log files, tmp directories), not persistent workload data."}'),
        ('DeletePod', '{"what": "Delete one or more specific pods without waiting for graceful termination.", "when_to_use": "Pods are stuck in a terminal state (Terminating, Unknown) and cannot be restarted through normal means.", "when_not_to_use": "Do not use as a general restart mechanism. Use RestartPod instead for transient runtime issues.", "preconditions": "Pod is genuinely stuck and not responding to graceful termination (verify via pod events and state duration)."}')
    ON CONFLICT (action_type) DO NOTHING;

    -- 3. Add action_type column to remediation_workflow_catalog
    -- Use 'ScaleReplicas' as default for existing rows (pre-release data only)
    ALTER TABLE remediation_workflow_catalog
        ADD COLUMN IF NOT EXISTS action_type TEXT;

    -- 4. Backfill existing rows with a default action type
    UPDATE remediation_workflow_catalog
    SET action_type = 'ScaleReplicas'
    WHERE action_type IS NULL;

    -- 5. Add NOT NULL constraint and FK after backfill
    ALTER TABLE remediation_workflow_catalog
        ALTER COLUMN action_type SET NOT NULL;

    ALTER TABLE remediation_workflow_catalog
        ADD CONSTRAINT fk_workflow_action_type
        FOREIGN KEY (action_type)
        REFERENCES action_type_taxonomy(action_type);

    -- 6. Add composite index for discovery queries (ListActions, ListWorkflowsByActionType)
    -- GAP-2: Replaces single-column idx_workflow_action_type with composite
    -- covering the three-column filter pattern: action_type + status + is_latest_version
    CREATE INDEX IF NOT EXISTS idx_workflow_action_type_status_version
        ON remediation_workflow_catalog(action_type, status, is_latest_version);

    -- 7. Add trigger for updated_at on taxonomy table
    DROP TRIGGER IF EXISTS trigger_action_type_taxonomy_updated_at ON action_type_taxonomy;
    CREATE TRIGGER trigger_action_type_taxonomy_updated_at
        BEFORE UPDATE ON action_type_taxonomy
        FOR EACH ROW
        EXECUTE FUNCTION update_updated_at();

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    DROP TRIGGER IF EXISTS trigger_action_type_taxonomy_updated_at ON action_type_taxonomy;
    ALTER TABLE remediation_workflow_catalog DROP CONSTRAINT IF EXISTS fk_workflow_action_type;
    DROP INDEX IF EXISTS idx_workflow_action_type_status_version;
    ALTER TABLE remediation_workflow_catalog DROP COLUMN IF EXISTS action_type;
    DROP TABLE IF EXISTS action_type_taxonomy;
    -- +goose StatementEnd
  026_demo_action_types.sql: |
    -- +goose Up
    -- +goose StatementBegin
    -- Migration: Add action types for demo remediation scenarios
    -- Authority: DD-WORKFLOW-016 (Action-Type Workflow Catalog Indexing)
    -- Purpose: Register new action types needed by demo scenario workflows (#114, #119-#130)

    INSERT INTO action_type_taxonomy (action_type, description) VALUES
        ('GitRevertCommit', '{"what": "Revert a bad commit in a Git repository managed by GitOps (ArgoCD/Flux).", "when_to_use": "Root cause is a recent Git commit that introduced a regression in a GitOps-managed environment, and the ArgoCD/Flux controller will reconcile the reverted state.", "when_not_to_use": "When the environment is not GitOps-managed. Use RollbackDeployment instead for direct kubectl-managed workloads.", "preconditions": "GitOps tooling (ArgoCD or Flux) is active, the source Git repository is accessible, and a previous healthy commit exists."}'),
        ('ProvisionNode', '{"what": "Request provisioning of a new Kubernetes node to increase cluster capacity.", "when_to_use": "Pods are Pending due to insufficient cluster-wide resources (CPU, memory) and no existing node has capacity.", "when_not_to_use": "When scheduling failures are caused by taints, affinity rules, or PDB constraints rather than resource exhaustion.", "preconditions": "Confirmed that all schedulable nodes are at or near capacity and pending pods have resource requests that cannot be satisfied."}'),
        ('GracefulRestart', '{"what": "Perform a graceful rolling restart of a workload to reset its runtime state.", "when_to_use": "Predictive analysis indicates an impending failure (memory leak, resource exhaustion) that can be prevented by proactively restarting before the crash occurs.", "when_not_to_use": "When the issue is caused by a code bug or misconfiguration that will recur after restart.", "preconditions": "Evidence of gradual resource degradation (trending metrics) and the workload supports rolling restarts without data loss."}'),
        ('CleanupPVC', '{"what": "Remove old or unnecessary files from a PersistentVolumeClaim to reclaim disk space.", "when_to_use": "PVC usage has exceeded a critical threshold due to accumulated logs, temp files, or cache data.", "when_not_to_use": "When disk usage is from essential application data. Deleting it would cause data loss.", "preconditions": "PVC is mounted and accessible, and the files to be cleaned are identified as non-essential (logs, temp, cache)."}'),
        ('RemoveTaint', '{"what": "Remove a taint from a Kubernetes node to allow pod scheduling.", "when_to_use": "Pods are Pending because the target node has a taint that prevents scheduling, and the taint is no longer necessary.", "when_not_to_use": "When the taint was applied intentionally for maintenance or isolation purposes.", "preconditions": "The taint condition has been resolved and the node is healthy for general workload scheduling."}'),
        ('PatchHPA', '{"what": "Patch a HorizontalPodAutoscaler to increase maxReplicas or adjust scaling thresholds.", "when_to_use": "HPA has scaled to maxReplicas but the workload still cannot handle the load, indicating the max ceiling is too low.", "when_not_to_use": "When the scaling issue is caused by a code-level performance regression. Fix the code instead of adding more replicas.", "preconditions": "HPA is at maxReplicas, CPU/memory utilization remains above target, and the cluster has capacity for additional replicas."}'),
        ('RelaxPDB', '{"what": "Temporarily relax a PodDisruptionBudget to unblock a pending node drain, then restore it.", "when_to_use": "A node drain is blocked because a PDB prevents evicting the required number of pods.", "when_not_to_use": "When the PDB is correctly preventing unsafe evictions and the drain should be postponed.", "preconditions": "Confirmed that the drain is intentional and the PDB is the sole blocker. The workload can tolerate temporary reduced availability."}'),
        ('ProactiveRollback', '{"what": "Proactively roll back a deployment based on predictive SLO burn rate analysis.", "when_to_use": "Predictive analysis shows the current error rate will exhaust the SLO error budget before the next maintenance window.", "when_not_to_use": "When the error rate is within normal bounds or the SLO budget has sufficient headroom.", "preconditions": "A previous stable deployment revision exists, and the error spike correlates with a recent deployment change."}'),
        ('CordonDrainNode', '{"what": "Cordon a node to prevent new scheduling, then drain existing pods to other nodes.", "when_to_use": "A node is in NotReady state or showing signs of hardware/OS-level failure, and pods need to be relocated.", "when_not_to_use": "When only a single pod is affected. Use a pod-targeted action instead.", "preconditions": "Other healthy nodes have capacity to absorb the drained pods, and the node issue is confirmed as node-scoped."}')
    ON CONFLICT (action_type) DO NOTHING;

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    DELETE FROM action_type_taxonomy WHERE action_type IN (
        'GitRevertCommit',
        'ProvisionNode',
        'GracefulRestart',
        'CleanupPVC',
        'RemoveTaint',
        'PatchHPA',
        'RelaxPDB',
        'ProactiveRollback',
        'CordonDrainNode'
    );
    -- +goose StatementEnd
  026_workflow_schema_camelcase.sql: |
    -- +goose Up
    -- +goose StatementBegin
    -- Migration 026: BR-WORKFLOW-004 schema format alignment
    -- Authority: BR-WORKFLOW-004 (Workflow Schema Format Specification)
    -- Authority: DD-WORKFLOW-017 (OCI-based Workflow Registration)
    --
    -- Changes:
    -- 1. Convert description column from TEXT to JSONB (structured description)
    -- 2. Update labels JSONB keys from snake_case to camelCase (signalType)
    -- 3. Update action_type_taxonomy description keys to camelCase

    -- 1. Convert description column from TEXT to JSONB
    -- Step 1a: Add temporary JSONB column
    ALTER TABLE remediation_workflow_catalog
        ADD COLUMN IF NOT EXISTS description_new JSONB;

    -- Step 1b: Migrate existing TEXT description to structured JSONB format
    -- Existing rows get their text description placed into the "what" field
    UPDATE remediation_workflow_catalog
    SET description_new = jsonb_build_object(
        'what', description,
        'whenToUse', ''
    )
    WHERE description_new IS NULL;

    -- Step 1c: Drop old TEXT column and rename new column
    ALTER TABLE remediation_workflow_catalog DROP COLUMN description;
    ALTER TABLE remediation_workflow_catalog RENAME COLUMN description_new TO description;
    ALTER TABLE remediation_workflow_catalog ALTER COLUMN description SET NOT NULL;
    ALTER TABLE remediation_workflow_catalog ALTER COLUMN description SET DEFAULT '{}'::jsonb;

    -- 2. Update labels JSONB: rename signal_type key to signalType
    -- Pre-release: no backwards compatibility needed
    UPDATE remediation_workflow_catalog
    SET labels = (labels - 'signal_type') || jsonb_build_object('signalType', labels->>'signal_type')
    WHERE labels ? 'signal_type';

    -- 3. Update action_type_taxonomy description keys to camelCase
    -- Convert when_to_use -> whenToUse, when_not_to_use -> whenNotToUse
    UPDATE action_type_taxonomy
    SET description = jsonb_build_object(
        'what', COALESCE(description->>'what', ''),
        'whenToUse', COALESCE(description->>'when_to_use', description->>'whenToUse', ''),
        'whenNotToUse', COALESCE(description->>'when_not_to_use', description->>'whenNotToUse', ''),
        'preconditions', COALESCE(description->>'preconditions', '')
    );

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    -- Reverse action_type_taxonomy description keys
    UPDATE action_type_taxonomy
    SET description = jsonb_build_object(
        'what', COALESCE(description->>'what', ''),
        'when_to_use', COALESCE(description->>'whenToUse', description->>'when_to_use', ''),
        'when_not_to_use', COALESCE(description->>'whenNotToUse', description->>'when_not_to_use', ''),
        'preconditions', COALESCE(description->>'preconditions', '')
    );

    -- Reverse labels JSONB: rename signalType back to signal_type
    UPDATE remediation_workflow_catalog
    SET labels = (labels - 'signalType') || jsonb_build_object('signal_type', labels->>'signalType')
    WHERE labels ? 'signalType';

    -- Convert description back from JSONB to TEXT
    ALTER TABLE remediation_workflow_catalog
        ADD COLUMN description_old TEXT;

    UPDATE remediation_workflow_catalog
    SET description_old = COALESCE(description->>'what', '');

    ALTER TABLE remediation_workflow_catalog DROP COLUMN description;
    ALTER TABLE remediation_workflow_catalog RENAME COLUMN description_old TO description;
    ALTER TABLE remediation_workflow_catalog ALTER COLUMN description SET NOT NULL;

    -- +goose StatementEnd
  027_remediation_history_indexes.sql: |
    -- +goose Up
    -- +goose StatementBegin
    -- ========================================
    -- DD-HAPI-016: Expression Indexes for Remediation History Context
    -- Migration: Add expression indexes on JSONB fields for efficient
    -- remediation history lookups.
    -- BR: BR-HAPI-016 (Remediation history context for LLM prompt enrichment)
    -- Date: February 2026
    -- ========================================
    --
    -- RATIONALE:
    -- The remediation history endpoint (GET /api/v1/remediation-history/context)
    -- queries audit events by:
    -- 1. target_resource (stored in event_data JSONB) for Tier 1 chain
    -- 2. pre_remediation_spec_hash (stored in event_data JSONB) for Tier 2 regression
    --
    -- While a GIN index exists on event_data (idx_audit_events_event_data_gin),
    -- expression (B-tree) indexes are more efficient for exact-match lookups on
    -- specific JSONB keys.
    -- ========================================

    -- Index 1: target_resource expression index
    -- Supports Tier 1 queries: find all remediation events for a specific target
    -- Format: "{namespace}/{kind}/{name}" (e.g. "prod/Deployment/my-app")
    CREATE INDEX IF NOT EXISTS idx_audit_events_target_resource
        ON audit_events ((event_data->>'target_resource'), event_timestamp DESC)
        WHERE event_type IN ('remediation.workflow_created', 'effectiveness.assessment.completed');

    -- Index 2: pre_remediation_spec_hash expression index
    -- Supports Tier 2 queries: find historical events matching the current spec hash
    -- indicating configuration regression
    CREATE INDEX IF NOT EXISTS idx_audit_events_pre_remediation_spec_hash
        ON audit_events ((event_data->>'pre_remediation_spec_hash'), event_timestamp DESC)
        WHERE event_data->>'pre_remediation_spec_hash' IS NOT NULL;

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    DROP INDEX IF EXISTS idx_audit_events_target_resource;
    DROP INDEX IF EXISTS idx_audit_events_pre_remediation_spec_hash;

    -- +goose StatementEnd
  028_rename_signaltype_to_signalname.sql: |
    -- +goose Up
    -- +goose StatementBegin
    -- Issue #166: Rename signalType -> signalName in workflow catalog JSONB labels
    -- This renames the JSONB key used for semantic signal name matching in the workflow catalog.
    UPDATE remediation_workflow_catalog
    SET labels = (labels - 'signalType') || jsonb_build_object('signalName', labels->>'signalType')
    WHERE labels ? 'signalType';
    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    -- Revert: Rename signalName -> signalType in workflow catalog JSONB labels
    UPDATE remediation_workflow_catalog
    SET labels = (labels - 'signalName') || jsonb_build_object('signalType', labels->>'signalName')
    WHERE labels ? 'signalName';
    -- +goose StatementEnd
  028_semantic_image_split.sql: |
    -- +goose Up
    -- +goose StatementBegin

    -- ========================================
    -- MIGRATION 028: Semantic split of container_image into schema_image + execution_bundle
    -- ========================================
    -- Authority: Issue #89 (Enforce digest-only references in execution.bundle)
    -- Authority: DD-WORKFLOW-017 (OCI-based Workflow Registration)
    -- ========================================
    --
    -- container_image/container_digest stored a single OCI reference with mixed semantics:
    --   - Schema extraction: the image pulled at registration to extract /workflow-schema.yaml
    --   - Execution: the Tekton bundle / Job image run at remediation time
    --
    -- This migration splits them into two distinct pairs:
    --   schema_image / schema_digest        OCI pullspec used for schema extraction
    --   execution_bundle / execution_bundle_digest  OCI bundle used at execution time (digest-pinned)
    --
    -- Pre-release: existing data in container_image is treated as schema_image (the
    -- registration pullspec). execution_bundle is populated as NULL until operators
    -- re-register workflows with the new field.

    -- Rename container_image  schema_image
    ALTER TABLE remediation_workflow_catalog
    RENAME COLUMN container_image TO schema_image;

    -- Rename container_digest  schema_digest
    ALTER TABLE remediation_workflow_catalog
    RENAME COLUMN container_digest TO schema_digest;

    -- Rename the digest index to match new column name
    ALTER INDEX IF EXISTS idx_workflow_catalog_container_digest
    RENAME TO idx_workflow_catalog_schema_digest;

    -- Add execution_bundle column (digest-pinned OCI reference for runtime)
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN execution_bundle TEXT;

    -- Add execution_bundle_digest column
    ALTER TABLE remediation_workflow_catalog
    ADD COLUMN execution_bundle_digest VARCHAR(71);

    -- Create index for execution_bundle digest lookups
    CREATE INDEX idx_workflow_catalog_execution_bundle_digest
        ON remediation_workflow_catalog(execution_bundle_digest)
        WHERE execution_bundle_digest IS NOT NULL;

    COMMENT ON COLUMN remediation_workflow_catalog.schema_image IS 'Issue #89: OCI image pulled at registration to extract /workflow-schema.yaml (DD-WORKFLOW-017)';
    COMMENT ON COLUMN remediation_workflow_catalog.schema_digest IS 'Issue #89: SHA256 digest of the schema image';
    COMMENT ON COLUMN remediation_workflow_catalog.execution_bundle IS 'Issue #89: OCI execution bundle reference (digest-pinned) for Tekton/Job runtime';
    COMMENT ON COLUMN remediation_workflow_catalog.execution_bundle_digest IS 'Issue #89: SHA256 digest portion of execution_bundle';

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin

    DROP INDEX IF EXISTS idx_workflow_catalog_execution_bundle_digest;

    ALTER TABLE remediation_workflow_catalog DROP COLUMN IF EXISTS execution_bundle_digest;
    ALTER TABLE remediation_workflow_catalog DROP COLUMN IF EXISTS execution_bundle;

    ALTER INDEX IF EXISTS idx_workflow_catalog_schema_digest
    RENAME TO idx_workflow_catalog_container_digest;

    ALTER TABLE remediation_workflow_catalog
    RENAME COLUMN schema_digest TO container_digest;

    ALTER TABLE remediation_workflow_catalog
    RENAME COLUMN schema_image TO container_image;

    -- +goose StatementEnd
  029_new_demo_action_types.sql: |
    -- +goose Up
    -- +goose StatementBegin
    -- Migration: Add action types for new demo scenarios (#133-#138)
    -- Authority: DD-WORKFLOW-016 (Action-Type Workflow Catalog Indexing)
    -- Purpose: Register action types for cert-manager, Helm, Linkerd, StatefulSet, and NetworkPolicy scenarios

    INSERT INTO action_type_taxonomy (action_type, description) VALUES
        ('FixCertificate', '{"what": "Recreate a missing or corrupted CA Secret backing a cert-manager ClusterIssuer to restore certificate issuance.", "when_to_use": "A cert-manager Certificate is stuck in NotReady because the CA Secret has been deleted or corrupted, and the ClusterIssuer cannot sign.", "when_not_to_use": "When the Certificate failure is caused by DNS validation issues, incorrect certificate spec, or an expired root CA that needs rotation rather than recreation.", "preconditions": "cert-manager is installed, the ClusterIssuer exists, and the failure is specifically due to a missing or corrupted CA Secret."}'),
        ('HelmRollback', '{"what": "Roll back a Helm release to its previous healthy revision.", "when_to_use": "A Helm-managed workload is crashing after a helm upgrade introduced a bad configuration, and the previous Helm revision was healthy.", "when_not_to_use": "When the workload is not managed by Helm (use RollbackDeployment or GracefulRestart instead), or when the crash is caused by an external dependency failure.", "preconditions": "The Helm release has at least one previous healthy revision in its history, and the Helm tiller/controller has access to roll back."}'),
        ('FixAuthorizationPolicy', '{"what": "Remove or fix a Linkerd AuthorizationPolicy that is blocking legitimate traffic.", "when_to_use": "A Linkerd-meshed workload has high error rates (403 Forbidden) because a restrictive AuthorizationPolicy is denying all unauthenticated or unrecognized traffic.", "when_not_to_use": "When the 403 errors are intentional security policy enforcement, or when the issue is caused by something other than an AuthorizationPolicy (e.g., network partition).", "preconditions": "Linkerd is installed, the workload is meshed with a Linkerd sidecar, and an AuthorizationPolicy has been identified as the traffic blocker."}'),
        ('FixStatefulSetPVC', '{"what": "Recreate a missing PVC for a StatefulSet and restart the stuck pod.", "when_to_use": "A StatefulSet pod is stuck in Pending because its PVC has been accidentally deleted or the backing PV is no longer available.", "when_not_to_use": "When the PVC exists but the storage backend itself is down (fix the storage backend instead), or when data recovery is needed before recreating.", "preconditions": "The StatefulSet exists with a volumeClaimTemplate, the missing PVC name follows the StatefulSet naming convention, and a StorageClass is available to provision a new PV."}'),
        ('FixNetworkPolicy', '{"what": "Remove a deny-all NetworkPolicy that is blocking legitimate ingress traffic and causing health check failures.", "when_to_use": "A NetworkPolicy is blocking all ingress traffic to a workload, causing liveness and readiness probe failures that lead to pod restarts or unavailability.", "when_not_to_use": "When the NetworkPolicy is intentionally restrictive for security purposes and the outage has a different root cause.", "preconditions": "A deny-all NetworkPolicy exists in the namespace, and the workload health checks are confirmed to be failing due to blocked network traffic rather than application errors."}')
    ON CONFLICT (action_type) DO NOTHING;

    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    DELETE FROM action_type_taxonomy WHERE action_type IN (
        'FixCertificate',
        'HelmRollback',
        'FixAuthorizationPolicy',
        'FixStatefulSetPVC',
        'FixNetworkPolicy'
    );
    -- +goose StatementEnd
  030_extend_partitions_2028.sql: |
    -- Extend partitions for resource_action_traces and audit_events through December 2028
    -- Migration: 030_extend_partitions_2028.sql
    -- Issue: #234  PostgreSQL partition expiry caused DS E2E failures on 2026-03-01
    --
    -- Previous partitions ended at 2026-03-01 (exclusive). This migration adds monthly
    -- partitions from March 2026 through December 2028 for both partitioned tables.
    -- Uses CREATE TABLE IF NOT EXISTS for idempotency (safe to re-run).

    -- ============================================================================
    -- DEFAULT partitions: catch rows outside defined monthly ranges
    -- ============================================================================

    CREATE TABLE IF NOT EXISTS resource_action_traces_default
        PARTITION OF resource_action_traces
        DEFAULT;

    CREATE TABLE IF NOT EXISTS audit_events_default
        PARTITION OF audit_events
        DEFAULT;

    -- ============================================================================
    -- resource_action_traces: March 2026  December 2028
    -- Naming convention: resource_action_traces_{YYYY}_{MM}
    -- ============================================================================

    -- 2026 (March  December)
    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_03
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_04
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-04-01') TO ('2026-05-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_05
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-05-01') TO ('2026-06-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_06
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-06-01') TO ('2026-07-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_07
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-07-01') TO ('2026-08-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_08
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-08-01') TO ('2026-09-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_09
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-09-01') TO ('2026-10-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_10
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-10-01') TO ('2026-11-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_11
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-11-01') TO ('2026-12-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2026_12
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2026-12-01') TO ('2027-01-01');

    -- 2027 (January  December)
    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_01
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-01-01') TO ('2027-02-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_02
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-02-01') TO ('2027-03-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_03
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-03-01') TO ('2027-04-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_04
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-04-01') TO ('2027-05-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_05
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-05-01') TO ('2027-06-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_06
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-06-01') TO ('2027-07-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_07
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-07-01') TO ('2027-08-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_08
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-08-01') TO ('2027-09-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_09
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-09-01') TO ('2027-10-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_10
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-10-01') TO ('2027-11-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_11
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-11-01') TO ('2027-12-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2027_12
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2027-12-01') TO ('2028-01-01');

    -- 2028 (January  December)
    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_01
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-01-01') TO ('2028-02-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_02
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-02-01') TO ('2028-03-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_03
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-03-01') TO ('2028-04-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_04
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-04-01') TO ('2028-05-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_05
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-05-01') TO ('2028-06-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_06
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-06-01') TO ('2028-07-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_07
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-07-01') TO ('2028-08-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_08
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-08-01') TO ('2028-09-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_09
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-09-01') TO ('2028-10-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_10
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-10-01') TO ('2028-11-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_11
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-11-01') TO ('2028-12-01');

    CREATE TABLE IF NOT EXISTS resource_action_traces_2028_12
        PARTITION OF resource_action_traces
        FOR VALUES FROM ('2028-12-01') TO ('2029-01-01');

    -- ============================================================================
    -- audit_events: March 2026  December 2028
    -- Naming convention: audit_events_{YYYY}_{MM}
    -- ============================================================================

    -- 2026 (March  December)
    CREATE TABLE IF NOT EXISTS audit_events_2026_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-04-01') TO ('2026-05-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-05-01') TO ('2026-06-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-06-01') TO ('2026-07-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-07-01') TO ('2026-08-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-08-01') TO ('2026-09-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-09-01') TO ('2026-10-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-10-01') TO ('2026-11-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-11-01') TO ('2026-12-01');

    CREATE TABLE IF NOT EXISTS audit_events_2026_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-12-01') TO ('2027-01-01');

    -- 2027 (January  December)
    CREATE TABLE IF NOT EXISTS audit_events_2027_01
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-01-01') TO ('2027-02-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_02
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-02-01') TO ('2027-03-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-03-01') TO ('2027-04-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-04-01') TO ('2027-05-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-05-01') TO ('2027-06-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-06-01') TO ('2027-07-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-07-01') TO ('2027-08-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-08-01') TO ('2027-09-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-09-01') TO ('2027-10-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-10-01') TO ('2027-11-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-11-01') TO ('2027-12-01');

    CREATE TABLE IF NOT EXISTS audit_events_2027_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-12-01') TO ('2028-01-01');

    -- 2028 (January  December)
    CREATE TABLE IF NOT EXISTS audit_events_2028_01
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-01-01') TO ('2028-02-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_02
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-02-01') TO ('2028-03-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-03-01') TO ('2028-04-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-04-01') TO ('2028-05-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-05-01') TO ('2028-06-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-06-01') TO ('2028-07-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-07-01') TO ('2028-08-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-08-01') TO ('2028-09-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-09-01') TO ('2028-10-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-10-01') TO ('2028-11-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-11-01') TO ('2028-12-01');

    CREATE TABLE IF NOT EXISTS audit_events_2028_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-12-01') TO ('2029-01-01');
  1000_create_audit_events_partitions.sql: |
    -- +goose Up
    -- Issue #234: Partitions starting from March 2026 (first release month) through December 2028

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-03-01') TO ('2026-04-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-04-01') TO ('2026-05-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-05-01') TO ('2026-06-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-06-01') TO ('2026-07-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-07-01') TO ('2026-08-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-08-01') TO ('2026-09-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-09-01') TO ('2026-10-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-10-01') TO ('2026-11-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-11-01') TO ('2026-12-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2026_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2026-12-01') TO ('2027-01-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_01
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-01-01') TO ('2027-02-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_02
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-02-01') TO ('2027-03-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-03-01') TO ('2027-04-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-04-01') TO ('2027-05-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-05-01') TO ('2027-06-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-06-01') TO ('2027-07-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-07-01') TO ('2027-08-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-08-01') TO ('2027-09-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-09-01') TO ('2027-10-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-10-01') TO ('2027-11-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-11-01') TO ('2027-12-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2027_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2027-12-01') TO ('2028-01-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_01
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-01-01') TO ('2028-02-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_02
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-02-01') TO ('2028-03-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_03
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-03-01') TO ('2028-04-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_04
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-04-01') TO ('2028-05-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_05
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-05-01') TO ('2028-06-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_06
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-06-01') TO ('2028-07-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_07
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-07-01') TO ('2028-08-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_08
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-08-01') TO ('2028-09-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_09
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-09-01') TO ('2028-10-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_10
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-10-01') TO ('2028-11-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_11
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-11-01') TO ('2028-12-01');
    -- +goose StatementEnd

    -- +goose StatementBegin
    CREATE TABLE IF NOT EXISTS audit_events_2028_12
        PARTITION OF audit_events
        FOR VALUES FROM ('2028-12-01') TO ('2029-01-01');
    -- +goose StatementEnd

    -- +goose Down
    -- +goose StatementBegin
    DROP TABLE IF EXISTS audit_events_2026_03;
    DROP TABLE IF EXISTS audit_events_2026_04;
    DROP TABLE IF EXISTS audit_events_2026_05;
    DROP TABLE IF EXISTS audit_events_2026_06;
    DROP TABLE IF EXISTS audit_events_2026_07;
    DROP TABLE IF EXISTS audit_events_2026_08;
    DROP TABLE IF EXISTS audit_events_2026_09;
    DROP TABLE IF EXISTS audit_events_2026_10;
    DROP TABLE IF EXISTS audit_events_2026_11;
    DROP TABLE IF EXISTS audit_events_2026_12;
    DROP TABLE IF EXISTS audit_events_2027_01;
    DROP TABLE IF EXISTS audit_events_2027_02;
    DROP TABLE IF EXISTS audit_events_2027_03;
    DROP TABLE IF EXISTS audit_events_2027_04;
    DROP TABLE IF EXISTS audit_events_2027_05;
    DROP TABLE IF EXISTS audit_events_2027_06;
    DROP TABLE IF EXISTS audit_events_2027_07;
    DROP TABLE IF EXISTS audit_events_2027_08;
    DROP TABLE IF EXISTS audit_events_2027_09;
    DROP TABLE IF EXISTS audit_events_2027_10;
    DROP TABLE IF EXISTS audit_events_2027_11;
    DROP TABLE IF EXISTS audit_events_2027_12;
    DROP TABLE IF EXISTS audit_events_2028_01;
    DROP TABLE IF EXISTS audit_events_2028_02;
    DROP TABLE IF EXISTS audit_events_2028_03;
    DROP TABLE IF EXISTS audit_events_2028_04;
    DROP TABLE IF EXISTS audit_events_2028_05;
    DROP TABLE IF EXISTS audit_events_2028_06;
    DROP TABLE IF EXISTS audit_events_2028_07;
    DROP TABLE IF EXISTS audit_events_2028_08;
    DROP TABLE IF EXISTS audit_events_2028_09;
    DROP TABLE IF EXISTS audit_events_2028_10;
    DROP TABLE IF EXISTS audit_events_2028_11;
    DROP TABLE IF EXISTS audit_events_2028_12;
    -- +goose StatementEnd
kind: ConfigMap
metadata:
  name: data-storage-schema-files
  namespace: kubernaut-system
