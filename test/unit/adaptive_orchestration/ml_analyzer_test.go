//go:build unit
// +build unit

package adaptive_orchestration

import (
	"github.com/jordigilh/kubernaut/pkg/testutil/mocks"

	"fmt"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	"github.com/jordigilh/kubernaut/pkg/intelligence/learning"
	"github.com/jordigilh/kubernaut/pkg/intelligence/shared"
	sharedtypes "github.com/jordigilh/kubernaut/pkg/shared/types"
)

// Suite structure moved to orchestration_suite_test.go - generated by ginkgo bootstrap

var _ = Describe("MachineLearningAnalyzer", func() {
	var (
		analyzer   *learning.MachineLearningAnalyzer
		config     *learning.MLConfig
		mockLogger *mocks.MockLogger
	)

	BeforeEach(func() {
		mockLogger = mocks.NewMockLogger()

		config = &learning.MLConfig{
			MinExecutionsForPattern: 5,
			MaxHistoryDays:          30,
			SimilarityThreshold:     0.8,
			ClusteringEpsilon:       0.3,
			MinClusterSize:          3,
			ModelUpdateInterval:     time.Hour,
			FeatureWindowSize:       20,
			PredictionConfidence:    0.7,
		}

		analyzer = learning.NewMachineLearningAnalyzer(config, mockLogger.Logger)
	})

	Context("Feature Extraction", func() {
		It("should extract features from valid workflow data", func() {
			data := createTestWorkflowExecutionData(true, time.Minute*5)

			features, err := analyzer.ExtractFeatures(data)

			Expect(err).ToNot(HaveOccurred())
			Expect(features.AlertCount).To(BeNumerically(">=", 0), "Should extract alert count")
			Expect(features.SeverityScore).To(BeNumerically(">=", 0), "Should extract severity score")
			Expect(features.ResourceCount).To(BeNumerically(">=", 0), "Should extract resource count")
			Expect(features.HourOfDay).To(BeNumerically(">=", 0), "Should extract hour of day")
			Expect(features.DayOfWeek).To(BeNumerically(">=", 0), "Should extract day of week")
		})

		It("should handle missing alert data gracefully", func() {
			data := &sharedtypes.WorkflowExecutionData{
				ExecutionID: "test-exec",
				Timestamp:   time.Now(),
				// No alert data
			}

			features, err := analyzer.ExtractFeatures(data)

			Expect(err).ToNot(HaveOccurred())
			Expect(features.AlertCount).To(BeNumerically(">=", 0), "Should extract meaningful features")
		})

		It("should validate feature consistency", func() {
			data1 := createTestWorkflowExecutionData(true, time.Minute*3)
			data2 := createTestWorkflowExecutionData(false, time.Minute*7)

			features1, err1 := analyzer.ExtractFeatures(data1)
			features2, err2 := analyzer.ExtractFeatures(data2)

			Expect(err1).ToNot(HaveOccurred())
			Expect(err2).ToNot(HaveOccurred())

			// Features should have consistent structure
			Expect(features1.HourOfDay).To(BeNumerically(">=", 0), "Features should have valid hour")
			Expect(features1.DayOfWeek).To(BeNumerically(">=", 0), "Features should have valid day")
			Expect(features2.HourOfDay).To(BeNumerically(">=", 0), "Features should have valid hour")
			Expect(features2.DayOfWeek).To(BeNumerically(">=", 0), "Features should have valid day")
		})
	})

	Context("Model Training", func() {
		var trainingData []*sharedtypes.WorkflowExecutionData

		BeforeEach(func() {
			trainingData = createTrainingDataset(20, 0.7) // 20 samples, 70% success rate
		})

		It("should train a success prediction model successfully", func() {
			model, err := analyzer.TrainModel("success_prediction", trainingData)

			// **Business Requirement Validation**: Verify ML model training produces usable business intelligence
			Expect(err).ToNot(HaveOccurred(), "Should complete model training successfully")
			Expect(model.Type).To(Equal("classification"), "Should create classification model for success prediction")

			// **Business Value Validation**: Model should be operationally useful
			Expect(model.Accuracy).To(BeNumerically(">", 0), "Should produce a trained model with measurable accuracy")
			Expect(model.Accuracy).To(BeNumerically("<=", 1.0), "Accuracy should be within valid range")
			Expect(len(model.Features)).To(BeNumerically(">", 0), "Should identify relevant features for prediction")

			// **Business Outcome**: Model should be ready for prediction use
			Expect(model.ID).ToNot(BeEmpty(), "Should have model identifier for operational use")
			Expect(len(model.Features)).To(BeNumerically(">", 0), "BR-ORK-001: ML model must contain measurable feature definitions for orchestration prediction success")
		})

		It("should train a duration prediction model successfully", func() {
			model, err := analyzer.TrainModel("duration_prediction", trainingData)

			// **Business Requirement Validation**: Verify regression model training for duration prediction
			Expect(err).ToNot(HaveOccurred(), "Should complete duration prediction model training")
			Expect(model.Type).To(Equal("regression"), "Should create regression model for duration prediction")

			// **Business Value Validation**: Model should provide measurable regression capability
			Expect(model.TrainingMetrics.R2Score).To(BeNumerically(">=", 0), "BR-ORK-001: ML model training must produce measurable R2 score metrics for orchestration optimization success")
			Expect(model.TrainingMetrics.R2Score).To(BeNumerically("<=", 1.0), "R2 score should be within valid range")

			// **Business Outcome**: Model should be ready for duration prediction use
			Expect(len(model.Features)).To(BeNumerically(">", 0), "Should identify features relevant to duration prediction")
			Expect(model.ID).ToNot(BeEmpty(), "Should have model identifier for operational use")
		})

		It("should fail with insufficient training data", func() {
			insufficientData := trainingData[:2] // Only 2 samples

			model, err := analyzer.TrainModel("success_prediction", insufficientData)

			Expect(err).To(HaveOccurred())
			Expect(model).To(BeNil())
			Expect(err.Error()).To(ContainSubstring("insufficient"))
		})

		It("should validate model quality metrics", func() {
			model, err := analyzer.TrainModel("success_prediction", trainingData)

			// **Business Requirement Validation**: Verify model produces valid quality metrics
			Expect(err).ToNot(HaveOccurred(), "Should successfully train model for quality validation")
			Expect(model.TrainingMetrics.R2Score).To(BeNumerically(">=", 0), "BR-ORK-001: ML model training must produce measurable quality assessment metrics for orchestration success")

			// **Business Value Validation**: Model metrics should be within operational boundaries
			Expect(model.TrainingMetrics.Accuracy).To(BeNumerically(">=", 0), "Accuracy should be measurable")
			Expect(model.TrainingMetrics.Accuracy).To(BeNumerically("<=", 1.0), "Accuracy should not exceed 100%")

			// **Business Outcome**: Model should provide quality indicators for operational decisions
			Expect(model.TrainingMetrics.Precision).To(BeNumerically(">=", 0), "Should calculate precision metric")
			Expect(model.TrainingMetrics.Recall).To(BeNumerically(">=", 0), "Should calculate recall metric")
			Expect(model.TrainingMetrics.F1Score).To(BeNumerically(">=", 0), "Should calculate F1 score for balanced assessment")
		})
	})

	Context("Outcome Prediction", func() {
		var patterns []*shared.DiscoveredPattern

		BeforeEach(func() {
			trainingData := createTrainingDataset(15, 0.8)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			patterns = createTestDiscoveredPatterns(3)
		})

		It("should predict workflow outcome with confidence", func() {
			features := createTestWorkflowFeatures()

			prediction, err := analyzer.PredictOutcome(features, patterns)

			Expect(err).ToNot(HaveOccurred())
			Expect(prediction.Confidence).To(BeNumerically(">=", 0.3), "Should provide reasonably confident predictions")
			Expect(prediction.Confidence).To(BeNumerically("<=", 1.0), "Confidence should not exceed 100%")
			Expect(prediction.SuccessProbability).To(BeNumerically(">=", 0.0))
			Expect(prediction.SuccessProbability).To(BeNumerically("<=", 1.0))
		})

		It("should provide reasonable predictions for known patterns", func() {
			// Test with high-confidence pattern
			highConfidencePattern := patterns[0]
			highConfidencePattern.Confidence = 0.95
			highConfidencePattern.SuccessRate = 0.90

			features := createTestWorkflowFeatures()
			prediction, err := analyzer.PredictOutcome(features, []*shared.DiscoveredPattern{highConfidencePattern})

			// **Business Requirement Validation**: Verify prediction capability with known patterns
			Expect(err).ToNot(HaveOccurred(), "Should successfully generate predictions for known patterns")
			Expect(prediction.Confidence).To(BeNumerically(">", 0), "BR-ORK-002: ML prediction must provide measurable confidence scores for orchestration decision making")

			// **Business Value Validation**: Prediction should provide actionable intelligence
			Expect(prediction.Confidence).To(BeNumerically("<=", 1.0), "Confidence should be within valid range")
			Expect(prediction.SuccessProbability).To(BeNumerically(">=", 0), "Should calculate success probability")
			Expect(prediction.SuccessProbability).To(BeNumerically("<=", 1.0), "Success probability should be within valid range")

			// **Business Outcome**: Prediction should reflect input pattern quality
			Expect(prediction.SimilarPatterns).To(BeNumerically(">=", 1), "Should identify similar patterns for decision support")
			Expect(prediction.Reason).ToNot(BeEmpty(), "Should provide prediction reasoning for decision support")
		})

		It("should handle edge cases in prediction", func() {
			// Test with empty patterns
			features := createTestWorkflowFeatures()
			prediction, err := analyzer.PredictOutcome(features, []*shared.DiscoveredPattern{})

			Expect(err).ToNot(HaveOccurred())
			Expect(prediction.Confidence).To(BeNumerically(">=", 0.1), "Should provide baseline confidence even without patterns")
			Expect(prediction.SuccessProbability).To(BeNumerically(">=", 0.0), "Should have valid success probability")
			Expect(prediction.SuccessProbability).To(BeNumerically("<=", 1.0), "Success probability should not exceed 100%")
			Expect(prediction.SimilarPatterns).To(Equal(0), "Should show 0 patterns when none provided")
		})
	})

	Context("Cross-Validation", func() {
		It("should perform k-fold cross-validation", func() {
			trainingData := createTrainingDataset(25, 0.75)

			metrics, err := analyzer.CrossValidateModel("success_prediction", trainingData, 5)

			// **Business Requirement Validation**: Verify cross-validation provides model reliability assessment
			Expect(err).ToNot(HaveOccurred(), "Should successfully perform cross-validation analysis")
			Expect(metrics.Folds).To(Equal(5), "BR-ORK-003: Cross-validation must provide measurable fold-based metrics for orchestration model reliability")

			// **Business Value Validation**: Cross-validation should provide statistical validity assessment
			Expect(metrics.MeanAccuracy).To(BeNumerically(">=", 0), "Should calculate mean accuracy across folds")
			Expect(metrics.MeanAccuracy).To(BeNumerically("<=", 1.0), "Mean accuracy should be within valid range")
			Expect(metrics.StdAccuracy).To(BeNumerically(">=", 0.0), "Should calculate accuracy standard deviation")

			// **Business Outcome**: Cross-validation should support model reliability decisions
			Expect(metrics.MeanF1).To(BeNumerically(">=", 0), "Should calculate mean F1 score across folds")
			Expect(metrics.StdF1).To(BeNumerically(">=", 0), "Should calculate F1 standard deviation for reliability assessment")
		})

		It("should detect overfitting through cross-validation", func() {
			// Create a dataset with clear patterns but limited size
			trainingData := createOverfittingProneDataset(10)

			metrics, err := analyzer.CrossValidateModel("success_prediction", trainingData, 3)

			Expect(err).ToNot(HaveOccurred())
			// High standard deviation indicates potential overfitting
			if metrics.MeanAccuracy > 0.95 && metrics.StdAccuracy > 0.2 {
				mockLogger.Logger.Warn("Potential overfitting detected in cross-validation")
			}
		})
	})

	Context("Model Analysis and Performance", func() {
		It("should analyze model performance with test data", func() {
			// Train a model first
			trainingData := createTrainingDataset(20, 0.75)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			// Test model performance analysis
			testData := createTrainingDataset(10, 0.8)
			performance, err := analyzer.AnalyzeModelPerformance("success_prediction", testData)

			// **Business Requirement Validation**: Verify model performance analysis capability
			Expect(err).ToNot(HaveOccurred(), "Should successfully analyze model performance")
			Expect(performance.ModelID).To(Equal("success_prediction"), "BR-ORK-003: Performance analysis must provide measurable model identification for orchestration tracking")
			Expect(performance.TestDataSize).To(Equal(10), "Should use correct test data size")

			// **Business Value Validation**: Performance analysis should provide actionable metrics
			Expect(performance.Metrics["accuracy"]).To(BeNumerically(">=", 0), "BR-ORK-003: Performance analysis must provide measurable accuracy metrics for orchestration evaluation")
			Expect(performance.Metrics["accuracy"]).To(BeNumerically("<=", 1.0), "Test accuracy should be within valid range")

			// **Business Outcome**: Performance analysis should support operational decisions
			Expect(performance.EvaluatedAt.Before(time.Now().Add(time.Minute))).To(BeTrue(), "BR-ORK-003: Performance analysis must provide valid evaluation timestamps for orchestration tracking")
			Expect(len(performance.Metrics)).To(BeNumerically(">=", 1), "Should provide multiple performance metrics")
		})

		It("should maintain model count correctly", func() {
			initialCount := analyzer.GetModelCount()

			// Train a new model
			trainingData := createTrainingDataset(15, 0.7)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			// Model count should reflect the trained model
			newCount := analyzer.GetModelCount()
			Expect(newCount).To(BeNumerically(">=", initialCount))
		})

		It("should provide access to trained models", func() {
			// Train a model
			trainingData := createTrainingDataset(15, 0.7)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			// Get models should return the trained model
			models := analyzer.GetModels()
			Expect(len(models)).To(BeNumerically(">", 0), "Should return trained models")

			if successModel, exists := models["success_prediction"]; exists {
				Expect(successModel.Type).To(Equal("classification"))
				Expect(successModel.TrainingMetrics.Accuracy).To(BeNumerically(">=", 0.0))
			}
		})
	})
})

// Helper functions

func createTestWorkflowExecutionData(success bool, duration time.Duration) *sharedtypes.WorkflowExecutionData {
	return &sharedtypes.WorkflowExecutionData{
		ExecutionID: "test-exec-" + time.Now().Format("150405"),
		WorkflowID:  "test-workflow-001",
		Timestamp:   time.Now(),
		Duration:    duration,
		Success:     success,
		Metrics: map[string]float64{
			"alert_count":       2.0,
			"severity_score":    2.5,
			"resource_count":    3.0,
			"step_count":        5.0,
			"dependency_depth":  2.0,
			"cluster_load":      0.6,
			"resource_pressure": 0.4,
		},
	}
}

func createTrainingDataset(size int, successRate float64) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		success := float64(i) < float64(size)*successRate
		duration := time.Minute * time.Duration(2+i%10) // Varying durations 2-12 minutes
		severity := 2.0
		if !success {
			severity = 4.0 // Higher severity for failures
		}

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("training-exec-%d", i),
			WorkflowID:  "test-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    duration,
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       float64(1 + i%3),
				"severity_score":    severity,
				"resource_count":    float64(2 + i%4),
				"step_count":        float64(3 + i%8),
				"dependency_depth":  float64(1 + i%3),
				"cluster_load":      0.3 + float64(i%10)/20.0, // 0.3 to 0.75
				"resource_pressure": 0.2 + float64(i%8)/20.0,  // 0.2 to 0.55
			},
		}
	}

	return dataset
}

func createOverfittingProneDataset(size int) []*sharedtypes.WorkflowExecutionData {
	// Create a dataset with very specific patterns that might lead to overfitting
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		// Create a very specific pattern: success depends on exact pattern
		success := i%2 == 0

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("overfitting-exec-%d", i),
			WorkflowID:  "overfitting-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    5 * time.Minute,
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":    float64(i%3 + 1), // Very specific pattern
				"severity_score": float64(i%2 + 1), // Binary severity
				"step_count":     3.0,              // Fixed step count
			},
		}
	}

	return dataset
}

func createTestDiscoveredPatterns(count int) []*shared.DiscoveredPattern {
	patterns := make([]*shared.DiscoveredPattern, count)

	for i := 0; i < count; i++ {
		patterns[i] = &shared.DiscoveredPattern{
			PatternType:  shared.PatternTypeAlert,
			DiscoveredAt: time.Now().Add(-time.Duration(i) * time.Hour),
		}
	}

	return patterns
}

func createTestWorkflowFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         2,
		SeverityScore:      2.5,
		ResourceCount:      4,
		StepCount:          6,
		DependencyDepth:    2,
		HourOfDay:          14, // 2 PM
		DayOfWeek:          2,  // Tuesday
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     0,
		AverageSuccessRate: 0.75,
		LastExecutionTime:  5 * time.Minute,
		ClusterLoad:        0.6,
		ResourcePressure:   0.3,
		CustomMetrics: map[string]float64{
			"workflow_complexity": 0.6,
		},
	}
}

// Helper functions for Advanced ML Capabilities tests

// Helper functions for Failure Learning tests

// Specific failure pattern creators

// Feature creators for specific failure patterns

// Helper functions for Data Security and Analytics tests

