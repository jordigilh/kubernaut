apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-frontend
  namespace: demo-netpol
  labels:
    app: web-frontend
    kubernaut.ai/managed: "true"
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-frontend
  template:
    metadata:
      labels:
        app: web-frontend
    spec:
      containers:
      - name: nginx
        image: nginx:1.27-alpine
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        resources:
          requests:
            memory: "32Mi"
            cpu: "10m"
          limits:
            memory: "64Mi"
            cpu: "50m"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 2
          periodSeconds: 5
      volumes:
      - name: config
        configMap:
          name: web-frontend-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: web-frontend-config
  namespace: demo-netpol
data:
  nginx.conf: |
    worker_processes auto;
    error_log /var/log/nginx/error.log warn;
    pid /tmp/nginx.pid;
    events { worker_connections 1024; }
    http {
        server {
            listen 8080;
            server_name _;
            location / { return 200 'ok\n'; add_header Content-Type text/plain; }
            location /healthz { return 200 'ok\n'; add_header Content-Type text/plain; }
        }
    }
---
apiVersion: v1
kind: Service
metadata:
  name: web-frontend
  namespace: demo-netpol
  labels:
    app: web-frontend
    kubernaut.ai/managed: "true"
spec:
  selector:
    app: web-frontend
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP
---
# Traffic generator: continuously probes the web-frontend Service from within
# the cluster. When the deny-all NetworkPolicy is applied, this pod's requests
# fail (inter-pod traffic blocked by CNI), providing a reliable signal regardless
# of whether kubelet probes are also blocked.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: traffic-gen
  namespace: demo-netpol
  labels:
    app: traffic-gen
spec:
  replicas: 1
  selector:
    matchLabels:
      app: traffic-gen
  template:
    metadata:
      labels:
        app: traffic-gen
    spec:
      containers:
      - name: curl
        image: curlimages/curl:8.12.1
        command: ["/bin/sh", "-c"]
        args:
        - |
          while true; do
            if curl -sf -o /dev/null -m 2 http://web-frontend.demo-netpol:8080/healthz; then
              echo "probe=success"
            else
              echo "probe=failure"
            fi
            sleep 5
          done
        resources:
          requests:
            memory: "16Mi"
            cpu: "5m"
          limits:
            memory: "32Mi"
            cpu: "20m"
        livenessProbe:
          exec:
            command: ["curl", "-sf", "-o", "/dev/null", "-m", "3", "http://web-frontend.demo-netpol:8080/healthz"]
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        readinessProbe:
          exec:
            command: ["curl", "-sf", "-o", "/dev/null", "-m", "3", "http://web-frontend.demo-netpol:8080/healthz"]
          initialDelaySeconds: 5
          periodSeconds: 5
