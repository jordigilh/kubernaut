# Memory Eater: OOMKill variant
# Triggers OOMKill events that Event Exporter forwards to Gateway.
# Extracted from: test/infrastructure/fullpipeline_e2e.go:DeployMemoryEater
#
# Behavior: allocates 40Mi, grows to 60Mi, but limit is 50Mi -> OOMKilled (exit 137)
# The remediation workflow (oomkill-increase-memory-job) fixes this by increasing the limit.
#
# After remediation (limit increased), the process completes its allocation and holds
# memory for ~indefinitely (hold_duration=999999s), keeping the pod Running.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: memory-eater-oomkill
  namespace: demo-workloads
  labels:
    app: memory-eater-oomkill
    kubernaut.ai/managed: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: memory-eater-oomkill
  template:
    metadata:
      labels:
        app: memory-eater-oomkill
        kubernaut.ai/managed: "true"
    spec:
      containers:
        - name: memory-eater
          image: us-central1-docker.pkg.dev/genuine-flight-317411/devel/memory-eater:1.0
          imagePullPolicy: IfNotPresent
          # Args: initial_memory initial_duration target_memory target_duration hold_duration
          # - 50Mi limit: OOMKilled at 60Mi target (proper OOMKilled event, PID 1)
          # - 128Mi limit (post-remediation): succeeds, holds memory for ~11.5 days â†’ pod stays Running
          args: ["40Mi", "1", "60Mi", "1", "999999"]
          resources:
            limits:
              memory: "50Mi"
            requests:
              memory: "20Mi"
