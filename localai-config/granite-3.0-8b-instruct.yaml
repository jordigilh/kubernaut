name: granite-3.0-8b-instruct
backend: llama
parameters:
  model: granite-3.0-8b-instruct.gguf
  
# Model configuration
context_size: 4096
threads: 4
temperature: 0.3
top_k: 40
top_p: 0.9

# Performance settings
batch: 512
gpu_layers: 0  # Set to > 0 if using GPU

# Chat template for IBM Granite
template:
  chat: |
    <|system|>
    You are a helpful AI assistant specialized in analyzing Kubernetes alerts and recommending automated remediation actions.
    <|user|>
    {{.Input}}
    <|assistant|>
    
  completion: |
    {{.Input}}

# Function calling support for structured responses
function:
  grammar: |
    root ::= object
    object ::= "{" ws members? ws "}"
    members ::= member ("," ws member)*
    member ::= ws string ws ":" ws value ws
    string ::= "\"" chars* "\""
    chars ::= [^"\\] | "\\" escape
    escape ::= ["\\/bfnrt] | "u" hex hex hex hex
    hex ::= [0-9a-fA-F]
    value ::= object | array | string | number | boolean | null
    array ::= "[" ws elements? ws "]"
    elements ::= value ("," ws value)*
    number ::= "-"? int frac? exp?
    int ::= "0" | [1-9] [0-9]*
    frac ::= "." [0-9]+
    exp ::= [eE] [+-]? [0-9]+
    boolean ::= "true" | "false"
    null ::= "null"
    ws ::= [ \t\n\r]*