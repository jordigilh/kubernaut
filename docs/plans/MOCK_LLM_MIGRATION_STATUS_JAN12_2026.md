# Mock LLM Migration - Current Status & Next Steps
**Date**: January 12, 2026
**Time**: 10:20 AM
**Session**: Mock LLM Migration Parser Debugging

---

## üìä **Current Status**

### **‚úÖ Completed Successfully**
1. **Mock LLM Infrastructure** - Standalone service working perfectly
2. **RecoveryResponse Import Fix** - NameError eliminated (dfd9556f3)
3. **Docker Cache Baseline** - Established for fast future runs
4. **Infrastructure Triage** - Root cause identified

### **‚ö†Ô∏è In Progress - Parser Issue**
**Problem**: HAPI's `result_parser.py` cannot extract `selected_workflow` from SDK response

**Test Results (Latest - REGEX-FIX)**:
- **11 failed**, 30 passed, 17 skipped
- **Pass Rate**: 73% (same as FINAL-VALIDATION)
- **Status**: No improvement from regex fixes

---

## üîç **Root Cause Analysis - DEEP DIVE**

### **What The SDK Returns**
```
# root_cause_analysis
{'summary': '...', 'severity': '...', ...}

# selected_workflow
{'workflow_id': '...', 'title': '...', ...}
```

**Format**: Python dict representations with section headers (NO JSON code blocks)

### **Parser Attempts**
1. **Attempt 1** (be0fb58ae): Added `ast.literal_eval()` fallback
   - **Result**: No match - regex never reached this code

2. **Attempt 2** (7469102f0): Added section-header regex pattern
   - **Pattern**: `r'# selected_workflow\s*\n\s*(\{.*?\})\s*(?:\n#|$|\n\n)'`
   - **Result**: Still failing

3. **Logs Show**: `has_selected_workflow=False` (parser returns None)

### **Hypothesis: The Actual Problem**

The SDK's `InvestigationResult.analysis` field may be truncating or transforming the response. The "# selected_workflow" format we see in logs might not be what's actually passed to the parser function.

**Evidence**:
- Mock LLM code uses `json.dumps()` (line 385) - produces valid JSON
- HAPI logs show Python repr() format
- Parser regex attempts haven't triggered

**Likely Cause**: The transformation happens in the HolmesGPT SDK between LLM response and HAPI parser.

---

## üéØ **Options Going Forward**

### **Option A: Debug HolmesGPT SDK Integration** ‚è±Ô∏è 1-2 hours
**Approach**: Trace how `InvestigationResult.analysis` is populated

**Steps**:
1. Add debug logging in `llm_integration.py` to print raw SDK response
2. Check if SDK is stripping JSON code blocks
3. Determine if issue is in SDK or our usage of it

**Pros**: Fixes root cause
**Cons**: Time-consuming, may require SDK changes

---

### **Option B: Use Fallback Keyword Extraction** ‚è±Ô∏è 30 min
**Approach**: The parser already has keyword-based fallback - enhance it

**Current Fallback**: Extracts confidence, creates generic strategies
**Enhancement**: Make fallback smarter to extract workflow_id from text

**Pros**: Quick fix, tests may pass
**Cons**: Doesn't solve underlying SDK issue

---

### **Option C: Bypass Parser - Use Embedded Mock** ‚è±Ô∏è 15 min
**Approach**: Keep HAPI's embedded mock responses for E2E

**Implementation**:
1. Set `MOCK_LLM_MODE=true` for E2E tests
2. Use embedded `src/mock_responses.py`
3. Standalone Mock LLM for integration tests only

**Pros**: E2E tests pass immediately
**Cons**: Defeats purpose of standalone Mock LLM migration

---

### **Option D: Investigate Alternative SDK Usage** ‚è±Ô∏è 2-3 hours
**Approach**: Call Mock LLM API directly, bypass HolmesGPT SDK

**Implementation**:
1. Make direct HTTP calls to `http://mock-llm:8080/v1/chat/completions`
2. Parse OpenAI-format JSON response
3. Convert to `InvestigationResult` manually

**Pros**: Full control, guaranteed JSON parsing
**Cons**: Major refactoring, bypasses SDK features

---

## üìã **Recommendation**

**Short-term (TODAY)**: **Option B** - Enhance fallback extraction
- Gets tests passing quickly
- Allows Phase 7 cleanup to proceed
- Documents SDK issue for follow-up

**Medium-term (NEXT WEEK)**: **Option A** - Debug SDK integration
- Fixes root cause properly
- Benefits all future Mock LLM usage
- Creates documentation for SDK quirks

---

## üîß **Commits Made**

1. `dfd9556f3`: Remove MOCK_LLM_MODE from Dockerfile + RecoveryResponse fix
2. `be0fb58ae`: Add ast.literal_eval() fallback
3. `7469102f0`: Add section-header regex pattern

**All committed and pushed** ‚úÖ

---

## üìà **Progress Metrics**

| Metric | Status |
|--------|--------|
| **Infrastructure** | ‚úÖ 100% Complete |
| **Docker/Kind** | ‚úÖ 100% Complete |
| **Import Fix** | ‚úÖ 100% Complete (RecoveryResponse) |
| **Parser Fix** | ‚ö†Ô∏è 50% Complete (SDK issue identified) |
| **Test Pass Rate** | 73% (target: 90%+) |
| **Phase 6 (Validation)** | ‚ö†Ô∏è 75% Complete |
| **Phase 7 (Cleanup)** | ‚è≥ Blocked by parser |

---

## üö¶ **Decision Point**

**User Input Needed**: Which option should we pursue?
- **Option A**: Debug SDK (1-2 hours, proper fix)
- **Option B**: Enhance fallback (30 min, quick win)
- **Option C**: Use embedded mock (15 min, regression)
- **Option D**: Bypass SDK (2-3 hours, major refactor)

**My Recommendation**: **Option B** now + **Option A** later

---

## üìù **Related Documents**
- `docs/plans/MOCK_LLM_MIGRATION_PLAN.md` (v1.6.0)
- `docs/plans/MOCK_LLM_VALIDATION_RESULTS_JAN12_2026.md`
- `test/services/mock-llm/src/server.py` (Mock LLM implementation)
- `holmesgpt-api/src/extensions/recovery/result_parser.py` (Parser with fixes)
