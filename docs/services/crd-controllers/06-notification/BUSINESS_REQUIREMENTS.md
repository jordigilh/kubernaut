# Notification Service - Business Requirements

**Service**: Notification Service (Notification Controller)
**Service Type**: CRD Controller
**CRD**: NotificationRequest
**Controller**: NotificationRequestReconciler
**Version**: v1.5.0
**Last Updated**: December 11, 2025
**Status**: Production-Ready

---

## üìã Overview

The **Notification Service** is a Kubernetes CRD controller that delivers multi-channel notifications with guaranteed delivery, complete audit trails, and graceful degradation. It watches `NotificationRequest` CRDs and delivers notifications to multiple channels (console, Slack, email, Teams, SMS, webhooks) with automatic retry, data sanitization, and comprehensive observability.

### Architecture

**Service Type**: CRD Controller (not a stateless REST API)

**Key Characteristics**:
- Watches `NotificationRequest` CRDs created by RemediationOrchestrator or other services
- Implements reconciliation loop with phases (Pending ‚Üí Sending ‚Üí Sent/PartiallySent/Failed)
- Delivers notifications to 6 channels: Console, Slack, Email, Teams, SMS, Webhook
- Updates CRD status with delivery attempts and outcomes
- Provides zero data loss through CRD persistence to etcd

**Relationship with Other Services**:
- **RemediationOrchestrator**: Creates `NotificationRequest` CRDs for escalation notifications
- **Data Storage Service**: Persists audit trails of notification delivery attempts
- **External Services**: Slack webhooks, email SMTP, Teams webhooks, SMS providers

### Service Responsibilities

1. **Multi-Channel Delivery**: Deliver notifications to 6 channels with channel-specific formatting
2. **Zero Data Loss**: CRD-based persistence ensures no notifications are lost
3. **Automatic Retry**: Exponential backoff retry with circuit breakers for transient failures
4. **Data Sanitization**: Redact 22 secret patterns (passwords, tokens, API keys) before delivery
5. **Graceful Degradation**: Per-channel circuit breakers prevent cascade failures
6. **Complete Audit Trail**: Record every delivery attempt in CRD status
7. **Observability**: 10 Prometheus metrics for delivery success/failure rates

---

## üéØ Business Requirements

### üìä Summary

**Total Business Requirements**: 18
**Categories**: 9
**Priority Breakdown**:
- P0 (Critical): 10 BRs (BR-NOT-050, 051, 052, 053, 054, 055, 058, 060, 061, 065)
- P1 (High): 8 BRs (BR-NOT-056, 057, 059, 066, 067, 068, 069)

**Implementation Status**:
- Implemented: 17 BRs (94%)
- Approved for Kubernaut V1.0: 1 BR (BR-NOT-069)

**Test Coverage**:
- Unit: 82 test specs (95% confidence)
- Integration: 21 test scenarios (90% confidence)
- E2E: 4 test files (100% passing)

---

### Category 1: Data Integrity & Persistence

#### BR-NOT-050: Data Loss Prevention

**Description**: The Notification Service MUST persist all notification requests to etcd via CRD before attempting delivery, ensuring zero data loss even if the controller crashes during delivery.

**Priority**: P0 (CRITICAL)

**Rationale**: Notifications contain critical escalation information. Losing a notification could result in unresolved production incidents. CRD persistence to etcd provides durability and crash recovery.

**Implementation**:
- `NotificationRequest` CRD persisted to etcd before delivery attempts
- Kubernetes reconciliation loop ensures delivery attempts resume after controller restart
- CRD status tracks delivery progress for crash recovery

**Acceptance Criteria**:
- ‚úÖ NotificationRequest CRD created and persisted to etcd before delivery
- ‚úÖ Controller restart does not lose pending notifications
- ‚úÖ Delivery attempts resume from last known state after crash

**Test Coverage**:
- Unit: CRD creation and persistence validation
- Integration: Controller restart with pending notifications
- E2E: End-to-end notification delivery with simulated crashes

**Related BRs**: BR-NOT-051 (Audit Trail), BR-NOT-053 (At-Least-Once Delivery)

---

#### BR-NOT-051: Complete Audit Trail

**Description**: The Notification Service MUST record every delivery attempt (success or failure) in the `NotificationRequest` CRD status, including timestamp, channel, status, and error message.

**Priority**: P0 (CRITICAL)

**Rationale**: Complete audit trails enable debugging failed deliveries, compliance reporting, and SLA tracking. Each delivery attempt must be traceable for post-incident analysis.

**Implementation**:
- `DeliveryAttempts` array in CRD status records all attempts
- Each attempt includes: channel, timestamp, status (success/failed), error message
- Status counters: `TotalAttempts`, `SuccessfulDeliveries`, `FailedDeliveries`

**Acceptance Criteria**:
- ‚úÖ Every delivery attempt recorded in `DeliveryAttempts` array
- ‚úÖ Timestamps accurate to millisecond precision
- ‚úÖ Error messages captured for failed attempts
- ‚úÖ Status counters updated correctly

**Test Coverage**:
- Unit: `test/unit/notification/status_test.go:25-236` (BR-NOT-051: Status Tracking)
- Integration: Multi-channel delivery with mixed success/failure
- E2E: Complete notification lifecycle audit trail validation

**Related BRs**: BR-NOT-050 (Data Loss Prevention), BR-NOT-054 (Observability)

---

### Category 2: Delivery Reliability

#### BR-NOT-052: Automatic Retry with Exponential Backoff

**Description**: The Notification Service MUST automatically retry failed deliveries using exponential backoff (30s ‚Üí 60s ‚Üí 120s ‚Üí 240s ‚Üí 480s) for transient errors, with a maximum of 5 attempts per channel.

**Priority**: P0 (CRITICAL)

**Rationale**: Transient network errors, rate limiting, and temporary service unavailability are common. Automatic retry with exponential backoff maximizes delivery success while avoiding overwhelming downstream services.

**Implementation**:
- Retry policy: max 5 attempts, base backoff 30s, max backoff 480s, multiplier 2.0
- Error classification: Transient (429, 500, 502, 503, 504, 508, timeouts) vs. Permanent (401, 403, 404, 400, 422)
- Exponential backoff calculation: `backoff = min(baseBackoff * (multiplier ^ attempt), maxBackoff)`

**Acceptance Criteria**:
- ‚úÖ Transient errors (503, 504, 429, timeouts) trigger retry
- ‚úÖ Permanent errors (401, 403, 404) do NOT trigger retry
- ‚úÖ Backoff durations follow exponential progression
- ‚úÖ Max 5 attempts enforced per channel
- ‚úÖ Max backoff capped at 480 seconds

**Test Coverage**:
- Unit: `test/unit/notification/retry_test.go:19-98` (BR-NOT-052: Retry Policy)
- Unit: Error classification for 12 HTTP status codes
- Integration: Retry with simulated transient failures
- E2E: End-to-end retry with real service failures

**Related BRs**: BR-NOT-053 (At-Least-Once Delivery), BR-NOT-055 (Graceful Degradation)

---

#### BR-NOT-053: At-Least-Once Delivery Guarantee

**Description**: The Notification Service MUST guarantee at-least-once delivery for all notifications through Kubernetes reconciliation loop, ensuring notifications are eventually delivered even after controller restarts or transient failures.

**Priority**: P0 (CRITICAL)

**Rationale**: Critical escalation notifications must be delivered. The reconciliation loop ensures pending notifications are retried until successful delivery or permanent failure.

**Implementation**:
- Kubernetes reconciliation loop continuously processes pending notifications
- CRD status phase transitions: Pending ‚Üí Sending ‚Üí Sent/PartiallySent/Failed
- Reconciliation triggered on CRD creation, status updates, and periodic requeue
- Notifications remain in reconciliation queue until terminal phase (Sent/Failed)

**Acceptance Criteria**:
- ‚úÖ Pending notifications reconciled until successful delivery
- ‚úÖ Controller restart does not prevent delivery
- ‚úÖ Transient failures trigger automatic retry via reconciliation
- ‚úÖ Terminal phases (Sent/Failed) stop reconciliation

**Test Coverage**:
- Unit: Reconciliation loop logic and phase transitions
- Integration: Controller restart with pending notifications
- E2E: End-to-end delivery with simulated controller crashes

**Related BRs**: BR-NOT-050 (Data Loss Prevention), BR-NOT-052 (Automatic Retry)

---

### Category 3: Observability & Monitoring

#### BR-NOT-054: Comprehensive Observability

**Description**: The Notification Service MUST expose 10 Prometheus metrics for delivery success/failure rates, retry attempts, circuit breaker state, and delivery latency, enabling real-time monitoring and alerting.

**Priority**: P0 (CRITICAL)

**Rationale**: Production monitoring requires real-time visibility into notification delivery health. Metrics enable SLA tracking, alerting on delivery failures, and capacity planning.

**Implementation**:
- **10 Prometheus Metrics**:
  1. `notification_delivery_total` (counter) - Total delivery attempts by channel and status
  2. `notification_delivery_duration_seconds` (histogram) - Delivery latency by channel
  3. `notification_retry_attempts_total` (counter) - Retry attempts by channel
  4. `notification_circuit_breaker_state` (gauge) - Circuit breaker state (0=closed, 1=open)
  5. `notification_pending_total` (gauge) - Pending notifications by priority
  6. `notification_failed_permanent_total` (counter) - Permanent failures by channel
  7. `notification_sanitization_redactions_total` (counter) - Secret patterns redacted
  8. `notification_delivery_success_rate` (gauge) - Success rate by channel (0-1)
  9. `notification_reconciliation_duration_seconds` (histogram) - Reconciliation loop latency
  10. `notification_crd_phase_transitions_total` (counter) - Phase transitions by phase

**Acceptance Criteria**:
- ‚úÖ All 10 metrics exposed on `/metrics` endpoint (port 9090)
- ‚úÖ Metrics updated in real-time during delivery
- ‚úÖ Prometheus scraping validated in integration tests
- ‚úÖ Grafana dashboard templates provided

**Test Coverage**:
- Unit: Metrics recording logic
- Integration: Prometheus scraping and metric validation
- E2E: End-to-end metric validation with real deliveries

**Related BRs**: BR-NOT-051 (Audit Trail), BR-NOT-055 (Graceful Degradation)

---

### Category 4: Fault Tolerance

#### BR-NOT-055: Graceful Degradation with Circuit Breakers

**Description**: The Notification Service MUST implement per-channel circuit breakers to prevent cascade failures, allowing partial delivery success when individual channels fail while continuing delivery to healthy channels.

**Priority**: P0 (CRITICAL)

**Rationale**: A single failing channel (e.g., Slack webhook down) should not block delivery to other channels (e.g., email, console). Circuit breakers prevent wasting resources on failing channels while allowing healthy channels to succeed.

**Implementation**:
- Per-channel circuit breaker with 3 states: Closed (healthy), Open (failing), Half-Open (testing recovery)
- Circuit opens after 5 consecutive failures (failure threshold)
- Circuit half-opens after 60 seconds (recovery timeout)
- Circuit closes after 2 consecutive successes in half-open state (success threshold)
- Partial delivery: CRD phase = `PartiallySent` if some channels succeed

**Acceptance Criteria**:
- ‚úÖ Circuit breaker opens after 5 consecutive failures
- ‚úÖ Circuit breaker half-opens after 60 seconds
- ‚úÖ Circuit breaker closes after 2 consecutive successes
- ‚úÖ Partial delivery succeeds when some channels fail
- ‚úÖ Circuit breaker state exposed via Prometheus metrics

**Test Coverage**:
- Unit: `test/unit/notification/retry_test.go:100-187` (BR-NOT-055: Circuit Breaker)
- Integration: Multi-channel delivery with simulated failures
- E2E: Circuit breaker state transitions with real service failures

**Related BRs**: BR-NOT-052 (Automatic Retry), BR-NOT-054 (Observability)

---

### Category 5: CRD Lifecycle Management

#### BR-NOT-056: CRD Lifecycle and Phase State Machine

**Description**: The Notification Service MUST implement a 5-phase state machine for `NotificationRequest` CRDs (Pending ‚Üí Sending ‚Üí Sent/PartiallySent/Failed) with deterministic phase transitions and status updates.

**Priority**: P0 (CRITICAL)

**Rationale**: Clear phase transitions enable external services to track notification delivery progress. Deterministic state machine prevents race conditions and ensures consistent behavior.

**Implementation**:
- **5 Phases**:
  1. `Pending`: Initial phase, delivery not yet attempted
  2. `Sending`: Delivery in progress
  3. `Sent`: All channels delivered successfully
  4. `PartiallySent`: Some channels succeeded, some failed permanently
  5. `Failed`: All channels failed permanently

- **Phase Transitions**:
  - `Pending` ‚Üí `Sending`: Reconciliation starts delivery
  - `Sending` ‚Üí `Sent`: All channels delivered successfully
  - `Sending` ‚Üí `PartiallySent`: Some channels succeeded, some failed permanently
  - `Sending` ‚Üí `Failed`: All channels failed permanently
  - `Sending` ‚Üí `Pending`: Transient failure, retry scheduled

**Acceptance Criteria**:
- ‚úÖ Phase transitions follow state machine rules
- ‚úÖ CRD status updated atomically with phase transitions
- ‚úÖ No invalid phase transitions (e.g., `Sent` ‚Üí `Pending`)
- ‚úÖ Phase transitions recorded in audit trail

**Test Coverage**:
- Unit: Phase transition logic and validation
- Integration: Complete phase lifecycle with mixed success/failure
- E2E: End-to-end phase transitions with real deliveries

**Related BRs**: BR-NOT-051 (Audit Trail), BR-NOT-053 (At-Least-Once Delivery)

---

### Category 6: Priority Handling

#### BR-NOT-057: Priority-Based Processing

**Description**: The Notification Service MUST support 4 priority levels (Critical, High, Medium, Low) and process notifications according to priority, ensuring critical notifications are delivered first during high load.

**Priority**: P1 (HIGH)

**Rationale**: Critical escalation notifications (e.g., production outages) must be delivered before low-priority status updates. Priority-based processing ensures important notifications are not delayed by low-priority traffic.

**Implementation**:
- **4 Priority Levels**: Critical, High, Medium, Low
- Priority field in `NotificationRequest` CRD spec
- Reconciliation queue prioritization (future enhancement - V1.0 processes all priorities)
- Prometheus metrics track pending notifications by priority

**Acceptance Criteria**:
- ‚úÖ All 4 priority levels supported in CRD schema
- ‚úÖ Priority field validated during CRD admission
- ‚úÖ Metrics expose pending notifications by priority
- ‚úÖ (V1.1) Critical notifications processed before low-priority

**Test Coverage**:
- Unit: Priority validation and CRD schema
- Integration: Mixed priority notifications processed correctly
- E2E: Priority-based delivery under high load (V1.1)

**Related BRs**: BR-NOT-054 (Observability), BR-NOT-058 (Validation)

---

### Category 7: Data Validation & Security

#### BR-NOT-058: CRD Validation and Data Sanitization

**Description**: The Notification Service MUST validate all `NotificationRequest` CRDs using Kubebuilder validation rules and sanitize notification content by redacting 22 secret patterns (passwords, tokens, API keys) before delivery.

**Priority**: P0 (CRITICAL)

**Rationale**: Invalid CRDs cause reconciliation failures. Sensitive data (passwords, API keys) must never be delivered to external channels (Slack, email) to prevent security breaches.

**Implementation**:
- **Kubebuilder Validation**:
  - Subject: required, min 1 char, max 200 chars
  - Body: required, min 1 char, max 10,000 chars
  - Channels: required, min 1 channel, max 6 channels
  - Priority: required, enum (Critical, High, Medium, Low)

- **22 Secret Patterns Redacted**:
  - Passwords: `password=`, `pwd=`, `pass=`
  - Tokens: `token=`, `bearer `, `authorization: `
  - API Keys: `api_key=`, `apikey=`, `key=`
  - AWS: `aws_access_key_id`, `aws_secret_access_key`
  - Database: `db_password`, `database_password`
  - Kubernetes: `kubeconfig`, `serviceaccount_token`
  - SSH: `ssh_private_key`, `id_rsa`
  - Certificates: `-----BEGIN PRIVATE KEY-----`
  - Generic: `secret=`, `credential=`, `auth=`

**Acceptance Criteria**:
- ‚úÖ Invalid CRDs rejected by Kubernetes API server
- ‚úÖ All 22 secret patterns redacted before delivery
- ‚úÖ Redaction preserves message readability (replaces with `[REDACTED]`)
- ‚úÖ Redaction metrics exposed via Prometheus

**Test Coverage**:
- Unit: `test/unit/notification/sanitization_test.go` (31 scenarios)
- Integration: CRD validation rejection tests
- E2E: End-to-end sanitization with real secret patterns

**Related BRs**: BR-NOT-054 (Observability)

---

### Category 8: Performance & Scalability

#### BR-NOT-059: Large Payload Support

**Description**: The Notification Service MUST handle large notification payloads (up to 10KB) without performance degradation or controller crashes, ensuring graceful degradation for oversized payloads.

**Priority**: P1 (HIGH)

**Rationale**: Production notifications may include large stack traces, log excerpts, or detailed error messages. The controller must handle large payloads gracefully without impacting other notifications.

**Implementation**:
- **Payload Size Limit**: Support payloads up to 10KB (10,240 bytes)
- **Validation**: Reject payloads exceeding 10KB with clear error message
- **Performance**: No degradation for payloads up to 10KB
- **Memory Management**: Efficient memory handling for large payloads
- **Graceful Degradation**: Truncate oversized payloads with warning

**Acceptance Criteria**:
- ‚úÖ Payloads up to 10KB processed without performance degradation
- ‚úÖ Oversized payloads (>10KB) rejected with clear error
- ‚úÖ Memory usage stable with large payloads
- ‚úÖ No controller crashes with large payloads
- ‚úÖ Warning logged for truncated payloads

**Test Coverage**:
- Unit: Payload size validation and truncation logic
- Integration: `test/integration/notification/INTEGRATION_TEST_FAILURES_TRIAGE.md:228, 334` (Large payload scenarios)
- E2E: End-to-end delivery with 10KB payloads

**Related BRs**: BR-NOT-058 (Validation), BR-NOT-060 (Concurrent Delivery)

---

#### BR-NOT-060: Concurrent Delivery Safety

**Description**: The Notification Service MUST safely handle concurrent notification deliveries (10+ simultaneous notifications) without race conditions, data corruption, or performance degradation.

**Priority**: P0 (CRITICAL)

**Rationale**: Production environments generate multiple notifications simultaneously. Race conditions in concurrent delivery can cause data corruption, duplicate deliveries, or controller crashes.

**Implementation**:
- **Thread-Safe Delivery**: Proper locking and synchronization for shared resources
- **Concurrent CRD Updates**: Optimistic concurrency control for status updates
- **Channel Isolation**: Per-channel delivery isolation to prevent cross-contamination
- **Resource Pooling**: Connection pooling for external services (Slack, email)
- **Race Detection**: Validated with Go race detector (`go test -race`)

**Acceptance Criteria**:
- ‚úÖ 10+ concurrent notifications delivered without race conditions
- ‚úÖ No data corruption in CRD status updates
- ‚úÖ No duplicate deliveries
- ‚úÖ Performance scales linearly with concurrency
- ‚úÖ Race detector reports zero races

**Test Coverage**:
- Unit: Concurrent delivery logic with race detection
- Integration: `test/integration/notification/INTEGRATION_TEST_FAILURES_TRIAGE.md:384` (Concurrent delivery scenarios)
- E2E: High-load concurrent delivery testing

**Related BRs**: BR-NOT-051 (Audit Trail), BR-NOT-059 (Large Payload Support)

---

#### BR-NOT-061: Circuit Breaker Protection

**Description**: The Notification Service MUST use circuit breakers to prevent cascading failures during rate limiting or service unavailability, failing fast instead of accumulating retry attempts.

**Priority**: P0 (CRITICAL)

**Rationale**: When downstream services (Slack, email) are rate-limiting or unavailable, continued retry attempts waste resources and delay failure detection. Circuit breakers prevent cascading failures and enable fast recovery.

**Implementation**:
- **Per-Channel Circuit Breakers**: Independent circuit breakers for each delivery channel
- **Fast Failure**: Fail immediately when circuit is open (no retry attempts)
- **Circuit States**: Closed (healthy), Open (failing), Half-Open (testing recovery)
- **Failure Threshold**: Circuit opens after 5 consecutive failures
- **Recovery Timeout**: Circuit half-opens after 60 seconds
- **Success Threshold**: Circuit closes after 2 consecutive successes

**Acceptance Criteria**:
- ‚úÖ Circuit breaker opens after 5 consecutive failures
- ‚úÖ Fast failure when circuit is open (no retry attempts)
- ‚úÖ Circuit half-opens after 60 seconds
- ‚úÖ Circuit closes after 2 consecutive successes
- ‚úÖ Prevents cascading failures during rate limiting
- ‚úÖ Circuit breaker state exposed via Prometheus metrics

**Test Coverage**:
- Unit: `test/unit/notification/retry_test.go:100-187` (Circuit breaker state machine)
- Integration: `test/integration/notification/INTEGRATION_TEST_FAILURES_TRIAGE.md:472` (Circuit breaker activation)
- E2E: Circuit breaker behavior under sustained failures

**Related BRs**: BR-NOT-052 (Automatic Retry), BR-NOT-055 (Graceful Degradation)

**Note**: This BR complements BR-NOT-055 (Graceful Degradation). BR-NOT-055 focuses on partial delivery success, while BR-NOT-061 focuses on fast failure and cascading failure prevention.

---

### Category 9: Channel Routing (V1.0)

#### BR-NOT-065: Channel Routing Based on Spec Fields

**Description**: The Notification Service MUST route notifications to appropriate channel(s) based on notification spec fields and `spec.metadata` (type, severity, environment, namespace, skip-reason) using configurable routing rules. CRD creators (e.g., RemediationOrchestrator) do NOT need to specify Recipients or Channels - routing rules determine these based on spec fields.

**Priority**: P0 (CRITICAL)

**Rationale**: Different notification types require different delivery channels. Approval requests may need PagerDuty for immediate attention, while completion notifications may only need Slack. Spec-field-based routing enables flexible, configurable channel selection without code changes.

**Implementation** (Issue #91):
- Routing based on spec fields: `spec.type`, `spec.severity`, `spec.phase`, `spec.reviewSource`, `spec.priority`
- Routing based on `spec.metadata`: `environment`, `namespace`, `skip-reason`, `investigation-outcome`
- Configurable routing rules in ConfigMap (simplified keys: `severity`, `type`, `skip-reason`, etc.)
- Field selectors (`+kubebuilder:selectablefield`) replace label-based filtering
- First matching rule wins (ordered evaluation)
- Default fallback channel if no rules match

**Supported Routing Spec Fields** (Issue #91: migrated from labels to immutable spec):

| Spec Field / Metadata Key | Config Match Key | Purpose | Example Values |
|---------------------------|------------------|---------|----------------|
| `spec.type` | `type` | Notification type routing | `escalation`, `approval`, `completion`, `manual-review` |
| `spec.severity` | `severity` | Severity-based routing | `critical`, `high`, `medium`, `low` |
| `spec.metadata["environment"]` | `environment` | Environment-based routing | `production`, `staging`, `development`, `test` |
| `spec.priority` | `priority` | Priority-based routing | `critical`, `high`, `medium`, `low` |
| `spec.metadata["namespace"]` | `namespace` | Namespace-based routing | Kubernetes namespace name |
| `spec.phase` | `phase` | Phase that triggered notification | `signal-processing`, `ai-analysis`, `workflow-execution` |
| `spec.reviewSource` | `review-source` | Manual review source | `WorkflowResolutionFailed`, `ExhaustedRetries` |
| `spec.remediationRequestRef` | (correlation) | Parent remediation link | ObjectReference (ownerRef sufficient) |
| `spec.metadata["skip-reason"]` | `skip-reason` | WFE skip reason routing | `PreviousExecutionFailed`, `ExhaustedRetries`, `ResourceBusy`, `RecentlyRemediated` |

**Removed** (Issue #91): `kubernaut.ai/component` (ownerRef sufficient), `kubernaut.ai/remediation-request` (use `spec.remediationRequestRef`).

**Skip Reason Routing** (DD-WE-004 v1.1):

| Skip Reason | Recommended Severity | Routing Target | Rationale |
|-------------|---------------------|----------------|-----------|
| `PreviousExecutionFailed` | `critical` | PagerDuty | Cluster state unknown - immediate action required |
| `ExhaustedRetries` | `high` | Slack | Infrastructure issues - team awareness required |
| `ResourceBusy` | `low` | Bulk (BR-ORCH-034) | Temporary - auto-resolves |
| `RecentlyRemediated` | `low` | Bulk (BR-ORCH-034) | Temporary - auto-resolves |

**Mandatory Spec Fields for CRD Creators** (REQUIRED):

CRD creators (RemediationOrchestrator, WorkflowExecution) MUST set these spec fields when creating NotificationRequest CRDs:

| Spec Field | Requirement | Source | Rationale |
|------------|-------------|--------|-----------|
| `spec.type` | **MANDATORY** | CRD creator | Required for type-based routing |
| `spec.severity` | **MANDATORY** | CRD creator | Required for severity-based routing |
| `spec.metadata["environment"]` | **MANDATORY** | RemediationRequest | Required for environment-based routing |
| `spec.metadata["skip-reason"]` | **CONDITIONAL** | WorkflowExecution (when skipped) | Required for skip-reason routing |
| `spec.remediationRequestRef` | **MANDATORY** | RemediationRequest | Required for correlation (or ownerRef) |

**Cross-Team Enforcement**: Per DD-WE-004 Q8, RO confirmed they will set all routing spec fields explicitly.

**Acceptance Criteria**:
- ‚úÖ Notifications routed based on spec field matching
- ‚úÖ Multiple routing rules supported with priority ordering
- ‚úÖ Default fallback channel configured
- ‚úÖ Routing decision logged for audit
- ‚úÖ Skip-reason routing supported for WFE failure routing (DD-WE-004)
- ‚úÖ Missing mandatory spec fields logged as warning

**Test Coverage**:
- Unit: Spec field matching and routing decision logic (37 tests)
- Integration: Multi-rule routing with various spec field combinations
- E2E: End-to-end routing validation

**Related BRs**: BR-NOT-066 (Config Format), BR-NOT-067 (Hot-Reload)
**Related DDs**: DD-NOTIFICATION-001 (Alertmanager Routing Reuse), DD-WE-004 (Exponential Backoff)
**Cross-Team**: NOTICE_WE_EXPONENTIAL_BACKOFF_DD_WE_004.md (Q7, Q8)
**Issue #91**: `kubernaut.ai/*` labels migrated to immutable spec fields; routing config keys simplified

---

#### BR-NOT-066: Alertmanager-Compatible Configuration Format

**Description**: The Notification Service MUST support Alertmanager-compatible routing configuration format, enabling SREs to use familiar syntax for channel selection rules.

**Priority**: P1 (HIGH)

**Rationale**: Alertmanager's routing configuration is the industry standard for Kubernetes notification routing. Using the same format reduces learning curve and enables reuse of existing configurations. Per DD-NOTIFICATION-001, we reuse Alertmanager's routing library directly.

**Implementation**:
- Configuration format matches Alertmanager `route` and `receivers` structure
- Import `github.com/prometheus/alertmanager/config` for parsing
- Import `github.com/prometheus/alertmanager/dispatch` for routing logic
- Zero custom routing implementation - reuse battle-tested Alertmanager code

**Acceptance Criteria**:
- ‚úÖ Alertmanager config format parsed correctly
- ‚úÖ Matchers support: exact match (`=`), regex (`=~`), negative (`!=`)
- ‚úÖ Receivers with Slack, PagerDuty, Email, Webhook configs
- ‚úÖ Config validation on load

**Test Coverage**:
- Unit: Config parsing and validation
- Integration: Routing with Alertmanager-style config
- E2E: End-to-end delivery with complex routing rules

**Related BRs**: BR-NOT-065 (Channel Routing)
**Related DDs**: DD-NOTIFICATION-001 (Alertmanager Routing Reuse)

---

#### BR-NOT-067: Routing Configuration Hot-Reload

**Description**: The Notification Service MUST reload routing configuration without restart when the ConfigMap changes, enabling dynamic routing updates without service disruption.

**Priority**: P1 (HIGH)

**Rationale**: Production routing changes should not require controller restart. Hot-reload enables immediate configuration updates and reduces operational risk.

**Implementation** (‚úÖ COMPLETE):
- Watch ConfigMap `notification-routing-config` in `kubernaut-notifications` namespace via controller-runtime
- Rebuild routing table on ConfigMap create/update/delete events
- Thread-safe Router with RWMutex protects in-flight notifications
- New notifications use updated config immediately after reload
- Before/after diff logged on configuration changes

**Acceptance Criteria**:
- ‚úÖ ConfigMap changes detected immediately (controller-runtime watch)
- ‚úÖ Routing table updated without restart via `Router.LoadConfig()`
- ‚úÖ In-flight notifications not affected (RWMutex protection)
- ‚úÖ Config reload logged with before/after diff (`GetConfigSummary()`)

**Files Modified**:
- `pkg/notification/routing/router.go` - Thread-safe Router with hot-reload support
- `internal/controller/notification/notificationrequest_controller.go` - ConfigMap watch integration

**Test Coverage**:
- Unit: 10 tests for Router config parsing, reload, and thread safety
- Integration: ConfigMap update triggers reload (via controller-runtime)
- E2E: Dynamic routing change validation

**Related BRs**: BR-NOT-065 (Channel Routing), BR-NOT-066 (Config Format)

---

#### BR-NOT-068: Multi-Channel Fanout

**Description**: The Notification Service MUST support delivering a single notification to multiple channels simultaneously when routing rules specify multiple receivers.

**Priority**: P1 (HIGH)

**Rationale**: Critical notifications may need to reach operators via multiple channels (e.g., both PagerDuty and Slack) for redundancy and visibility.

**Implementation**:
- Routing rules can specify multiple receivers via `continue: true` (Alertmanager pattern)
- Parallel delivery to all matched channels
- Partial success: CRD status tracks per-channel delivery status
- All channels attempted even if some fail

**Acceptance Criteria**:
- ‚úÖ Single notification delivered to multiple channels
- ‚úÖ Per-channel delivery status tracked
- ‚úÖ Partial success handled (some channels succeed, some fail)
- ‚úÖ All channels attempted in parallel

**Test Coverage**:
- Unit: Multi-receiver routing logic
- Integration: Fanout delivery with mixed success/failure
- E2E: End-to-end multi-channel delivery

**Related BRs**: BR-NOT-065 (Channel Routing), BR-NOT-055 (Graceful Degradation)

---

#### BR-NOT-069: Routing Rule Visibility via Kubernetes Conditions

**Full Specification**: [BR-NOT-069-routing-rule-visibility-conditions.md](../../../requirements/BR-NOT-069-routing-rule-visibility-conditions.md)

**Description**: The Notification Service MUST expose routing rule resolution status via Kubernetes Conditions, enabling operators to debug spec-field-based channel routing without accessing controller logs.

**Priority**: P1 (HIGH)

**Key Features**:
- `RoutingResolved` condition shows matched rule name and channels
- Visible via `kubectl describe` (no log access needed)
- Fallback detection when no rules match

**Implementation Status**: ‚úÖ Approved (V1.1 - Q1 2026)

**Related BRs**: BR-NOT-065 (Channel Routing), BR-NOT-066 (Config Format), BR-NOT-067 (Hot-Reload)

---

## üìä Test Coverage Summary

### Unit Tests
- **Total**: 19 test specs
- **Coverage**: 95% confidence
- **Files**:
  - `test/unit/notification/retry_test.go` (BR-NOT-052, BR-NOT-055)
  - `test/unit/notification/status_test.go` (BR-NOT-051)
  - `test/unit/notification/sanitization_test.go` (BR-NOT-058)
  - `test/unit/notification/slack_delivery_test.go` (BR-NOT-052, BR-NOT-053)
  - `test/unit/notification/controller_edge_cases_test.go` (BR-NOT-056, BR-NOT-057)

### Integration Tests
- **Total**: 21 test specs
- **Coverage**: 92% confidence
- **Scenarios**: CRD validation, multi-channel delivery, retry logic, circuit breakers, controller restart

### E2E Tests
- **Status**: Deferred to full system deployment
- **Planned**: Real Slack webhook delivery, end-to-end lifecycle validation

### BR Coverage
- **Total BRs**: 18 (BR-NOT-050 through BR-NOT-069)
- **Unit Test Coverage**: 94% (17/18 BRs) - BR-NOT-069 pending implementation
- **Integration Test Coverage**: 94% (17/18 BRs) - BR-NOT-069 pending implementation
- **Overall Coverage**: 94% (17/18 implemented, 1 approved for Kubernaut V1.0)

---

## üîó Related Documentation

- [Notification Controller README](./README.md) - Service overview and version history
- [Production Readiness Checklist](./PRODUCTION_READINESS_CHECKLIST.md) - 99% production-ready validation
- [Official Completion Announcement](./OFFICIAL_COMPLETION_ANNOUNCEMENT.md) - Phase 1 completion status
- [Controller Implementation](./controller-implementation.md) - Reconciler logic and phase handling
- [Testing Strategy](./testing-strategy.md) - Unit/Integration/E2E test patterns
- [ADR-017: NotificationRequest Creator Responsibility](../../../architecture/decisions/ADR-017-NOTIFICATIONREQUEST-CREATOR-RESPONSIBILITY.md) - Who creates NotificationRequest CRDs

---

## üìù Version History

### Version 1.5.0 (2025-12-11)
- Added BR-NOT-069: Routing Rule Visibility via Kubernetes Conditions
- 18 business requirements total (17 implemented, 1 approved for Kubernaut V1.0)
- Response to AIAnalysis team Conditions implementation request
- RoutingResolved condition approved, ChannelReachable deferred
- Target: December 2025 (before Kubernaut V1.0 release)

### Version 1.4.0 (2025-12-07)
- Updated documentation to reflect actual implementation status
- 17 business requirements implemented (added BR-NOT-065 to BR-NOT-068)
- Channel routing with hot-reload (DD-WE-004 integration)
- 100% BR coverage (unit + integration + E2E tests)
- 35 test files (12 unit + 18 integration + 4 E2E)

### Version 1.0 (2025-10-12)
- Initial production-ready release
- 9 business requirements implemented
- 100% BR coverage (unit + integration tests)
- Zero data loss, automatic retry, graceful degradation

---

**Document Version**: v1.5.0
**Last Updated**: December 11, 2025
**Maintained By**: Kubernaut Architecture Team
**Status**: Production-Ready (18 BRs: 17 implemented, 1 approved for V1.1)

