# DD-GATEWAY-008: Storm Aggregation First-Alert Handling Strategy

## Status
**‚ö†Ô∏è PARTIALLY SUPERSEDED** (2025-12-07) by DD-GATEWAY-011
**Original Approval**: 2025-11-18 - Alternative 2: Buffered First-Alert Aggregation
**Last Reviewed**: 2025-12-07
**Confidence**: 95%
**Next Action**: Implement Async Storm Aggregation (see Supersession Notice below)

---

## üö® SUPERSESSION NOTICE (2025-12-07)

### What Changed

**Alternative 2 (Sync Buffering in Redis)** is superseded by **Async Storm Aggregation** per DD-GATEWAY-011.

| Aspect | Alternative 2 (Superseded) | Async Storm Aggregation (Current) |
|--------|---------------------------|-----------------------------------|
| **Buffering** | Sync in Redis, wait for threshold | Async in RR status, process immediately |
| **RR Creation** | After window closes | On first alert |
| **Redis** | Required | **Not required** |
| **First alert delay** | Up to 5 minutes | **None** |
| **Single CRD guarantee** | ‚úÖ Yes | ‚úÖ Yes |

### Why the Change

1. **DD-GATEWAY-011** approved status-based persistence, enabling Redis deprecation
2. **DD-ORCHESTRATOR-001** confirms RCA uses point-in-time snapshots (storm context not blocking)
3. **Business goal**: Zero external infrastructure dependencies

### New Approach: Async Storm Aggregation

**Core Principle**: Storm detection prevents multiple CRDs, but does NOT block RCA processing.

```
Alert 1 ‚Üí Create RR immediately (phase="Pending", isStorm=false, count=1)
          ‚Üí RO processes ‚Üí AIAnalysis created ‚Üí RCA starts

Alert 2-4 ‚Üí Dedup: RR exists ‚Üí Update status.stormAggregation.count++
            ‚Üí RO already processing (point-in-time snapshot)

Alert 5 ‚Üí Threshold! Update RR (isStorm=true, count=5)
          ‚Üí RO/AI continue with original snapshot (OK per DD-ORCHESTRATOR-001)

If remediation ineffective:
          ‚Üí Retry creates new AIAnalysis with updated snapshot
          ‚Üí Storm context now available for coordinated remediation
```

**Benefits**:
- ‚úÖ **No Redis** - storm state in RR status
- ‚úÖ **No delay** - first alert processed immediately
- ‚úÖ **Single CRD** - deduplication prevents multiple CRDs
- ‚úÖ **Storm context for retry** - if first remediation ineffective

### What Remains Valid

The following from Alternative 2 **remain valid**:
- ‚úÖ Sliding window with inactivity timeout (for storm window tracking in status)
- ‚úÖ Maximum window duration (5 minutes)
- ‚úÖ Configurable threshold (default=5)
- ‚úÖ Multi-tenant isolation (per-namespace)

**Removed** (no longer needed):
- ‚ùå Redis buffering
- ‚ùå Buffer overflow handling (RR status has no size limit for counts)
- ‚ùå Blocking first alert until threshold

---

## Original Decision (Historical Reference)

**Decision**: User approved Alternative 2 with v1.0 enhancements:
- ‚úÖ Sliding window with inactivity timeout (industry best practice)
- ‚úÖ Maximum window duration (5 minutes)
- ‚úÖ Configurable threshold (default=5)
- ~~Buffer overflow handling (sampling + force close)~~ - SUPERSEDED
- ‚úÖ Multi-tenant isolation (per-namespace buffers)

## Context & Problem

### Current Behavior (BR-GATEWAY-016 Implementation)

The Gateway's storm aggregation feature currently creates **individual CRDs for alerts received BEFORE the storm threshold is reached**, then creates an aggregated CRD for subsequent alerts.

**Example with 15 alerts and threshold=2**:
1. **Alert 1**: No storm detected ‚Üí Individual CRD created (201 Created)
2. **Alert 2**: No storm detected ‚Üí Individual CRD created (201 Created)
3. **Alert 3**: Storm detected (threshold reached) ‚Üí Aggregation window starts (202 Accepted)
4. **Alerts 4-15**: Added to aggregation window (202 Accepted)
5. **After 5 seconds**: Aggregated CRD created with alerts 3-15 (13 resources)

**Total CRDs**: **3 CRDs** (2 individual + 1 aggregated)
**Expected**: **1 CRD** (all 15 resources aggregated)

### The Problem

This defeats the purpose of storm aggregation:
- ‚ùå **Partial aggregation**: First N alerts (N=threshold) create individual CRDs
- ‚ùå **AI cost not fully optimized**: 3 AI analysis requests instead of 1
- ‚ùå **Inconsistent remediation**: Some resources handled individually, others aggregated
- ‚ùå **Fragmented audit trail**: Storm split across multiple CRDs

### Business Impact

**Without full aggregation**:
- 15 alerts ‚Üí 3 CRDs ‚Üí 3 AI analysis requests ‚Üí $0.06 cost
- **Savings**: 80% reduction (vs. 15 individual CRDs)

**With full aggregation** (desired):
- 15 alerts ‚Üí 1 CRD ‚Üí 1 AI analysis request ‚Üí $0.02 cost
- **Savings**: 93% reduction (vs. 15 individual CRDs)

**Gap**: Missing 13% additional cost savings

### Key Requirements

1. **BR-GATEWAY-016**: Storm aggregation must reduce AI analysis costs by 90%+
2. **BR-GATEWAY-008**: Storm detection must identify alert storms (>10 alerts/minute)
3. **Consistency**: All alerts in a storm should be handled the same way
4. **Audit trail**: Complete storm context in single CRD
5. **Latency**: Acceptable delay for first-alert CRD creation

---

## Alternatives Considered

### Alternative 1: Current Behavior (Threshold-Based Immediate CRD Creation)

**Approach**: Create individual CRDs until threshold is reached, then start aggregation window.

**Implementation**:
```go
// Current code (pkg/gateway/server.go:808-820)
if isStorm && stormMetadata != nil {
    shouldContinue, response := s.processStormAggregation(ctx, signal, stormMetadata)
    if !shouldContinue {
        // Storm was aggregated, return response immediately
        return response, nil
    }
    // Aggregation failed, create individual CRD
}
```

**Pros**:
- ‚úÖ **Low latency**: First alerts processed immediately (no waiting)
- ‚úÖ **Simple implementation**: No buffering logic needed
- ‚úÖ **Predictable**: Deterministic behavior for first N alerts
- ‚úÖ **No retroactive changes**: CRDs never modified after creation

**Cons**:
- ‚ùå **Partial aggregation**: First N alerts not aggregated (defeats purpose)
- ‚ùå **Suboptimal cost savings**: 80% instead of 93% (13% gap)
- ‚ùå **Inconsistent handling**: Some resources individual, others aggregated
- ‚ùå **Fragmented audit**: Storm split across multiple CRDs
- ‚ùå **Complex downstream logic**: AI service must handle both individual and aggregated CRDs

**Confidence**: 40% (current implementation, but doesn't meet BR-GATEWAY-016 fully)

---

### Alternative 2: Buffered First-Alert Aggregation (Retroactive CRD Creation)

**Approach**: Buffer first N alerts in Redis, create NO CRDs until storm threshold is reached. When threshold is reached, create aggregated CRD with ALL buffered alerts.

**Implementation**:
```go
// Proposed: pkg/gateway/server.go
func (s *Server) ProcessSignal(ctx context.Context, signal *types.NormalizedSignal) (*ProcessingResponse, error) {
    // ... deduplication check ...

    // Check if this alert is part of a potential storm (buffer first N alerts)
    isBuffered, bufferID, err := s.stormBuffer.AddToBuffer(ctx, signal)
    if err != nil {
        // Buffering failed, fall back to immediate CRD creation
        return s.createRemediationRequestCRD(ctx, signal, start)
    }

    if isBuffered {
        // Alert buffered, check if threshold reached
        bufferCount, _ := s.stormBuffer.GetBufferCount(ctx, bufferID)

        if bufferCount < s.stormThreshold {
            // Below threshold, return 202 Accepted (buffered, no CRD yet)
            return NewBufferedResponse(signal.Fingerprint, bufferID, bufferCount), nil
        }

        // Threshold reached! Storm detected
        // Retrieve ALL buffered alerts (including this one)
        bufferedSignals, err := s.stormBuffer.GetBufferedSignals(ctx, bufferID)
        if err != nil {
            // Buffer retrieval failed, fall back to individual CRD
            return s.createRemediationRequestCRD(ctx, signal, start)
        }

        // Start aggregation window with ALL buffered alerts
        windowID, err := s.stormAggregator.StartAggregationWithBuffer(ctx, bufferedSignals, stormMetadata)
        if err != nil {
            // Aggregation failed, create individual CRDs for all buffered alerts
            return s.createBufferedCRDs(ctx, bufferedSignals)
        }

        // Schedule aggregated CRD creation after window expires
        go s.createAggregatedCRDAfterWindow(context.Background(), windowID, signal, stormMetadata)

        return NewStormAggregationResponse(signal.Fingerprint, windowID, stormMetadata.StormType, bufferCount, true), nil
    }

    // Not buffered (different alert type), proceed with normal flow
    return s.createRemediationRequestCRD(ctx, signal, start)
}
```

**Pros**:
- ‚úÖ **Full aggregation**: ALL alerts in storm aggregated (100% consistency)
- ‚úÖ **Optimal cost savings**: 93% reduction (meets BR-GATEWAY-016 fully)
- ‚úÖ **Single audit trail**: Complete storm context in one CRD
- ‚úÖ **Consistent handling**: All resources treated the same way
- ‚úÖ **Simplified downstream logic**: AI service only handles aggregated CRDs for storms

**Cons & Mitigations**:
- ‚ö†Ô∏è **Increased latency**: First N alerts delayed by buffer window (5-60 seconds)
  - **Context**: NOT A REAL CONCERN - MTTR reduction from 45-60 minutes to <10 minutes makes 60-second delay negligible
  - **Mitigation 1**: Correctness over speed - Complete storm context is more valuable than immediate action
  - **Mitigation 2**: 60-second buffer window is 1.6% of target MTTR (<10 min) - acceptable trade-off
  - **Mitigation 3**: Expose buffer status via `/metrics` endpoint for monitoring
  - **Mitigation 4**: Document expected latency in API specification (SLA: <60s P95 for first-alert CRD creation)

- ‚ùå **Complex implementation**: Requires buffer management in Redis
  - **Mitigation 1**: Reuse existing Redis infrastructure (already used for deduplication)
  - **Mitigation 2**: Comprehensive unit tests for buffer logic (target: 90%+ coverage)
  - **Mitigation 3**: Integration tests with real Redis (validate Lua script atomicity)
  - **Mitigation 4**: Use atomic Lua scripts for buffer operations (prevent race conditions)
  - **Mitigation 5**: Extensive error handling with fallback to individual CRDs

- ‚ùå **Buffer failure risk**: If buffer fails, must fall back to individual CRDs
  - **Mitigation 1**: Circuit breaker pattern: After N consecutive buffer failures, bypass buffering for 5 minutes
  - **Mitigation 2**: Health check endpoint monitors buffer failure rate (alert if >5%)
  - **Mitigation 3**: Graceful degradation: Buffer failure ‚Üí immediate individual CRD creation (no data loss)
  - **Mitigation 4**: Metrics: `gateway_storm_buffer_failures_total` counter for monitoring
  - **Mitigation 5**: Retry logic with exponential backoff for transient Redis failures

- ‚ùå **Memory overhead**: Buffering N signals in Redis before threshold
  - **Mitigation 1**: TTL-based expiration (60s max) prevents unbounded growth
  - **Mitigation 2**: Max buffer size limit (100 alerts per buffer) with overflow handling
  - **Mitigation 3**: Compact signal representation (store only essential fields, not full payload)
  - **Mitigation 4**: Redis memory monitoring with alerts if usage >80%
  - **Mitigation 5**: Automatic buffer eviction if Redis memory pressure detected

- ‚ùå **Edge case complexity**: What if buffer expires before threshold?
  - **Mitigation 1**: Configurable buffer expiration handler (default: create individual CRDs)
  - **Mitigation 2**: Metrics: `gateway_storm_buffer_expirations_total` to track false positives
  - **Mitigation 3**: Adaptive threshold: Lower threshold if expiration rate >10%
  - **Mitigation 4**: Alert operators if buffer expiration rate is high (indicates threshold misconfiguration)
  - **Mitigation 5**: Buffer expiration creates individual CRDs with `kubernaut.io/buffered=true` label for tracking

**Confidence**: 90% (comprehensive mitigations address all concerns; latency is non-issue given MTTR context)

---

### Alternative 3: Predictive Storm Detection (Machine Learning)

**Approach**: Use ML model to predict storms based on historical patterns. Buffer alerts when storm is predicted, create individual CRDs otherwise.

**Implementation**:
```go
// Proposed: pkg/gateway/processing/storm_predictor.go
type StormPredictor struct {
    mlModel     *MLModel
    redisClient *redis.Client
}

func (p *StormPredictor) PredictStorm(ctx context.Context, signal *types.NormalizedSignal) (isPredicted bool, confidence float64, err error) {
    // Analyze historical patterns for this alert type
    history, err := p.getAlertHistory(ctx, signal.AlertName, 1*time.Hour)
    if err != nil {
        return false, 0, err
    }

    // ML model predicts storm probability
    features := p.extractFeatures(signal, history)
    prediction := p.mlModel.Predict(features)

    // Threshold: 70% confidence to buffer
    if prediction.Confidence >= 0.7 {
        return true, prediction.Confidence, nil
    }

    return false, prediction.Confidence, nil
}
```

**Pros**:
- ‚úÖ **Intelligent buffering**: Only buffer when storm is likely
- ‚úÖ **Low latency for non-storms**: No unnecessary buffering
- ‚úÖ **Adaptive**: Learns from historical patterns
- ‚úÖ **Optimal cost savings**: 93% reduction when prediction is accurate

**Cons**:
- ‚ùå **Complex implementation**: Requires ML model training and maintenance
- ‚ùå **Prediction errors**: False positives/negatives impact user experience
- ‚ùå **Cold start problem**: No predictions for new alert types
- ‚ùå **Infrastructure overhead**: ML model serving infrastructure
- ‚ùå **V2 feature**: Too complex for V1.0 scope

**Confidence**: 30% (deferred to V2.0 - too complex for V1)

---

### Alternative 4: Hybrid Approach (Threshold + Short Buffer)

**Approach**: Buffer first 2-3 alerts for a short window (10 seconds). If threshold is reached within window, create aggregated CRD. Otherwise, create individual CRDs after window expires.

**Implementation**:
```go
// Proposed: pkg/gateway/processing/hybrid_storm_buffer.go
type HybridStormBuffer struct {
    redisClient   *redis.Client
    bufferWindow  time.Duration // 10 seconds
    threshold     int           // 2-3 alerts
}

func (b *HybridStormBuffer) ProcessAlert(ctx context.Context, signal *types.NormalizedSignal) (*ProcessingResponse, error) {
    // Add to short-term buffer
    bufferID, count, err := b.addToBuffer(ctx, signal, b.bufferWindow)
    if err != nil {
        // Buffer failed, create individual CRD immediately
        return s.createRemediationRequestCRD(ctx, signal, start)
    }

    if count >= b.threshold {
        // Threshold reached within window! Storm detected
        bufferedSignals, _ := b.getBufferedSignals(ctx, bufferID)
        return s.startStormAggregation(ctx, bufferedSignals, stormMetadata)
    }

    // Below threshold, schedule individual CRD creation after buffer window
    go b.createIndividualCRDsAfterBuffer(ctx, bufferID, b.bufferWindow)

    return NewBufferedResponse(signal.Fingerprint, bufferID, count), nil
}
```

**Pros**:
- ‚úÖ **Balanced approach**: Full aggregation for real storms, low latency for non-storms
- ‚úÖ **Moderate complexity**: Simpler than full buffering, more effective than current
- ‚úÖ **Acceptable latency**: 10-second delay acceptable for first few alerts
- ‚úÖ **Graceful degradation**: Falls back to individual CRDs if threshold not reached

**Cons**:
- ‚ùå **Still creates individual CRDs**: If threshold not reached in 10 seconds
- ‚ùå **Timing sensitivity**: 10-second window might be too short/long
- ‚ùå **Partial solution**: Doesn't guarantee 100% aggregation
- ‚ùå **Added complexity**: Requires buffer expiration logic

**Confidence**: 65% (good balance, but still partial aggregation)

---

## Decision

**PENDING USER APPROVAL**

**Recommendation**: **Alternative 2 - Buffered First-Alert Aggregation**

### Rationale

1. **Meets BR-GATEWAY-016 fully**: 93% cost reduction (vs. 80% current)
2. **Consistent behavior**: All alerts in storm handled identically
3. **Single audit trail**: Complete storm context in one CRD
4. **Acceptable latency trade-off**: 5-60 second delay acceptable for storm scenarios
5. **Simplified downstream logic**: AI service doesn't need to handle mixed CRD types

### Key Insight

**Correctness over speed**: A 60-second buffer delay is negligible in the context of MTTR reduction (45-60 min ‚Üí <10 min).

Storm scenarios are **high-volume, systemic issues** where complete context is critical:
- **MTTR context**: 60-second delay = 1.6% of target MTTR (<10 minutes) - acceptable trade-off
- **Correctness priority**: Complete storm context (all 15 resources) enables better AI root cause analysis
- **Coordinated remediation**: Single aggregated CRD prevents conflicting parallel remediations
- **Audit trail**: Single CRD with all affected resources provides complete incident context
- **Cost optimization**: 93% AI cost reduction (vs. 80% with partial aggregation)

**Trade-off decision**: Waiting 60 seconds for complete context is better than acting immediately with incomplete information.

### Implementation

**Primary Implementation Files**:
- `pkg/gateway/processing/storm_buffer.go` - New buffer management
- `pkg/gateway/server.go` - Modified ProcessSignal() flow
- `pkg/gateway/processing/storm_aggregator.go` - Enhanced StartAggregationWithBuffer()

**Data Flow**:
1. Alert arrives ‚Üí Add to buffer (Redis key: `alert:buffer:<namespace>:<alertname>`, TTL: 60s)
2. Check buffer count ‚Üí If < threshold, return 202 Accepted
3. If threshold reached ‚Üí Retrieve all buffered alerts
4. Start aggregation window with ALL buffered alerts
5. After window expires ‚Üí Create single aggregated CRD with all resources

**Graceful Degradation**:
- If buffer fails ‚Üí Fall back to immediate individual CRD creation
- If buffer expires before threshold ‚Üí Create individual CRDs for buffered alerts
- If aggregation fails ‚Üí Create individual CRDs for all buffered alerts

---

## v1.0 Implementation Details

### Window Behavior Strategy

**Decision**: **Sliding Window with Inactivity Timeout** (Industry Best Practice)

**How It Works**:
```
T=0s:   Alert 1 arrives ‚Üí Window starts, will close at T=60s
T=10s:  Alert 2 arrives ‚Üí Window timer RESETS, will now close at T=70s (10s + 60s)
T=30s:  Alert 3 arrives ‚Üí Window timer RESETS, will now close at T=90s (30s + 60s)
T=50s:  Alert 4 arrives ‚Üí Window timer RESETS, will now close at T=110s (50s + 60s)
T=110s: No more alerts for 60s ‚Üí Window closes, create aggregated CRD with all 4 alerts
```

**Key Principle**: Each new alert **resets the 60-second countdown**. Window closes only after 60 seconds of **inactivity** (no new alerts).

**Industry Alignment**: Matches Apache Storm, Spark, Logstash session windows

---

### Window Duration Limits

**Inactivity Timeout**: 60 seconds (resets on each alert)
**Maximum Window Duration**: 5 minutes (absolute limit)

**Rationale**: Prevents unbounded windows in case of continuous alert stream

**Behavior**:
- Window starts at T=0s
- Each alert resets inactivity timer to 60s
- Window FORCE CLOSES at T=300s (5 minutes) even if alerts still arriving
- New window starts for subsequent alerts

**Safety Limits**:
- **Inactivity timeout**: 60 seconds (configurable)
- **Maximum window duration**: 5 minutes (prevents unbounded windows)
- **Maximum alerts per window**: 1000 (prevents memory exhaustion)

**Industry Alignment**: Matches Logstash `timeout` (absolute max) + `inactivity_timeout` (reset) pattern

---

### Storm Detection Threshold

**Default**: 5 alerts (configurable)
**Range**: 2-20 alerts
**BR-GATEWAY-008 Alignment**: >10 alerts/minute defines "storm"

**Configuration**:
```yaml
gateway:
  storm:
    threshold: 5  # Number of alerts to trigger buffering (configurable)
    inactivity_timeout: 60s  # Window reset timeout
    max_window_duration: 5m  # Absolute maximum window duration
```

**Threshold Analysis**:
- **Threshold=2**: Very aggressive, buffers almost everything (max aggregation, adds 60s latency)
- **Threshold=5**: Balanced (recommended default)
- **Threshold=10**: Conservative, matches BR-GATEWAY-008 definition (low latency, less aggregation)

**Recommendation**:
- **Production**: threshold=5 (balanced)
- **High-volume environments**: threshold=10 (conservative)
- **Cost-optimization priority**: threshold=2 (aggressive)

---

### Buffer Overflow Handling

**Max Buffer Size**: 1000 alerts per window (per namespace)

**Behavior**:
- **< 90% capacity (< 900 alerts)**: Normal operation
- **90% capacity (900 alerts)**: Log warning, continue buffering
- **95% capacity (950 alerts)**: Enable sampling (accept 50% of alerts)
- **100% capacity (1000 alerts)**: Force close window, create CRD immediately

**Backpressure Strategy**:
```go
// When buffer reaches capacity
if bufferSize >= 1000 {
    // Force close window immediately
    return forceCloseWindow(ctx, bufferID)
}

// When buffer near capacity
if bufferSize >= 950 {
    // Enable sampling (50% acceptance rate)
    if rand.Float64() > 0.5 {
        return http.StatusAccepted, "Alert sampled due to high buffer load"
    }
}
```

**Metrics**:
- `gateway_storm_buffer_overflow_total{namespace}`: Counter of buffer overflows
- `gateway_storm_buffer_sampling_enabled{namespace}`: Gauge (0/1) indicating sampling active
- `gateway_storm_buffer_force_closed_total{namespace}`: Counter of forced window closures

---

### Late-Arriving Events

**Scenario**: Alert arrives after window has closed

**Example**:
```
T=0s:   Alert 1 ‚Üí Window starts
T=10s:  Alert 2 ‚Üí Window extends to T=70s
T=70s:  Window closes ‚Üí Create CRD
T=75s:  Alert 3 arrives (5s late) ‚Üí What happens?
```

**Decision**: **Treat as new incident** (start new window)

**Rationale**:
- Simplest implementation
- Correct for most cases (late alert likely indicates new incident)
- Avoids complexity of grace periods and window reopening

**Alternative Considered**: Reopen window for short grace period (5-10s)
- **Rejected**: Adds complexity, edge cases, and potential for unbounded windows

---

### Multi-Tenant Isolation (v1.0)

**Feature**: Per-namespace buffer limits

**Redis Key Structure**:
```redis
# Before (global buffer):
alert:storm:buffer:HighMemoryUsage = [all namespaces mixed]

# After (per-namespace buffer):
alert:storm:buffer:prod-api:HighMemoryUsage = [prod-api only]
alert:storm:buffer:dev-test:HighMemoryUsage = [dev-test only]
```

**Configuration**:
```yaml
gateway:
  storm:
    default_max_size: 1000  # Default per-namespace limit
    per_namespace_limits:   # Optional namespace-specific limits
      prod-api: 500         # Critical namespace: lower limit
      dev-test: 100         # Dev namespace: minimal limit
    global_max_size: 5000   # Absolute max across all namespaces
```

**Behavior**:
- Each namespace has independent buffer (isolation)
- Namespace A buffer full doesn't block namespace B
- Per-namespace metrics for observability
- Configurable per-namespace limits

**Benefits**:
- ‚úÖ **Isolation**: Namespace A storm doesn't block namespace B alerts
- ‚úÖ **Fairness**: Each namespace gets dedicated buffer capacity
- ‚úÖ **Observability**: Per-namespace metrics for troubleshooting
- ‚úÖ **Flexibility**: Different limits for different namespaces (prod vs dev)

**Metrics**:
```go
gateway_storm_buffer_size{namespace="prod-api"}
gateway_storm_buffer_size{namespace="dev-test"}
gateway_storm_buffer_overflow_total{namespace="prod-api"}
gateway_storm_buffer_overflow_total{namespace="dev-test"}
```

**Implementation Effort**: 8-10 hours (1-1.5 days)
- Redis key structure changes: 30 min
- Buffer limit enforcement: 2-3 hours
- Metrics & observability: 1 hour
- Unit tests: 2 hours
- Integration tests: 2 hours
- Documentation: 1 hour

**v1.1 Enhancements** (deferred):
- Dynamic limit adjustment based on usage patterns
- Priority queues (critical namespaces first)
- Fair queuing (round-robin across namespaces)
- Advanced quota management (alerts per namespace per hour)

---

## Consequences

### Positive

- ‚úÖ **Full BR-GATEWAY-016 compliance**: 93% cost reduction achieved
- ‚úÖ **Consistent storm handling**: All alerts aggregated uniformly
- ‚úÖ **Better AI analysis**: Complete context for root cause analysis
- ‚úÖ **Simplified downstream logic**: No mixed CRD types for storms
- ‚úÖ **Complete audit trail**: Single CRD contains full storm context

### Negative

- ‚ö†Ô∏è **Increased latency**: First N alerts delayed by 5-60 seconds
  - **Mitigation**: Acceptable for storm scenarios (high-volume, low-urgency)
- ‚ö†Ô∏è **Implementation complexity**: Buffer management in Redis
  - **Mitigation**: Comprehensive error handling and fallback logic
- ‚ö†Ô∏è **Memory overhead**: Buffering N signals before threshold
  - **Mitigation**: TTL-based expiration (60s), max buffer size limit (100 alerts)

### Neutral

- üîÑ **Test updates required**: Integration tests must account for buffering delay
- üîÑ **Metrics changes**: New metrics for buffer hit rate, expiration rate
- üîÑ **Documentation updates**: API behavior change (202 Accepted means buffered, not aggregated)

---

## Validation Results

### Confidence Assessment Progression

- **Initial assessment**: 60% confidence (problem identified, alternatives outlined)
- **After user decision**: TBD
- **After implementation review**: TBD

### Key Validation Points

- ‚úÖ **Problem identified**: Current implementation creates 3 CRDs instead of 1
- ‚úÖ **Business impact quantified**: 13% cost savings gap
- ‚úÖ **Alternatives evaluated**: 4 approaches with pros/cons
- ‚è∏Ô∏è **User decision pending**: Awaiting approval for Alternative 2

---

## Related Decisions

- **Builds On**: [BR-GATEWAY-016](../../services/stateless/gateway-service/BUSINESS_REQUIREMENTS.md#br-gateway-016-storm-aggregation) - Storm aggregation requirement
- **Builds On**: [BR-GATEWAY-008](../../services/stateless/gateway-service/BUSINESS_REQUIREMENTS.md#br-gateway-008-storm-detection) - Storm detection requirement
- **Related**: [DD-GATEWAY-004](DD-GATEWAY-004-redis-memory-optimization.md) - Redis memory optimization
- **Related**: [DD-015](DD-015-timestamp-based-crd-naming.md) - Timestamp-based CRD naming for unique occurrences

---

## Review & Evolution

### When to Revisit

- If storm aggregation cost savings < 90% in production
- If first-alert latency becomes user complaint
- If buffer failure rate > 5%
- If V2.0 considers ML-based prediction (Alternative 3)

### Success Metrics

- **Cost reduction**: ‚â•90% AI analysis cost savings for storms
- **Aggregation rate**: ‚â•95% of storm alerts fully aggregated
- **Buffer hit rate**: ‚â•90% of buffered alerts reach threshold
- **Latency P95**: <60 seconds for first-alert CRD creation
- **Fallback rate**: <5% buffer failures requiring individual CRDs

---

## Next Steps

1. ~~**User Decision**: Approve Alternative 2~~ ‚úÖ DONE (2025-11-18), then SUPERSEDED (2025-12-07)
2. ~~**Implementation**: Create `storm_buffer.go`~~ ‚ùå SUPERSEDED - Use RR status instead
3. **Implementation**: Update deduplication to track storm aggregation in `status.stormAggregation`
4. **Testing**: Update tests for async storm aggregation (no buffering delay)
5. **Documentation**: Update API specification - 202 means "deduplicated/aggregated"
6. **Metrics**: Add storm detection metrics (threshold reached, aggregated count)

---

## Changelog

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| v1.0 | 2025-11-18 | Gateway Team | Initial decision - Alternative 2 (Sync Redis Buffering) approved |
| v2.0 | 2025-12-07 | Gateway Team | **PARTIALLY SUPERSEDED** by DD-GATEWAY-011: Sync buffering replaced with Async Storm Aggregation. Redis dependency removed. First alert no longer delayed. Core goal preserved: single CRD per signal/storm. |
