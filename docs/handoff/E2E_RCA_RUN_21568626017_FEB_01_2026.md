# E2E Test Failures Root Cause Analysis - Run 21568626017

**Date**: February 1, 2026  
**CI Run**: https://github.com/jordigilh/kubernaut/actions/runs/21568626017  
**Commit**: `2cb5b8ac9` (fix: disable HAPI coverage)  
**Branch**: `feature/k8s-sar-user-id-stateless-services`

---

## Executive Summary

**3 of 9 E2E services failed** in CI Run 21568626017:
- ‚ùå **HolmesGPT-API (HAPI)**: 401 Unauthorized - Missing Authorization header (27/31 tests failed)
- ‚ùå **Notification**: Pod readiness timeout - controller pod failed to become ready (BeforeSuite failure)
- ‚ùå **WorkflowExecution**: tkn CLI `--override` flag error (BeforeSuite failure)

**6 services PASSED**:
- ‚úÖ AIAnalysis, DataStorage, Gateway, AuthWebhook, RemediationOrchestrator, SignalProcessing

**Must-Gather Availability**: ‚ùå NOT available for any failed service
- **Reason**: All 3 services failed in BeforeSuite (before AfterSuite could run must-gather export)
- **Fix Applied**: Commit `322ab9cd1` - CI now manually exports Kind logs when must-gather directory is missing

---

## Failure 1: HolmesGPT-API (HAPI) E2E - 401 Unauthorized

### Symptoms
```
holmesgpt_api_client.exceptions.UnauthorizedException: (401)
Reason: Unauthorized
HTTP response body: {
  "type":"about:blank",
  "title":"Unauthorized",
  "status":401,
  "detail":"Missing Authorization header with ***"
}
```

**Test Failures**: 27 out of 31 tests failed
- ‚úÖ 4 tests PASSED (skipped tests marked as SKIPPED)
- ‚ùå 27 tests FAILED with identical 401 errors

### Evidence from CI Logs

#### HAPI Deployment (Successful)
```
üöÄ Deploying HolmesGPT-API (NodePort 30120)...
   Using HAPI image: ghcr.io/jordigilh/kubernaut/holmesgpt-api:pr-24
deployment.apps/holmesgpt-api created
   HolmesGPT-API deployment applied in namespace: holmesgpt-api-e2e
‚è≥ Waiting for HAPI pod ready...
   ‚úÖ HAPI pod became ready
```

#### ServiceAccount RBAC (Created)
```
üîê Creating HolmesGPT-API ServiceAccount + RBAC...
serviceaccount/holmesgpt-api-sa created
clusterrole.rbac.authorization.k8s.io/data-storage-client created
rolebinding.rbac.authorization.k8s.io/holmesgpt-api-data-storage-client created
   ‚úÖ ServiceAccount + RBAC created
```

#### Test Execution (All 401 Errors)
```
tests/e2e/test_audit_pipeline_e2e.py::TestAuditPipelineE2E::test_llm_request_event_persisted FAILED [  1%]
tests/e2e/test_audit_pipeline_e2e.py::TestAuditPipelineE2E::test_llm_response_event_persisted FAILED [  3%]
tests/e2e/test_recovery_endpoint_e2e.py::TestRecoveryEndpointE2EHappyPath::test_recovery_endpoint_returns_complete_response_e2e FAILED [ 42%]
... (24 more identical 401 failures)
```

### Root Cause Analysis

**Problem**: HAPI pod is not using the mounted ServiceAccount token to authenticate with DataStorage.

**Evidence Chain**:
1. ‚úÖ ServiceAccount `holmesgpt-api-sa` created with proper RBAC
2. ‚úÖ Deployment includes `serviceAccountName: holmesgpt-api-sa` (added in commit `001721b93`)
3. ‚úÖ Pod became ready (health check passed)
4. ‚ùå **All HAPI requests to DataStorage receive 401 Unauthorized**

**Kubernetes Auto-Mount Behavior**:
- When `serviceAccountName` is set in a deployment, Kubernetes **auto-mounts** the token at:
  `/var/run/secrets/kubernetes.io/serviceaccount/token`
- The `ServiceAccountAuthPoolManager` in HAPI's Python code reads from this **default path** (hardcoded)

**Critical Gap**: HAPI pod must be reading the token, but DataStorage is still rejecting it as "Missing Authorization header".

**Possible Causes** (Priority Order):
1. **Token Not Being Sent** (MOST LIKELY):
   - `ServiceAccountAuthPoolManager` may not be initializing properly
   - Environment configuration may not be triggering SA auth mode
   - Check: Is `USE_SERVICE_ACCOUNT_AUTH` or similar env var required but missing?

2. **Token Mount Failed**:
   - Kubernetes auto-mount may have failed
   - Check: Pod describe should show volume mount at `/var/run/secrets/kubernetes.io/serviceaccount/`

3. **Token Format/Header Issue**:
   - Token may be read but not properly formatted in HTTP Authorization header
   - Check: HAPI logs should show token read attempt

### Investigation Required

**Step 1**: Check HAPI pod logs for authentication initialization
```bash
kubectl logs -n holmesgpt-api-e2e deployment/holmesgpt-api | grep -i "auth\|token\|serviceaccount"
```

**Step 2**: Verify token mount in pod
```bash
kubectl describe pod -n holmesgpt-api-e2e -l app=holmesgpt-api
# Look for: volumeMounts and volumes sections
```

**Step 3**: Check HAPI deployment manifest env vars
```bash
kubectl get deployment holmesgpt-api -n holmesgpt-api-e2e -o yaml | grep -A 20 "env:"
```

**Step 4**: Verify token file exists in pod
```bash
kubectl exec -n holmesgpt-api-e2e deployment/holmesgpt-api -- ls -la /var/run/secrets/kubernetes.io/serviceaccount/
kubectl exec -n holmesgpt-api-e2e deployment/holmesgpt-api -- cat /var/run/secrets/kubernetes.io/serviceaccount/token
```

### Proposed Fix Options

**Option A: Add Environment Variable** (If required by Python code)
```yaml
env:
  - name: USE_SERVICE_ACCOUNT_AUTH
    value: "true"
  - name: KUBERNETES_SERVICE_ACCOUNT_TOKEN_PATH
    value: "/var/run/secrets/kubernetes.io/serviceaccount/token"
```
**Confidence**: 60% (User feedback: "no test logic in business code" - this was reverted in commit `738162bbf`)

**Option B: Fix Python Code** (If auto-detection broken)
- Ensure `ServiceAccountAuthPoolManager` automatically detects in-cluster mode
- Check for presence of `/var/run/secrets/kubernetes.io/serviceaccount/token` at initialization
- **Confidence**: 70%

**Option C: Verify RBAC Permissions** (If token valid but insufficient)
- Ensure `data-storage-client` ClusterRole has correct verbs
- Check DataStorage logs for authorization (not authentication) errors
- **Confidence**: 30% (unlikely, as error says "Missing Authorization header", not "Forbidden")

### References
- **Authority**: DD-AUTH-014 (ServiceAccount authentication pattern)
- **History**: Commit `001721b93` added ServiceAccount + RBAC
- **Revert**: Commit `738162bbf` removed env vars based on "no test logic in business code" principle
- **Integration Test Pattern**: `aianalysis/suite_test.go:477` shows token mounting for HAPI in INT tier

---

## Failure 2: Notification E2E - Pod Readiness Timeout

### Symptoms
```
[FAILED] in [SynchronizedBeforeSuite]
controller pod did not become ready: exit status 1
error: timed out waiting for the condition on pods/notification-controller-77ccd99cdc-vc8h8
```

**Impact**: BeforeSuite failure - 0 tests executed

### Evidence from CI Logs

#### Infrastructure Setup (Successful)
```
üîê Creating Notification Controller ServiceAccount + RBAC...
serviceaccount/notification-controller created
clusterrole.rbac.authorization.k8s.io/notification-controller-role created
clusterrolebinding.rbac.authorization.k8s.io/notification-controller-rolebinding created

üîê Adding DataStorage access for notification-controller SA (DD-AUTH-014)...
‚úÖ RoleBinding created: notification-controller-datastorage-access
```

#### Deployment (Successful)
```
üöÄ Deploying Notification Controller...
   Using Notification image: ghcr.io/jordigilh/kubernaut/notification:pr-24
deployment.apps/notification-controller created
   Notification Controller deployment applied in namespace: notification-e2e
```

#### Pod Status (FAILED)
```
‚è≥ Waiting for controller pod to be created...
   ‚úÖ Controller pod created
‚è≥ Waiting for controller pod ready...
error: timed out waiting for the condition on pods/notification-controller-77ccd99cdc-vc8h8
```

### Root Cause Analysis

**Problem**: Notification controller pod fails readiness probe, preventing E2E tests from starting.

**Current Readiness Probe Configuration** (from `test/e2e/notification/manifests/notification-deployment.yaml`):
```yaml
readinessProbe:
  httpGet:
    path: /readyz
    port: 8081
  initialDelaySeconds: 15
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 6  # ‚Üê Increased from 3 in commit 072e9432f
```

**Maximum Wait Time**: 15s (initial) + (6 √ó 15s) = **105 seconds**

**Historical Context**:
- **Local execution**: Pod becomes ready in ~38 seconds ‚úÖ (documented in previous RCA)
- **CI execution**: Consistently fails after 105 seconds timeout ‚ùå

**Dependency Chain**:
1. Notification controller starts
2. Initializes DataStorage client (requires network call to DS service)
3. DS client initialization completes
4. Readiness probe succeeds

**CI Environment Characteristics**:
- Shared GitHub Actions runner (ubuntu-24.04)
- Resource contention from parallel E2E matrix jobs
- Network latency for inter-service communication (Kind NodePort ‚Üí DS pod)

### Investigation Required (Must-Gather Needed)

**Critical Logs Needed**:
1. **Notification Pod Logs**:
   ```bash
   kubectl logs -n notification-e2e notification-controller-77ccd99cdc-vc8h8
   ```
   - Look for: "DataStorage client initialized" or initialization errors
   - Check for: Connection timeouts, dial failures, context deadlines

2. **Notification Pod Events**:
   ```bash
   kubectl describe pod -n notification-e2e notification-controller-77ccd99cdc-vc8h8
   ```
   - Look for: Readiness probe failures, restart count
   - Check for: Resource constraints (CPU/memory throttling)

3. **DataStorage Service Availability**:
   ```bash
   kubectl get svc -n notification-e2e datastorage -o wide
   kubectl get pods -n notification-e2e -l app=datastorage
   ```
   - Verify DS pod is ready before NT pod starts
   - Check DS service endpoints

### Proposed Fix Options

**Option A: Increase Readiness Probe Timeout** (Band-Aid)
```yaml
failureThreshold: 10  # 15s + (10 √ó 15s) = 165 seconds
```
**Confidence**: 40% (May just delay the inevitable; doesn't fix root cause)

**Option B: Optimize DataStorage Client Initialization** (Architectural)
- Implement lazy initialization for DS client
- Don't block readiness probe on DS client availability
- Mark ready when core controller logic is initialized
- **Confidence**: 70% (Aligns with Kubernetes best practices)

**Option C: Add Startup Probe** (Separate Concerns)
```yaml
startupProbe:
  httpGet:
    path: /readyz
    port: 8081
  initialDelaySeconds: 10
  periodSeconds: 10
  failureThreshold: 18  # 10s + (18 √ó 10s) = 190 seconds for startup
readinessProbe:
  httpGet:
    path: /readyz
    port: 8081
  periodSeconds: 5
  failureThreshold: 3  # Fast failure once started
```
**Confidence**: 80% (Recommended: separates startup vs. runtime health)

**Option D: Fix DataStorage Health Check** (Root Cause - User Directive)
- User stated: "we should fix the health point properly to avoid these warmup patches. It's a hack"
- Check `if s.authenticator != nil && s.authorizer != nil` in DS health endpoint
- Ensure DS reports healthy immediately when auth middleware is ready
- **Confidence**: 90% (User-identified root cause)

### References
- **Authority**: DD-TEST-001 (E2E infrastructure standards)
- **Fix Applied**: Commit `072e9432f` increased failureThreshold from 3 to 6
- **Local Validation**: Pod ready in 38s (handoff doc: GATEWAY_E2E_COMPLETE_FIX_JAN_29_2026.md)
- **User Directive**: "fix the health point properly to avoid these warmup patches"

---

## Failure 3: WorkflowExecution E2E - tkn CLI `--override` Flag Error

### Symptoms
```
[FAILED] in [SynchronizedBeforeSuite]
failed to build and register test workflows: 
  failed to build hello-world bundle: 
    tkn bundle push failed for localhost/kubernaut-test-workflows/hello-world:v1.0.0: 
      exit status 1
Error: unknown flag: --override
```

**Impact**: BeforeSuite failure - 0 tests executed

### Evidence from CI Logs

#### Tekton Bundle Build (FAILED)
```
Building Tekton test bundles...
  Building bundle: localhost/kubernaut-test-workflows/hello-world:v1.0.0
Error: unknown flag: --override
exit status 1
```

### Root Cause Analysis

**Problem**: The `tkn bundle push` command is being called with a `--override` flag that does not exist in any version of the tkn CLI.

**Evidence**:
1. ‚úÖ tkn CLI installed (version 0.40.0, upgraded in commit `4cae2b862`)
2. ‚ùå `--override` flag does not exist in tkn v0.39.0, v0.40.0, or any other version
3. ‚úÖ **Fix already applied** in commit `9d2ddb9e4` (removed `--override` flag from `test/infrastructure/tekton_bundles.go`)

**Code Verification** (`test/infrastructure/tekton_bundles.go:122-124`):
```go
// Build bundle using tkn CLI
// Note: tkn bundle push creates an OCI image in the local container registry
// The --override flag does not exist in tkn CLI - bundles are naturally overwritable
cmd := exec.Command("tkn", "bundle", "push", bundleRef,
    "-f", pipelineYAML,
)
```

**Critical Discovery**: Fix IS in the code, but CI run `21568626017` is using commit `2cb5b8ac9`, which **includes** the fix from commit `9d2ddb9e4`.

**Unexplained Behavior**: The error should NOT be occurring if the fix is present. Possible causes:
1. **Cached build artifacts**: Go test binary may be stale
2. **Incorrect commit**: CI may be running wrong SHA
3. **Vendor/module issue**: Old version pulled from go.mod cache

### Investigation Required

**Step 1**: Verify actual code running in CI
```bash
# In CI job logs, check git SHA
git log --oneline -1 HEAD

# Verify tekton_bundles.go content
git show HEAD:test/infrastructure/tekton_bundles.go | grep -A 5 "exec.Command.*tkn"
```

**Step 2**: Check for stale test binaries
```bash
# CI should clean test cache before running
go clean -testcache
```

**Step 3**: Verify tkn CLI version
```bash
tkn version
# Expected: v0.40.0 (from .github/workflows/ci-pipeline.yml line 642)
```

### Proposed Fix Options

**Option A: Force Clean Test Cache** (If stale binary issue)
```yaml
- name: Clean Go test cache
  run: go clean -testcache
```
**Confidence**: 60%

**Option B: Re-verify Fix Presence** (If code mismatch)
- Inspect actual git commit being tested
- Ensure `9d2ddb9e4` is ancestor of `2cb5b8ac9`
- **Confidence**: 80%

**Option C: Add Debugging Output** (For future diagnosis)
```go
fmt.Fprintf(output, "DEBUG: tkn command: %v\n", cmd.Args)
```
**Confidence**: 50% (diagnostic only, doesn't fix issue)

### References
- **Fix Commit**: `9d2ddb9e4` (removed `--override` flag)
- **Upgrade Commit**: `4cae2b862` (upgraded tkn to v0.40.0)
- **Code Location**: `test/infrastructure/tekton_bundles.go:122-124`
- **CI Job**: `.github/workflows/ci-pipeline.yml:642` (tkn installation step)

---

## CI Infrastructure Issue: Must-Gather Logs Not Available

### Problem
All 3 failed E2E services show:
```
‚ö†Ô∏è  No must-gather directory found at /tmp/kubernaut-must-gather
    This may be expected if tests failed before must-gather was triggered
```

### Root Cause
- **BeforeSuite failures**: All 3 services failed during infrastructure setup
- **AfterSuite not executed**: Must-gather export only happens in AfterSuite
- **Result**: `/tmp/kubernaut-must-gather` never created, no logs to triage

### Fix Applied (Commit `322ab9cd1`)

**Enhancement to CI Workflow**:
```yaml
- name: Collect must-gather logs on failure
  if: failure()
  run: |
    if [ -d "/tmp/kubernaut-must-gather" ]; then
      # AfterSuite case: use existing must-gather
      tar -czf must-gather-e2e-${{ matrix.service }}-${TIMESTAMP}.tar.gz -C /tmp kubernaut-must-gather/
    else
      # BeforeSuite case: manually export Kind logs
      kind export logs "$EXPORT_DIR" --name "kubernaut-e2e"
      tar -czf must-gather-e2e-${{ matrix.service }}-${TIMESTAMP}.tar.gz -C "$EXPORT_DIR" .
    fi
```

**Impact**:
- ‚úÖ Must-gather artifacts will now be created for **all** E2E failures
- ‚úÖ BeforeSuite failures will have Kind cluster logs available
- ‚úÖ RCA will be possible for infrastructure setup issues

---

## Summary of Required Actions

### 1. HAPI E2E (CRITICAL - Blocking PR)
**Priority**: P0  
**Action**: Investigate why ServiceAccount token is not being used for DataStorage authentication
**Steps**:
1. Run next CI (commit `322ab9cd1`) to get must-gather logs
2. Extract HAPI pod logs from must-gather
3. Verify token mount and `ServiceAccountAuthPoolManager` initialization
4. Apply fix based on findings (likely Python code or env var configuration)

**Expected Outcome**: All 31 HAPI E2E tests pass

### 2. Notification E2E (CRITICAL - Blocking PR)
**Priority**: P0  
**Action**: Fix DataStorage health check to avoid readiness probe timeout
**Steps**:
1. Run next CI to get must-gather logs (Notification pod logs)
2. Identify DS client initialization bottleneck
3. Implement startup probe to separate startup vs. runtime health
4. Fix DS health endpoint per user directive: "check if authenticator/authorizer != nil"

**Expected Outcome**: Notification pod becomes ready within 90 seconds

### 3. WorkflowExecution E2E (MYSTERY - Needs Verification)
**Priority**: P1  
**Action**: Verify fix presence and investigate why error still occurs
**Steps**:
1. Check next CI run to see if error persists (may be transient)
2. If persists: Add test cache cleaning step to CI workflow
3. If persists: Add debugging output to tekton_bundles.go

**Expected Outcome**: Tekton bundles build successfully without `--override` flag error

### 4. CI Infrastructure (COMPLETED)
**Priority**: P2  
**Action**: ‚úÖ Ensure must-gather logs always available
**Steps**:
1. ‚úÖ Commit `322ab9cd1` pushed (manual Kind export on BeforeSuite failure)
2. Verify next CI run uploads must-gather artifacts for all 3 failed services

**Expected Outcome**: Must-gather artifacts available for all E2E failures

---

## Next Steps

1. **Monitor CI Run** (commit `322ab9cd1`):
   - Wait for new run to complete
   - Download must-gather artifacts for HAPI, Notification, WorkflowExecution
   - Extract pod logs and events for detailed RCA

2. **HAPI Priority Investigation**:
   - User can test locally: `make test-e2e-holmesgpt-api` (currently running in terminal 706161)
   - Keep cluster on failure to inspect pod state
   - Check HAPI logs: `kubectl logs -n holmesgpt-api-e2e deployment/holmesgpt-api`

3. **Notification Priority Investigation**:
   - Local test already completed (24/30 passed, pod ready in 38s)
   - Issue is CI-specific (resource/timing)
   - Focus on DS health check fix per user directive

4. **Create Fixes**:
   - HAPI: Address ServiceAccount token usage
   - Notification: Implement startup probe + fix DS health endpoint
   - WE: Verify tkn fix presence

5. **Validate**:
   - All unit tests: 9/9 PASSING ‚úÖ
   - All E2E tests: Target 9/9 PASSING
   - Create PR only when 100% pass rate achieved

---

## Confidence Assessment

| Service | Root Cause Confidence | Fix Confidence | Must-Gather Needed |
|---------|---------------------|----------------|-------------------|
| **HAPI** | 75% (token not being sent) | 60% (need pod logs) | ‚úÖ YES - CRITICAL |
| **Notification** | 80% (DS client init slow) | 80% (startup probe) | ‚úÖ YES - HELPFUL |
| **WorkflowExecution** | 50% (mystery - fix exists) | 70% (verify/cache) | ‚ö†Ô∏è  MAYBE |

**Overall Confidence**: 65% - Need must-gather logs for definitive RCA

---

**Document Authority**: CI Run 21568626017 logs, previous RCA documents, commit history  
**Created**: February 1, 2026  
**Next Review**: After CI run with commit `322ab9cd1` completes
