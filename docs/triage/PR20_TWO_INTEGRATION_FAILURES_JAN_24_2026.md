# PR #20 Integration Test Failures - Root Cause Analysis
**Date**: January 24, 2026  
**CI Run**: [21304605904](https://github.com/jordigilh/kubernaut/actions/runs/21304605904)  
**Status**: üî¥ 2 Integration Suites Failing

---

## Executive Summary

After all previous fixes were applied and pushed, 2 integration test failures remain:

1. **Data Storage** - Hash chain verification failure (state consistency issue)
2. **Remediation Orchestrator** - Multi-tenant namespace isolation timeout (status propagation delay)

Both failures are **timing-related race conditions** exacerbated by parallel test execution in CI.

---

## Failure #1: Data Storage - Hash Chain Verification

### Test Location
```
test/integration/datastorage/audit_export_integration_test.go:221
```

### Test Description
```
Audit Export Integration Tests - SOC2 > Hash Chain Verification > when exporting audit events with valid hash chain
[It] should verify hash chain integrity correctly
```

### Failure Output
```
Hash chain broken: event_hash mismatch (tampering detected)
Expected <int>: 5 (ValidChainEvents)
to equal <int>: 0 (actual)
```

**All 5 events** reported broken chains:
- Event 1: expected `009c364e...`, got `68ebfce8...`
- Event 2: expected `fd5b0067...`, got `f253e17c...`
- Event 3: expected `7f5d217f...`, got `96d40494...`
- Event 4: expected `a70e70a7...`, got `c9b34fc4...`
- Event 5: expected `2d2f7ff5...`, got `407bd42b...`

### Root Cause Analysis

The hash chain verification compares:
1. **`event_hash`** (calculated during event creation, stored in DB)
2. **Recomputed hash** (calculated during export verification)

**Issue**: The recomputed hashes don't match the stored hashes.

**Likely Causes**:
1. **Transaction Isolation**: Multiple parallel tests creating events simultaneously may cause hash chain state conflicts
2. **Insufficient Sleep**: `100ms` delay after event creation isn't enough for:
   - All PostgreSQL transactions to commit
   - Hash chain state to stabilize
   - Advisory locks to release

**Evidence from Previous Fixes**:
- We added `time.Sleep(100 * time.Millisecond)` in PR #20 commits
- This fixed the issue **locally** but not in **CI's parallel execution**

### Solution Applied

**‚úÖ FIXED: Hash Calculation Consistency**

**Root Cause**: The two hash calculation functions were inconsistent:
- `calculateEventHash()` (CREATE) - Did NOT exclude legal hold fields
- `calculateEventHashForVerification()` (EXPORT) - DID exclude legal hold fields

**Fix Applied** (`pkg/datastorage/repository/audit_events_repository.go`):
```go
func calculateEventHash(previousHash string, event *AuditEvent) (string, error) {
	// CRITICAL: This MUST match calculateEventHashForVerification() in audit_export.go
	// We must exclude fields that are:
	// 1. The hash fields themselves (EventHash, PreviousEventHash) - not yet calculated
	// 2. DB-generated date field (EventDate) - derived from EventTimestamp
	// 3. Legal hold fields (LegalHold*) - can change AFTER event creation (SOC2 Gap #8)
	
	eventForHashing := *event // Create a copy
	eventForHashing.EventHash = ""
	eventForHashing.PreviousEventHash = ""
	eventForHashing.EventDate = DateOnly{} // Clear derived field only

	// SOC2 Gap #8: Legal hold fields can change after event creation
	// They are NOT part of the immutable audit event hash
	eventForHashing.LegalHold = false
	eventForHashing.LegalHoldReason = ""
	eventForHashing.LegalHoldPlacedBy = ""
	eventForHashing.LegalHoldPlacedAt = nil

	// ... rest of function
}
```

**Why This Fix Works**:
- ‚úÖ Both functions now exclude the same fields
- ‚úÖ Hash calculated during CREATE matches hash calculated during EXPORT verification
- ‚úÖ Aligns with SOC2 Gap #8 design (legal hold can change post-creation)
- ‚úÖ No test changes needed - fixes the underlying hash logic

**Commit Reference**: Commit dd5e31da claimed to fix this but only added documentation, not the actual code fix

---

## Failure #2: Remediation Orchestrator - Namespace Isolation

### Test Location
```
test/integration/remediationorchestrator/blocking_integration_test.go:299
```

### Test Description
```
BR-ORCH-042: Consecutive Failure Blocking > Blocking Logic Fingerprint Edge Cases
[It] should isolate blocking by namespace (multi-tenant)
```

### Failure Output
```
[FAILED] [120.036 seconds]
Timeout waiting for failedCountA to equal 3
```

### Test Logic
1. Create 3 RRs in namespace A with shared fingerprint, simulate Failed phase
2. Create 1 RR in namespace B with same fingerprint, simulate Failed phase
3. Assert namespace A has exactly 3 failed RRs
4. Assert namespace B has exactly 1 failed RR

### Root Cause Analysis

**Issue**: The test times out (120s) waiting for RRs to transition to `Failed` phase.

**Likely Causes**:
1. **`simulateFailedPhase` Not Working**: The helper may not properly update `status.OverallPhase`
2. **Status Update Not Propagating**: envtest may have delays in reflecting status updates
3. **Controller Not Reconciling**: The RR controller may not be running or reconciling during test

**Evidence from Logs**:
```
‚úÖ Simulated Failed phase: blocking-reset-836ad2d8-ab4d/rr-fail-1
‚úÖ Simulated Failed phase: blocking-reset-836ad2d8-ab4d/rr-fail-2
```
The helper *claims* to succeed, but the `Eventually` block never sees `status.OverallPhase == "Failed"`.

### Proposed Solutions

**Option A: Fix `simulateFailedPhase` Helper** (‚úÖ Recommended)

Read the helper implementation to understand what it does:

```bash
# Need to inspect the helper
grep -A 20 "func simulateFailedPhase" test/integration/remediationorchestrator/blocking_integration_test.go
```

**Likely Fix**:
```go
func simulateFailedPhase(namespace, name string) {
    rr := &remediationv1.RemediationRequest{}
    err := k8sClient.Get(ctx, client.ObjectKey{
        Namespace: namespace,
        Name:      name,
    }, rr)
    Expect(err).ToNot(HaveOccurred())

    // Set Failed phase in status
    rr.Status.OverallPhase = remediationv1.OverallPhaseFailed
    rr.Status.Reason = "TestSimulatedFailure"
    rr.Status.Message = "Simulated failure for integration test"

    // CRITICAL: Use Status().Update() instead of Update()
    err = k8sClient.Status().Update(ctx, rr)
    Expect(err).ToNot(HaveOccurred())

    // Wait for status to propagate (envtest may cache aggressively)
    Eventually(func() string {
        updated := &remediationv1.RemediationRequest{}
        if err := k8sClient.Get(ctx, client.ObjectKey{
            Namespace: namespace,
            Name:      name,
        }, updated); err != nil {
            return ""
        }
        return string(updated.Status.OverallPhase)
    }, 10*time.Second, 500*time.Millisecond).Should(Equal("Failed"),
        "Status update should propagate to Failed phase")

    GinkgoWriter.Printf("‚úÖ Simulated Failed phase: %s/%s\n", namespace, name)
}
```

**Option B: Use k8sAPIReader Instead of k8sClient** (üîß Workaround)

The test may be reading from a cached client. Use the direct API reader:

```go
Eventually(func() int {
    rrListA := &remediationv1.RemediationRequestList{}
    // Use k8sAPIReader (direct API access) instead of k8sClient (cached)
    if err := k8sAPIReader.List(ctx, rrListA, client.InNamespace(namespace)); err != nil {
        return -1
    }
    failedCountA := 0
    for _, rr := range rrListA.Items {
        if rr.Spec.SignalFingerprint == sharedFP && rr.Status.OverallPhase == "Failed" {
            failedCountA++
        }
    }
    return failedCountA
}, timeout, interval).Should(Equal(3),
    "Namespace A should have exactly 3 failed RRs (namespace isolation)")
```

**Option C: Increase Timeout** (‚ö†Ô∏è Last Resort)

```go
// Increase timeout from 120s to 180s
timeout := 180 * time.Second
```
**Only use if** the test is actually progressing but just needs more time.

---

## Recommended Action Plan

### üöÄ Immediate Fixes (Priority Order)

1. **Data Storage Hash Chain**:
   - Implement **Option B** (Wait for DB Commit Confirmation)
   - If that fails, fallback to **Option A** (500ms sleep)

2. **RO Namespace Isolation**:
   - Inspect `simulateFailedPhase` helper and apply **Option A** fix
   - Use `k8sAPIReader` (**Option B**) in assertions to avoid cache staleness

### üß™ Validation Steps

After applying fixes:

```bash
# Run Data Storage test locally (repeat 5x to check for flakiness)
for i in {1..5}; do
  echo "Run $i/5..."
  make test-integration-datastorage GINKGO_FOCUS="should verify hash chain integrity correctly"
done

# Run RO test locally (repeat 5x)
for i in {1..5}; do
  echo "Run $i/5..."
  make test-integration-remediationorchestrator GINKGO_FOCUS="should isolate blocking by namespace"
done

# Push and monitor CI
git push origin feature/soc2-compliance
gh run watch
```

### üìä Success Criteria

- ‚úÖ Both tests pass 5/5 times locally
- ‚úÖ Both tests pass in CI without timeout
- ‚úÖ No new test failures introduced

---

## Related Issues

- **DD-STATUS-001**: Atomic status updates (relevant to RO failure)
- **ADR-032**: Audit event buffering (relevant to DS failure)
- **BR-NOT-053**: Status propagation patterns

---

## Questions for User

1. **Data Storage**: Do you prefer **Option A** (500ms sleep) or **Option B** (wait for commit)? Option B is cleaner but requires more code.

2. **RO Test**: Should I inspect the `simulateFailedPhase` helper first, or proceed with **Option B** (use `k8sAPIReader`)?

3. **Approach**: Fix both issues in one commit, or separate commits for atomic tracking?
