/*
Copyright 2025 Jordi Gil.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package datastorage

import (
	"context"
	"crypto/sha256"
	"database/sql"
	"fmt"
	"net/http"
	"time"

	"github.com/go-logr/logr"
	_ "github.com/jackc/pgx/v5/stdlib"
	dsgen "github.com/jordigilh/kubernaut/pkg/datastorage/ogen-client"
	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	"github.com/google/uuid"
)

// Scenario 4: Workflow Search with Hybrid Weighted Scoring (P0)
//
// Business Requirements:
// - BR-STORAGE-012: Workflow catalog persistence
// - BR-STORAGE-013: Semantic search with hybrid weighted scoring
//
// Business Value: Verify workflow search selects correct workflow using hybrid scoring
//
// Test Flow:
// 1. Deploy Data Storage Service in isolated namespace
// 2. Seed workflow catalog with 5 test workflows (various labels)
// 3. Simulate HolmesGPT API calling workflow search endpoint
// 4. Verify hybrid weighted scoring selects correct workflow
// 5. Validate boost/penalty calculations in results
//
// Expected Results:
// - Workflow search returns results ranked by hybrid score
// - GitOps workflow ranked higher than manual workflow (boost applied)
// - Mandatory labels (signal_type, severity) strictly enforced
// - Search latency <200ms (p95, local testing)
//
// Parallel Execution: âœ… ENABLED
// - Each test gets unique namespace (datastorage-e2e-p{N}-{timestamp})
// - Complete infrastructure isolation
// - No data pollution between tests

var _ = Describe("BR-DS-003: Workflow Search Accuracy - Hybrid Weighted Scoring (Semantic + Label)", Label("e2e", "workflow-search", "p0"), Ordered, func() {
	var (
		testCancel    context.CancelFunc
		testLogger    logr.Logger
		httpClient    *http.Client
		testNamespace string
		serviceURL    string
		db            *sql.DB
		testID        string
	)

	BeforeAll(func() {
		_, testCancel = context.WithTimeout(ctx, 15*time.Minute)
		testLogger = logger.WithValues("test", "workflow-search")
		httpClient = &http.Client{Timeout: 10 * time.Second}

		testLogger.Info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
		testLogger.Info("Scenario 4: Workflow Search - Setup")
		testLogger.Info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

		// Generate unique test ID for workflow isolation
		testID = fmt.Sprintf("e2e-%d-%s", GinkgoParallelProcess(), uuid.New().String()[:8])

		// Use shared deployment from SynchronizedBeforeSuite (no per-test deployment)
		// Services are deployed ONCE and shared via NodePort (no port-forwarding needed)
		testNamespace = sharedNamespace
		serviceURL = dataStorageURL
		testLogger.Info("Using shared deployment", "namespace", testNamespace, "url", serviceURL)

		// Wait for service to be ready
		testLogger.Info("â³ Waiting for Data Storage Service to be ready...")
		Eventually(func() error {
			resp, err := httpClient.Get(serviceURL + "/health/ready")
			if err != nil {
				return err
			}
			defer func() { _ = resp.Body.Close() }()
			if resp.StatusCode != http.StatusOK {
				return fmt.Errorf("service not ready: status %d", resp.StatusCode)
			}
			return nil
		}, "2m", "5s").Should(Succeed())

		testLogger.Info("âœ… Data Storage Service is ready")

		// Connect to PostgreSQL for direct database verification (using shared NodePort - no port-forward needed)
		testLogger.Info("ðŸ”Œ Connecting to PostgreSQL via NodePort...")

		connStr := "host=localhost port=25433 user=slm_user password=test_password dbname=action_history sslmode=disable" // Per DD-TEST-001
		var err error
		db, err = sql.Open("pgx", connStr)
		Expect(err).ToNot(HaveOccurred())
		Expect(db.Ping()).To(Succeed())

		testLogger.Info("âœ… PostgreSQL connection established")
	})

	AfterAll(func() {
		testLogger.Info("ðŸ§¹ Cleaning up test namespace...")
		if db != nil {
			if err := db.Close(); err != nil {
				testLogger.Info("warning: failed to close database connection", "error", err)
			}
		}
		if testCancel != nil {
			testCancel()
		}

		// Note: Namespace cleanup is handled by Kind cluster deletion in SynchronizedAfterSuite
		// Individual namespaces are left for debugging if tests fail
	})

	Context("when searching for workflows with hybrid weighted scoring", func() {
		It("should select correct workflow using hybrid scoring (BR-STORAGE-013)", func() {
			testLogger.Info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
			testLogger.Info("Test: Workflow Search with Hybrid Weighted Scoring")
			testLogger.Info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

			// ARRANGE: Seed workflow catalog with 5 test workflows
			testLogger.Info("ðŸ“¦ Seeding workflow catalog with test workflows...")

			// Test workflows with all 7 mandatory labels per DD-WORKFLOW-001
			// JSON labels use hyphenated keys (signal_type, risk_tolerance)
			// YAML content uses underscored keys (signal_type, risk_tolerance)
			workflows := []struct {
				workflowID  string
				name        string
				description string
				labels      map[string]interface{} // JSON labels (hyphenated keys)
				embedding   []float64
			}{
				{
					workflowID:  fmt.Sprintf("wf-gitops-argocd-%s", testID),
					name:        "OOM Recovery with GitOps (ArgoCD)",
					description: "Recover from OOMKilled using GitOps with ArgoCD",
					labels: map[string]interface{}{
						"signal_type": "OOMKilled",  // mandatory (DD-WORKFLOW-001 v1.4)
						"severity":    "critical",   // mandatory
						"component":   "deployment", // mandatory
						"priority":    "P0",         // mandatory
						"environment": "production", // mandatory
					},
					embedding: nil, // V1.0: no embeddings
				},
				{
					workflowID:  fmt.Sprintf("wf-gitops-flux-%s", testID),
					name:        "OOM Recovery with GitOps (Flux)",
					description: "Recover from OOMKilled using GitOps with Flux",
					labels: map[string]interface{}{
						"signal_type": "OOMKilled",  // mandatory (DD-WORKFLOW-001 v1.4)
						"severity":    "critical",   // mandatory
						"component":   "deployment", // mandatory
						"priority":    "P0",         // mandatory
						"environment": "production", // mandatory
					},
					embedding: nil, // V1.0: no embeddings
				},
				{
					workflowID:  fmt.Sprintf("wf-manual-%s", testID),
					name:        "OOM Recovery with Manual Intervention",
					description: "Recover from OOMKilled using manual kubectl commands",
					labels: map[string]interface{}{
						"signal_type":         "OOMKilled",
						"severity":            "critical",
						"resource_management": "manual",
						"environment":         "production",
						"business_category":   "revenue-critical",
						"priority":            "P0",
						"risk_tolerance":      "low",
						"component":           "deployment",
					},
					embedding: generateTestEmbedding("OOMKilled critical manual kubectl production"),
				},
				{
					workflowID:  fmt.Sprintf("wf-generic-%s", testID),
					name:        "OOM Recovery (Generic)",
					description: "Generic OOM recovery workflow",
					labels: map[string]interface{}{
						"signal_type": "OOMKilled",  // mandatory (DD-WORKFLOW-001 v1.4)
						"severity":    "critical",   // mandatory
						"component":   "pod",        // mandatory
						"priority":    "P1",         // mandatory
						"environment": "production", // mandatory
					},
					embedding: nil, // V1.0: no embeddings
				},
				{
					workflowID:  fmt.Sprintf("wf-different-signal-%s", testID),
					name:        "CrashLoopBackOff Recovery",
					description: "Recover from CrashLoopBackOff",
					labels: map[string]interface{}{
						"signal_type": "CrashLoopBackOff", // mandatory (DD-WORKFLOW-001 v1.4)
						"severity":    "high",             // mandatory
						"component":   "pod",              // mandatory
						"priority":    "P2",               // mandatory
						"environment": "staging",          // mandatory
					},
					embedding: nil, // V1.0: no embeddings
				},
			}

			// Create workflows via API with ADR-043 compliant content
			for i, wf := range workflows {
				// V1.0: Only 4 mandatory labels
				severity := wf.labels["severity"]
				environment := wf.labels["environment"]
				priority := wf.labels["priority"]
				component := wf.labels["component"]

				// Generate ADR-043 compliant workflow-schema.yaml content
				// V1.0: Simplified 4-label schema
				workflowSchemaContent := fmt.Sprintf(`apiVersion: kubernaut.io/v1alpha1
kind: WorkflowSchema
metadata:
  workflow_id: %s
  version: "1.0.0"
  description: %s
labels:
  severity: %s
  environment: %s
  priority: %s
  component: %s
parameters:
  - name: NAMESPACE
    type: string
    required: true
    description: Target namespace
  - name: POD_NAME
    type: string
    required: true
    description: Name of the pod to remediate
execution:
  engine: tekton
  bundle: ghcr.io/kubernaut/workflows/test:v1.0.0
`, wf.workflowID, wf.description, severity, environment, priority, component)

				// DD-WORKFLOW-002 v2.4: container_image is MANDATORY with digest
				containerImage := fmt.Sprintf("ghcr.io/kubernaut/workflows/%s:v1.0.0@sha256:%064d", wf.workflowID, i+1)

				// Calculate content_hash (SHA-256 of workflow schema content)
				contentHashBytes := sha256.Sum256([]byte(workflowSchemaContent))
				contentHash := fmt.Sprintf("%x", contentHashBytes)

				// DD-WORKFLOW-002 v3.0: workflow_name is the human identifier, workflow_id is auto-generated UUID
				// DD-API-001: Use typed OpenAPI struct
				workflowReq := dsgen.RemediationWorkflow{
					WorkflowName:    wf.workflowID, // Using workflowID test field as workflow_name
					Version:         "1.0.0",
					Name:            wf.name,
					Description:     wf.description,
					Content:         workflowSchemaContent,
					ContentHash:     contentHash, // Required per OpenAPI spec
					ExecutionEngine: "tekton",    // Required per OpenAPI spec
					Labels: dsgen.MandatoryLabels{
						SignalType:  wf.labels["signal_type"].(string),
						Severity:    dsgen.MandatoryLabelsSeverity(wf.labels["severity"].(string)),
						Component:   wf.labels["component"].(string),
						Priority:    dsgen.MandatoryLabelsPriority(wf.labels["priority"].(string)),
						Environment: wf.labels["environment"].(string),
					},
					ContainerImage: dsgen.NewOptString(containerImage),
					Status:         dsgen.RemediationWorkflowStatusActive,
				}

				_, err := dsClient.CreateWorkflow(ctx, &workflowReq)
				Expect(err).ToNot(HaveOccurred())
				Expect(201).To(Equal(http.StatusCreated),
					fmt.Sprintf("Failed to create workflow %d: Status=%d", i+1, 201))

				testLogger.Info(fmt.Sprintf("âœ… Created workflow %d/%d", i+1, len(workflows)),
					"workflow_id", wf.workflowID)
			}

			// Verify workflows were created in database
			// DD-WORKFLOW-002 v3.0: workflow_id is UUID, use workflow_name for filtering
			var count int
			err := db.QueryRow("SELECT COUNT(*) FROM remediation_workflow_catalog WHERE workflow_name LIKE $1",
				fmt.Sprintf("%%-%s", testID)).Scan(&count)
			Expect(err).ToNot(HaveOccurred())
			Expect(count).To(Equal(5), "All 5 workflows should be created")

			testLogger.Info("âœ… All workflows created successfully")

			// ACT: Search for OOMKilled workflows with V1.0 label-only filtering
			testLogger.Info("ðŸ” Searching for workflows with V1.0 label-only scoring...")
			testLogger.Info("   Filters: signal_type=OOMKilled, severity=critical, component=deployment, environment=production, priority=P0")

			// DD-API-001: Use typed OpenAPI struct for workflow search
			topK := 5
			searchReq := dsgen.WorkflowSearchRequest{
				Filters: dsgen.WorkflowSearchFilters{
					SignalType:  "OOMKilled",                                 // mandatory (DD-WORKFLOW-001 v1.4)
					Severity:    dsgen.WorkflowSearchFiltersSeverityCritical, // mandatory
					Component:   "deployment",                                // mandatory
					Environment: "production",                                // mandatory
					Priority:    dsgen.WorkflowSearchFiltersPriorityP0,       // mandatory
				},
				TopK: dsgen.NewOptInt(topK),
			}

			start := time.Now()
			resp, err := dsClient.SearchWorkflows(ctx, &searchReq)
			searchDuration := time.Since(start)
			Expect(err).ToNot(HaveOccurred())

			testLogger.Info("âœ… Search completed", "duration", searchDuration)

			// ASSERT: Verify V1.0 semantic search results (base similarity only)
			// Authority: DD-WORKFLOW-002 v3.0 (flat response structure, UUID workflow_id)
			// Authority: DD-WORKFLOW-004 v2.0 (V1.0: confidence = base_similarity)
			// DD-API-001: Use typed response from OpenAPI client with type assertion
			Expect(resp).ToNot(BeNil())
			searchResults, ok := resp.(*dsgen.WorkflowSearchResponse)
			Expect(ok).To(BeTrue(), "Expected *WorkflowSearchResponse type")
			Expect(searchResults.Workflows).ToNot(BeNil())
			results := searchResults.Workflows

			testLogger.Info("ðŸ“Š Search Results (V1.0 - Base Similarity Only):")
			for i, result := range results {
				testLogger.Info(fmt.Sprintf("  %d. %s", i+1, result.Title),
					"confidence", result.Confidence)
			}

			// Assertion 1: Search should return results
			Expect(results).ToNot(BeEmpty(), "Search should return workflows")
			Expect(searchResults.TotalResults.Value).To(BeNumerically(">=", 1), "Should return at least 1 matching workflow")

			// Assertion 2: All results should have signal_type matching the query
			// DD-WORKFLOW-002 v3.0: signal_type is singular string (not array)
			for _, result := range results {
				Expect(result.SignalType).To(Equal("OOMKilled"),
					"All results should have matching signal_type")
			}

			// Assertion 3: Confidence scores should be valid (0.0-1.0)
			for _, result := range results {
				Expect(result.Confidence).To(BeNumerically(">=", 0.0),
					"Confidence should be >= 0.0")
				Expect(result.Confidence).To(BeNumerically("<=", 1.0),
					"Confidence should be <= 1.0")
			}

			// Assertion 4: Results should be ordered by confidence descending
			for i := 1; i < len(results); i++ {
				Expect(results[i-1].Confidence).To(BeNumerically(">=", results[i].Confidence),
					"Results should be ordered by confidence descending")
			}

			// Assertion 5: Search latency should be acceptable (<1s for E2E environment)
			// Note: E2E environment (Docker/Kind + PostgreSQL) has overhead vs production
			Expect(searchDuration).To(BeNumerically("<", 1000*time.Millisecond),
				"Search latency should be <1s for E2E test (Docker/Kind overhead)")

			// Assertion 6: CrashLoopBackOff workflow should NOT be returned (different signal_type)
			// DD-WORKFLOW-002 v3.0: WorkflowID is UUID, verify signal_type filtering works
			for _, result := range results {
				// DD-WORKFLOW-002 v3.0: WorkflowID is UUID format
				Expect(result.WorkflowID).To(MatchRegexp(`^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$`),
					"WorkflowID should be UUID format")
				// Verify CrashLoopBackOff is filtered out by signal_type
				Expect(result.SignalType).ToNot(Equal("CrashLoopBackOff"),
					"CrashLoopBackOff workflow should NOT be returned (mandatory label mismatch)")
			}

			testLogger.Info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
			testLogger.Info("âœ… V1.0 Semantic Search Validation Complete")
			testLogger.Info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
			testLogger.Info("Key Validations (DD-WORKFLOW-004 v1.5):")
			testLogger.Info("  âœ… Mandatory label filtering enforced (signal_type, severity)")
			testLogger.Info("  âœ… Confidence scores valid (0.0-1.0)")
			testLogger.Info("  âœ… Results ordered by confidence descending")
			testLogger.Info("  âœ… Search latency <200ms")
			testLogger.Info("  âœ… V1.0: Label-based scoring with boost/penalty (0.10, 0.05, 0.02)")
			testLogger.Info("  âœ… V2.0+: Vector embeddings + label weights (hybrid semantic)")
			testLogger.Info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
		})
	})
})

// generateTestEmbedding creates a simple test embedding based on text
// NOTE: V1.0 uses label-only architecture - embeddings are for future V2.0 semantic search
func generateTestEmbedding(text string) []float64 {
	// Generate a deterministic 768-dimensional embedding (per migration 016)
	// For testing, we use a simple hash-based approach
	embedding := make([]float64, 768)
	hash := 0
	for _, c := range text {
		hash = (hash*31 + int(c)) % 1000
	}

	// Fill embedding with deterministic values based on hash
	for i := range embedding {
		embedding[i] = float64((hash+i)%100) / 100.0
	}

	return embedding
}
