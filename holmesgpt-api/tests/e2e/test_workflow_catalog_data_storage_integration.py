"""
Copyright 2025 Jordi Gil.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

"""
Integration Tests for Workflow Catalog with Real Data Storage Service

Business Requirement: BR-STORAGE-013 - Semantic Search for Remediation Workflows
Design Decisions:
  - DD-WORKFLOW-002 v3.0 - MCP Workflow Catalog Architecture (UUID workflow_id, flat response)
  - DD-STORAGE-008 - Workflow Catalog Schema
  - DD-WORKFLOW-004 v2.0 - V1.0: Base similarity only (no boost/penalty - deferred to V2.0+)
  - DD-TEST-001 - Port Allocation Strategy

üîÑ PRODUCTION INTEGRATION TESTS

These tests validate the complete workflow search flow with real services:
  - Data Storage Service (Go + PostgreSQL + pgvector)
  - Embedding Service (Python + sentence-transformers)
  - HolmesGPT API Workflow Catalog Toolset (Python)

Test Coverage (6 tests):
1. End-to-end workflow search with semantic similarity
2. Hybrid scoring validation (base + boost - penalty)
3. Empty results handling (no matching workflows)
4. Filter validation (mandatory and optional labels)
5. Top-k limiting (result count validation)
6. Error handling (service unavailable, timeout)

Prerequisites:
  - Docker/Podman running
  - Data Storage Service container (port 18090 per DD-TEST-001)
  - Embedding Service container (port 18000 per DD-TEST-001)
  - PostgreSQL with pgvector extension (port 15433 per DD-TEST-001)
  - Redis (port 16380 per DD-TEST-001)
  - Test data bootstrapped in database

Setup:
  Run: ./tests/integration/setup_workflow_catalog_integration.sh

Teardown:
  Run: ./tests/integration/teardown_workflow_catalog_integration.sh
"""

import pytest
import json
import time
import requests
from typing import Dict, Any, List
from src.toolsets.workflow_catalog import (
    SearchWorkflowCatalogTool,
    WorkflowCatalogToolset
)
from holmes.core.tools import StructuredToolResultStatus

# Data Storage OpenAPI Client imports (DD-API-001)
from datastorage import ApiClient as DSApiClient, Configuration as DSConfiguration
from datastorage.api.workflow_catalog_api_api import WorkflowCatalogAPIApi
from datastorage.models.workflow_search_request import WorkflowSearchRequest
from datastorage.models.workflow_search_filters import WorkflowSearchFilters

# Note: Uses fixtures from tests/e2e/conftest.py (V1.0 Go infrastructure)
# DATA_STORAGE_URL is provided via data_storage_stack fixture


# Test timeouts
SERVICE_STARTUP_TIMEOUT = 30  # seconds
HTTP_REQUEST_TIMEOUT = 10  # seconds


# ========================================
# FIXTURES
# ========================================

@pytest.fixture(scope="module")
def wait_for_services(data_storage_stack):
    """
    Wait for Data Storage Service to be ready

    This fixture depends on data_storage_stack from conftest.py,
    which uses Go-managed Kind cluster infrastructure.
    """
    print(f"\n‚úÖ Services ready:")
    print(f"   Data Storage: {data_storage_stack}")

    yield {"data_storage_url": data_storage_stack}
    print(f"\nüßπ Integration tests complete")


@pytest.fixture(scope="module")
def workflow_catalog_tool(wait_for_services):
    """
    Create WorkflowCatalogTool configured for integration testing

    Configures tool to use Data Storage Service from Go infrastructure.
    """
    toolset = WorkflowCatalogToolset()
    tool = toolset.tools[0]

    # Override Data Storage URL for E2E testing (Go infrastructure: port 8081)
    data_storage_url = wait_for_services["data_storage_url"]
    tool.data_storage_url = data_storage_url

    print(f"üîß Workflow Catalog Tool configured: {data_storage_url}")
    return tool


@pytest.fixture(scope="module")
def test_workflows():
    """
    Test workflow data that should be bootstrapped in database

    Returns expected workflow data for validation.
    These workflows should be created by the bootstrap script.
    """
    return [
        {
            "workflow_id": "oomkill-increase-memory-limits",
            "version": "1.0.0",
            "title": "OOMKill Remediation - Increase Memory Limits",
            "description": "Increases memory limits for pods experiencing OOMKilled events",
            "labels": {
                "signal-type": "OOMKilled",
                "severity": "critical",
                "resource-management": "gitops",
                "gitops-tool": "argocd",
                "environment": "production"
            }
        },
        {
            "workflow_id": "oomkill-scale-down-replicas",
            "version": "1.0.0",
            "title": "OOMKill Remediation - Scale Down Replicas",
            "description": "Reduces replica count for deployments experiencing OOMKilled",
            "labels": {
                "signal-type": "OOMKilled",
                "severity": "high",
                "resource-management": "manual"
            }
        },
        {
            "workflow_id": "crashloop-fix-configuration",
            "version": "1.0.0",
            "title": "CrashLoopBackOff - Fix Configuration",
            "description": "Identifies and fixes configuration issues causing CrashLoopBackOff",
            "labels": {
                "signal-type": "CrashLoopBackOff",
                "severity": "high",
                "resource-management": "gitops"
            }
        }
    ]


# ========================================
# INTEGRATION TESTS
# ========================================

class TestWorkflowCatalogEndToEnd:
    """
    BR-STORAGE-013: End-to-End Workflow Search Integration Tests

    BEHAVIOR: Complete workflow search flow with real services
    CORRECTNESS: Results match expected format and business rules

    üîÑ PRODUCTION: Tests with real Data Storage Service + PostgreSQL + pgvector
    """

    def test_semantic_search_with_exact_match_br_storage_013(
        self,
        workflow_catalog_tool,
        test_workflows,
        test_workflows_bootstrapped
    ):
        """
        BR-STORAGE-013: Semantic search returns workflows with high similarity

        BEHAVIOR: Tool finds workflows matching query semantically
        CORRECTNESS: Results ranked by confidence (cosine similarity)

        DD-WORKFLOW-002 v3.0 Response Format:
        - workflow_id: UUID (auto-generated primary key)
        - title: string (display name)
        - description: string
        - signal_type: string (singular, not array)
        - container_image: string
        - container_digest: string
        - confidence: float (0.0-1.0 similarity score)

        V1.0: No boost/penalty - just base similarity (deferred to V2.0+)
        """
        # Execute search with lower min_score for testing
        result = workflow_catalog_tool.invoke(params={
            "query": "OOMKilled critical",
            "top_k": 5
        })

        # BEHAVIOR VALIDATION: Successful search
        assert result.status == StructuredToolResultStatus.SUCCESS, \
            f"BR-STORAGE-013: Search must succeed, got error: {result.error}"

        # CORRECTNESS VALIDATION: Result format
        data = json.loads(result.data)
        assert "workflows" in data, \
            "DD-WORKFLOW-002: Response must contain 'workflows' array"

        workflows = data["workflows"]
        assert len(workflows) > 0, \
            "BR-STORAGE-013: Must find at least 1 OOMKilled workflow"

        # CORRECTNESS VALIDATION: DD-WORKFLOW-002 v3.0 Workflow structure
        first_workflow = workflows[0]
        required_fields = [
            "workflow_id", "title", "description",
            "signal_type", "confidence",
            "container_image", "container_digest"
        ]

        for field in required_fields:
            assert field in first_workflow, \
                f"DD-WORKFLOW-002 v3.0: Workflow must have '{field}' field"

        # CORRECTNESS VALIDATION: UUID format for workflow_id
        import re
        uuid_pattern = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.I)
        assert uuid_pattern.match(first_workflow["workflow_id"]), \
            f"DD-WORKFLOW-002 v3.0: workflow_id must be UUID, got: {first_workflow['workflow_id']}"

        # CORRECTNESS VALIDATION: Confidence score (V1.0 - base similarity only)
        assert 0.0 <= first_workflow["confidence"] <= 1.0, \
            "DD-WORKFLOW-004 v2.0: confidence must be between 0.0 and 1.0"

        # CORRECTNESS VALIDATION: Signal type matching (singular string in v3.0)
        assert first_workflow["signal_type"] == "OOMKilled", \
            f"BR-STORAGE-013: Top result must match signal type, got: {first_workflow['signal_type']}"

        print(f"‚úÖ Found {len(workflows)} workflows, top confidence: {first_workflow['confidence']:.3f}")

    def test_confidence_scoring_dd_workflow_004_v1(
        self,
        workflow_catalog_tool,
        test_workflows,
        test_workflows_bootstrapped
    ):
        """
        DD-WORKFLOW-004 v2.0: V1.0 scoring uses base similarity only

        BEHAVIOR: Workflows ranked by cosine similarity (confidence)
        CORRECTNESS: confidence = (1 - cosine_distance)

        DD-WORKFLOW-004 v2.0 V1.0 Scoring Formula:
        - confidence = base_similarity (no boost/penalty)
        - Custom labels are customer-defined via Rego policies
        - Configurable weights deferred to V2.0+

        Test Flow:
        1. Search for "OOMKilled critical"
        2. Expect multiple OOMKilled workflows
        3. Confirm workflows sorted by confidence (descending)
        """
        # Execute search
        result = workflow_catalog_tool.invoke(params={
            "query": "OOMKilled critical",
            "top_k": 5
        })

        assert result.status == StructuredToolResultStatus.SUCCESS, \
            f"Search failed: {result.error}"

        data = json.loads(result.data)
        workflows = data["workflows"]

        assert len(workflows) > 0, \
            "DD-WORKFLOW-004: Must find at least 1 workflow"

        # CORRECTNESS VALIDATION: Results sorted by confidence (descending)
        for i in range(len(workflows) - 1):
            assert workflows[i]["confidence"] >= workflows[i + 1]["confidence"], \
                f"DD-WORKFLOW-004: Results must be sorted by confidence descending"

        # CORRECTNESS VALIDATION: Confidence values valid
        for wf in workflows:
            assert 0.0 <= wf["confidence"] <= 1.0, \
                f"DD-WORKFLOW-004: confidence must be 0.0-1.0, got {wf['confidence']}"

        print(f"‚úÖ V1.0 scoring validated: {len(workflows)} workflows, " +
              f"top confidence: {workflows[0]['confidence']:.3f}")

    def test_empty_results_handling_br_hapi_250(
        self,
        workflow_catalog_tool
    ):
        """
        BR-HAPI-250: Tool handles queries with no matching workflows gracefully

        BEHAVIOR: Returns empty array (not error) for no matches
        CORRECTNESS: Response format valid with empty workflows array

        Test Flow:
        1. Search for non-existent signal type
        2. Expect SUCCESS status (not ERROR)
        3. Confirm empty workflows array returned
        """
        # Execute search with non-existent signal type
        result = workflow_catalog_tool.invoke(params={
            "query": "NonExistentSignalType12345 critical",
            "top_k": 5
        })

        # BEHAVIOR VALIDATION: Success (not error)
        assert result.status == StructuredToolResultStatus.SUCCESS, \
            "BR-HAPI-250: No matches is not an error condition"

        # CORRECTNESS VALIDATION: Empty array
        data = json.loads(result.data)
        assert data["workflows"] == [], \
            "BR-HAPI-250: Empty results must return empty array, not null"

        print(f"‚úÖ Empty results handled gracefully")

    def test_filter_validation_dd_llm_001(
        self,
        workflow_catalog_tool,
        test_workflows
    ):
        """
        DD-LLM-001: Mandatory label filters applied correctly

        BEHAVIOR: Only workflows matching mandatory labels returned
        CORRECTNESS: All results have matching signal_type

        DD-WORKFLOW-002 v3.0: signal_type is singular string (not array)

        Test Flow:
        1. Search for "CrashLoopBackOff high"
        2. Expect only CrashLoopBackOff workflows
        3. Validate no OOMKilled workflows returned
        """
        # Execute search for CrashLoopBackOff
        result = workflow_catalog_tool.invoke(params={
            "query": "CrashLoopBackOff high",
            "top_k": 5
        })

        assert result.status == StructuredToolResultStatus.SUCCESS, \
            f"Search failed: {result.error}"

        data = json.loads(result.data)
        workflows = data["workflows"]

        # If we got results, validate signal_type
        if len(workflows) > 0:
            # CORRECTNESS VALIDATION: DD-WORKFLOW-002 v3.0 signal_type is string
            for workflow in workflows:
                assert "signal_type" in workflow, \
                    "DD-WORKFLOW-002 v3.0: Workflow must have signal_type field"
                assert isinstance(workflow["signal_type"], str), \
                    f"DD-WORKFLOW-002 v3.0: signal_type must be string, got {type(workflow['signal_type'])}"

            # Check if CrashLoopBackOff is in results
            crashloop_count = sum(1 for wf in workflows if wf["signal_type"] == "CrashLoopBackOff")
            print(f"‚úÖ Filter validation: {crashloop_count} CrashLoopBackOff workflows found")
        else:
            # Empty results with semantic search is valid (embeddings may not match well)
            print(f"‚úÖ Filter validation passed: 0 workflows found (semantic search)")

        print(f"‚úÖ Total workflows returned: {len(workflows)}")

    def test_top_k_limiting_br_hapi_250(
        self,
        workflow_catalog_tool
    ):
        """
        BR-HAPI-250: Tool respects top_k parameter

        BEHAVIOR: Result count <= top_k
        CORRECTNESS: Most relevant workflows returned first (by confidence)

        DD-WORKFLOW-002 v3.0: Uses 'confidence' field (not 'similarity_score')

        Test Flow:
        1. Search with top_k=1
        2. Expect exactly 1 result (or 0 if no matches)
        3. Confirm result has positive confidence
        """
        # Execute search with top_k=1
        result = workflow_catalog_tool.invoke(params={
            "query": "OOMKilled critical",
            "top_k": 1
        })

        assert result.status == StructuredToolResultStatus.SUCCESS, \
            f"Search failed: {result.error}"

        data = json.loads(result.data)
        workflows = data["workflows"]

        # CORRECTNESS VALIDATION: Result count limit
        assert len(workflows) <= 1, \
            f"BR-HAPI-250: top_k=1 must return at most 1 result, got {len(workflows)}"

        if len(workflows) == 1:
            # DD-WORKFLOW-002 v3.0: Validate confidence (not similarity_score)
            assert "confidence" in workflows[0], \
                "DD-WORKFLOW-002 v3.0: Workflow must have 'confidence' field"
            first_score = workflows[0]["confidence"]
            assert first_score > 0, \
                "BR-HAPI-250: Returned workflow must have positive confidence"

        print(f"‚úÖ Top-k limiting validated: {len(workflows)} result(s) returned")

    def test_error_handling_service_unavailable_br_storage_013(
        self,
        workflow_catalog_tool
    ):
        """
        BR-STORAGE-013: Tool handles Data Storage Service errors gracefully

        BEHAVIOR: Returns ERROR status with meaningful message
        CORRECTNESS: Error message indicates service unavailable

        Test Flow:
        1. Configure tool with invalid Data Storage URL
        2. Execute search
        3. Expect ERROR status (not exception)
        4. Validate error message is meaningful
        """
        # Create tool with invalid URL
        toolset = WorkflowCatalogToolset()
        tool = toolset.tools[0]
        tool.data_storage_url = "http://invalid-host:99999"

        # Execute search
        result = tool.invoke(params={
            "query": "OOMKilled critical",
            "top_k": 5
        })

        # BEHAVIOR VALIDATION: Error status (not exception)
        assert result.status == StructuredToolResultStatus.ERROR, \
            "BR-STORAGE-013: Must return ERROR status when service unavailable"

        # CORRECTNESS VALIDATION: Meaningful error message
        assert result.error is not None, \
            "BR-STORAGE-013: Error message must be provided"

        # Error message can indicate various network issues
        error_lower = result.error.lower()
        error_indicators = ["data storage", "connect", "failed", "error", "parse", "unavailable", "host"]
        has_error_indicator = any(indicator in error_lower for indicator in error_indicators)
        assert has_error_indicator, \
            f"BR-STORAGE-013: Error message must indicate service issue, got: {result.error}"

        print(f"‚úÖ Error handling validated: {result.error}")


# ========================================
# TEST UTILITIES
# ========================================

def verify_test_data_exists():
    """
    Verify test data is bootstrapped in Data Storage Service

    This function can be called manually to check if test data exists.
    Uses Data Storage OpenAPI client per DD-API-001.
    """
    try:
        # Configure Data Storage OpenAPI client
        config = DSConfiguration(host=DATA_STORAGE_URL)
        config.timeout = 60  # CRITICAL: Prevent "read timeout=0" errors
        api_client = DSApiClient(configuration=config)
        search_api = WorkflowCatalogAPIApi(api_client=api_client)

        # Construct search request using OpenAPI models
        filters = WorkflowSearchFilters(**{
            "signal-type": "OOMKilled",
            "severity": "critical"
        })
        search_request = WorkflowSearchRequest(
            query="OOMKilled critical",
            filters=filters,
            top_k=10,
            min_score=0.0
        )

        # Execute search using OpenAPI client
        response = search_api.search_workflows(
            workflow_search_request=search_request
        )

        # Response is a model object, access workflows directly
        workflow_count = len(response.workflows)
        print(f"‚úÖ Test data verified: {workflow_count} workflows found")
        return True

    except Exception as e:
        print(f"‚ùå Test data verification failed: {e}")
        return False


if __name__ == "__main__":
    # Manual test data verification
    print("üîç Verifying test data...")
    verify_test_data_exists()

