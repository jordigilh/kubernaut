# DD-009: Audit Write Error Recovery - Dead Letter Queue Pattern

**Status**: âœ… Approved (Phase 0 Day 0.1 - User Decision 2c)
**Date**: 2025-11-02
**Decision Makers**: Development Team, User Approval
**Supersedes**: None
**Authority**: ADR-032 v1.1 mandate "No Audit Loss"

---

## ğŸ“‹ **Context**

ADR-032 v1.1 ("Data Access Layer Isolation") mandates that all CRD controllers write audit data exclusively via Data Storage Service REST API. This creates a critical dependency:

**Problem**: What happens when Data Storage Service write fails?

**Impact Areas**:
1. **Audit Completeness**: ADR-032 mandates "No Audit Loss" for 7+ year compliance
2. **Service Availability**: Controller reconciliation should not block/fail on audit write failures
3. **V2.0 RAR Generation**: Remediation Analysis Reports require complete audit timelines
4. **Fault Tolerance**: Network partitions, database outages, service restarts

**Business Requirements**:
- BR-AUDIT-001: Complete audit trail with no data loss
- BR-RAR-001 to BR-RAR-004: V2.0 RAR generation requires 100% audit coverage
- BR-PLATFORM-005: Service resilience during infrastructure failures

---

## ğŸ¯ **Decision**

Implement **Dead Letter Queue (DLQ) with Async Retry** using Redis Streams for audit write error recovery.

**Pattern**: Best-effort synchronous write â†’ Fallback to DLQ â†’ Async retry with exponential backoff

**Decision Rationale** (User-Approved Decision 2c):
- âœ… Ensures audit completeness (ADR-032 "No Audit Loss" mandate)
- âœ… Service availability (reconciliation doesn't block on audit writes)
- âœ… Fault tolerance (survives Data Storage Service outages)
- âœ… Observability (DLQ depth monitoring, retry metrics)
- âš ï¸ Eventual consistency (audit data may lag during outages)
- âš ï¸ Adds complexity (new component: DLQ client library)

---

## ğŸ—ï¸ **Architecture**

### **High-Level Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CRD Controller  â”‚
â”‚ (Any Service)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1. Attempt audit write
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Storage API    â”‚
â”‚ POST /api/v1/audit/*â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚     â”‚
   âœ…  â”‚     â”‚ âŒ (failure)
       â”‚     â”‚
       â”‚     â†“
       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  â”‚ DLQ Fallback     â”‚
       â”‚  â”‚ (Redis Streams)  â”‚
       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚           â”‚
       â”‚           â†“
       â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚     â”‚ Async Retry      â”‚
       â”‚     â”‚ Worker           â”‚
       â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚ Retry with backoff
       â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º SUCCESS
```

### **Components**

#### **1. DLQ Client Library** (`pkg/datastorage/dlq/`)

**Purpose**: Provide fallback persistence when Data Storage Service unavailable

```go
// pkg/datastorage/dlq/client.go
package dlq

import (
    "context"
    "encoding/json"
    "fmt"
    "time"

    "github.com/redis/go-redis/v9"
)

// Client provides Dead Letter Queue fallback for audit writes
type Client struct {
    redisClient *redis.Client
    streamName  string // "audit-dlq"
    maxLen      int64  // Max stream length (default: 10,000)
}

// NewClient creates a DLQ client
func NewClient(redisAddr string) (*Client, error) {
    client := redis.NewClient(&redis.Options{
        Addr:         redisAddr,
        MaxRetries:   3,
        DialTimeout:  5 * time.Second,
        ReadTimeout:  3 * time.Second,
        WriteTimeout: 3 * time.Second,
    })

    // Verify connection
    if err := client.Ping(context.Background()).Err(); err != nil {
        return nil, fmt.Errorf("Redis connection failed: %w", err)
    }

    return &Client{
        redisClient: client,
        streamName:  "audit-dlq",
        maxLen:      10000, // ~10K messages = ~10MB at 1KB/message
    }, nil
}

// WriteAuditMessage writes audit data to DLQ (fallback path)
// Generic method supporting all 6 audit types
func (c *Client) WriteAuditMessage(ctx context.Context, auditType string, auditData interface{}) error {
    // Serialize audit data
    dataJSON, err := json.Marshal(auditData)
    if err != nil {
        return fmt.Errorf("failed to marshal audit data: %w", err)
    }

    // Write to Redis Stream with TTL
    _, err = c.redisClient.XAdd(ctx, &redis.XAddArgs{
        Stream: c.streamName,
        MaxLen: c.maxLen, // Capped collection (FIFO eviction)
        ID:     "*",      // Auto-generate timestamp-based ID
        Values: map[string]interface{}{
            "audit_type": auditType, // orchestration, signal_processing, ai_analysis, etc.
            "audit_data": string(dataJSON),
            "created_at": time.Now().Unix(),
            "retry_count": 0,
        },
    }).Result()

    if err != nil {
        return fmt.Errorf("failed to write to DLQ: %w", err)
    }

    return nil
}

// GetPendingCount returns DLQ depth (for monitoring)
func (c *Client) GetPendingCount(ctx context.Context) (int64, error) {
    info, err := c.redisClient.XInfoStream(ctx, c.streamName).Result()
    if err != nil {
        return 0, err
    }
    return info.Length, nil
}
```

#### **2. Async Retry Worker**

**V1.0 Approach** (Approved Dec 2025): **Goroutine inside Data Storage Service**

**V1.1+ Approach** (Original DD-009): Standalone binary (`cmd/audit-retry-worker/`)

##### **V1.0: Integrated Goroutine** (`pkg/datastorage/server/dlq_retry_worker.go`)

**Rationale for V1.0**:
1. **Simplicity**: One less binary to deploy, configure, and monitor
2. **Consistency**: Same pattern as `BufferedAuditStore.backgroundWriter()` goroutine
3. **Infrastructure**: No additional Kubernetes deployment required
4. **DLQ events are rare**: Only triggers when primary write fails after 3 retries
5. **V1.1 graduation**: Can extract to standalone binary if DLQ volume becomes significant

**Trade-offs Accepted**:
- âš ï¸ Shares resources with Data Storage primary requests (mitigated by rate limiting)
- âš ï¸ Can't scale independently (acceptable for V1.0 volume)
- âš ï¸ Multiple DS replicas = multiple workers (handled by Redis consumer groups)

```go
// pkg/datastorage/server/dlq_retry_worker.go
package server

import (
    "context"
    "time"
    
    "github.com/go-logr/logr"
    "github.com/jordigilh/kubernaut/pkg/datastorage/dlq"
    "github.com/jordigilh/kubernaut/pkg/datastorage/repository"
)

// DLQRetryWorker processes DLQ messages and retries writes to PostgreSQL
// V1.0: Runs as goroutine inside Data Storage server (DD-007 lifecycle)
type DLQRetryWorker struct {
    dlqClient     *dlq.Client
    auditRepo     *repository.AuditEventsRepository
    logger        logr.Logger
    consumerGroup string
    consumerName  string
    
    // Configuration
    pollInterval      time.Duration // Default: 30 seconds
    maxBatchSize      int64         // Default: 10 messages per iteration
    maxRetriesPerMsg  int           // Default: 6 (matching backoff intervals)
    
    // Lifecycle
    stopCh       chan struct{}
    doneCh       chan struct{}
}

// DLQRetryWorkerConfig holds configuration for the DLQ retry worker
type DLQRetryWorkerConfig struct {
    PollInterval     time.Duration
    MaxBatchSize     int64
    MaxRetries       int
    ConsumerGroup    string
    ConsumerName     string
}

// DefaultDLQRetryWorkerConfig returns sensible defaults
func DefaultDLQRetryWorkerConfig() DLQRetryWorkerConfig {
    return DLQRetryWorkerConfig{
        PollInterval:  30 * time.Second,
        MaxBatchSize:  10,
        MaxRetries:    6,
        ConsumerGroup: "data-storage-retry-workers",
        ConsumerName:  "worker-default",
    }
}

// NewDLQRetryWorker creates a new DLQ retry worker
func NewDLQRetryWorker(
    dlqClient *dlq.Client,
    auditRepo *repository.AuditEventsRepository,
    config DLQRetryWorkerConfig,
    logger logr.Logger,
) *DLQRetryWorker {
    return &DLQRetryWorker{
        dlqClient:        dlqClient,
        auditRepo:        auditRepo,
        logger:           logger.WithName("dlq-retry-worker"),
        consumerGroup:    config.ConsumerGroup,
        consumerName:     config.ConsumerName,
        pollInterval:     config.PollInterval,
        maxBatchSize:     config.MaxBatchSize,
        maxRetriesPerMsg: config.MaxRetries,
        stopCh:           make(chan struct{}),
        doneCh:           make(chan struct{}),
    }
}

// Start begins the retry loop in a background goroutine
func (w *DLQRetryWorker) Start() {
    go w.retryLoop()
    w.logger.Info("DLQ retry worker started (DD-009 V1.0)",
        "poll_interval", w.pollInterval,
        "max_batch_size", w.maxBatchSize,
        "consumer_group", w.consumerGroup)
}

// Stop gracefully stops the retry worker (DD-007 integration)
func (w *DLQRetryWorker) Stop() {
    close(w.stopCh)
    <-w.doneCh
    w.logger.Info("DLQ retry worker stopped")
}

func (w *DLQRetryWorker) retryLoop() {
    defer close(w.doneCh)
    
    ticker := time.NewTicker(w.pollInterval)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            w.processRetryBatch()
        case <-w.stopCh:
            // Process any remaining messages before shutdown
            w.processRetryBatch()
            return
        }
    }
}

func (w *DLQRetryWorker) processRetryBatch() {
    ctx := context.Background()
    
    // Read batch from DLQ (non-blocking, immediate return)
    streams, err := w.dlqClient.ReadMessages(ctx, w.consumerGroup, w.consumerName, 0, w.maxBatchSize)
    if err != nil {
        w.logger.Error(err, "Failed to read from DLQ")
        return
    }
    
    for _, stream := range streams {
        for _, msg := range stream.Messages {
            w.processMessage(ctx, msg.ID, msg.Values)
        }
    }
}

func (w *DLQRetryWorker) processMessage(ctx context.Context, msgID string, values map[string]interface{}) {
    auditType := values["audit_type"].(string)
    auditDataJSON := values["audit_data"].(string)
    retryCountStr := values["retry_count"].(string)
    createdAtStr := values["created_at"].(string)
    
    retryCount := parseRetryCount(retryCountStr)
    createdAt := parseCreatedAt(createdAtStr)
    
    // Check backoff interval
    if !w.isReadyForRetry(retryCount, createdAt) {
        return // Skip, not ready yet
    }
    
    // Attempt write to PostgreSQL (direct, not via HTTP)
    if err := w.writeToPostgres(ctx, auditType, auditDataJSON); err != nil {
        w.handleRetryFailure(ctx, msgID, auditType, retryCount, err)
        return
    }
    
    // Success - acknowledge message
    if err := w.dlqClient.AckMessage(ctx, w.consumerGroup, msgID, auditType); err != nil {
        w.logger.Error(err, "Failed to ack DLQ message after successful write",
            "message_id", msgID, "audit_type", auditType)
    } else {
        w.logger.Info("DLQ message processed successfully",
            "message_id", msgID, "audit_type", auditType, "retry_count", retryCount)
    }
}

func (w *DLQRetryWorker) isReadyForRetry(retryCount int, createdAt time.Time) bool {
    // Exponential backoff: 1m, 5m, 15m, 1h, 4h, 24h
    backoffIntervals := []time.Duration{
        1 * time.Minute,
        5 * time.Minute,
        15 * time.Minute,
        1 * time.Hour,
        4 * time.Hour,
        24 * time.Hour,
    }
    
    if retryCount >= len(backoffIntervals) {
        return true // Max retries exceeded, process immediately for dead letter
    }
    
    nextRetry := createdAt.Add(backoffIntervals[retryCount])
    return time.Now().After(nextRetry)
}

func (w *DLQRetryWorker) writeToPostgres(ctx context.Context, auditType, auditDataJSON string) error {
    // Direct PostgreSQL write (bypass HTTP layer)
    return w.auditRepo.CreateFromJSON(ctx, auditType, []byte(auditDataJSON))
}

func (w *DLQRetryWorker) handleRetryFailure(ctx context.Context, msgID, auditType string, retryCount int, writeErr error) {
    w.logger.Error(writeErr, "DLQ retry failed",
        "message_id", msgID, "audit_type", auditType, "retry_count", retryCount)
    
    if retryCount >= w.maxRetriesPerMsg {
        // Move to dead letter stream (permanent failure)
        auditMsg := &dlq.AuditMessage{AuditType: auditType}
        if err := w.dlqClient.MoveToDeadLetter(ctx, msgID, auditType, auditMsg); err != nil {
            w.logger.Error(err, "CRITICAL: Failed to move to dead letter (audit data loss)",
                "message_id", msgID, "audit_type", auditType)
        } else {
            // Ack original message after moving to dead letter
            w.dlqClient.AckMessage(ctx, w.consumerGroup, msgID, auditType)
            w.logger.Info("Message moved to dead letter after max retries (ADR-032 violation)",
                "message_id", msgID, "audit_type", auditType, "retry_count", retryCount)
        }
    } else {
        // Increment retry count for next attempt
        if err := w.dlqClient.IncrementRetryCount(ctx, msgID, auditType, retryCount); err != nil {
            w.logger.Error(err, "Failed to increment retry count", "message_id", msgID)
        }
    }
}

// Helper functions (implemented elsewhere)
func parseRetryCount(s string) int { /* ... */ return 0 }
func parseCreatedAt(s string) time.Time { /* ... */ return time.Time{} }
```

##### **V1.1+: Standalone Binary** (`cmd/audit-retry-worker/main.go`) - DEFERRED

**When to graduate to standalone binary**:
1. DLQ depth consistently > 1,000 messages
2. Retry processing impacts primary request latency
3. Need to scale retry workers independently
4. Production workload justifies additional infrastructure

**Original standalone design preserved below for V1.1 reference**:

```go
// cmd/audit-retry-worker/main.go (V1.1+ - DEFERRED)
package main

import (
    "context"
    "encoding/json"
    "fmt"
    "log"
    "time"

    "github.com/redis/go-redis/v9"
    "github.com/jordigilh/kubernaut/pkg/datastorage/client"
    "github.com/jordigilh/kubernaut/pkg/datastorage/dlq"
)

type RetryWorker struct {
    dlqClient     *dlq.Client
    storageClient *client.Client
    consumerGroup string
    consumerName  string
}

func main() {
    worker := &RetryWorker{
        dlqClient:     dlq.NewClient("redis:6379"),
        storageClient: client.NewClient("http://data-storage:8080"),
        consumerGroup: "audit-retry-workers",
        consumerName:  "worker-1",
    }

    ctx := context.Background()
    if err := worker.Run(ctx); err != nil {
        log.Fatalf("Retry worker failed: %v", err)
    }
}

func (w *RetryWorker) Run(ctx context.Context) error {
    for {
        // Read from DLQ (blocking, 5 second timeout)
        messages, err := w.dlqClient.ReadMessages(ctx, w.consumerGroup, w.consumerName, 5*time.Second)
        if err != nil {
            log.Printf("Failed to read from DLQ: %v", err)
            time.Sleep(1 * time.Second)
            continue
        }

        for _, msg := range messages {
            if err := w.processMessage(ctx, msg); err != nil {
                log.Printf("Failed to process message %s: %v", msg.ID, err)
                w.handleRetryFailure(ctx, msg)
            } else {
                // Success - acknowledge message
                w.dlqClient.AckMessage(ctx, w.consumerGroup, msg.ID)
            }
        }
    }
}

func (w *RetryWorker) processMessage(ctx context.Context, msg *dlq.Message) error {
    auditType := msg.Values["audit_type"].(string)
    auditDataJSON := msg.Values["audit_data"].(string)
    retryCount := int(msg.Values["retry_count"].(int64))

    // Exponential backoff check
    if err := w.checkBackoff(retryCount, msg.CreatedAt); err != nil {
        return err // Not ready for retry yet
    }

    // Attempt write to Data Storage Service
    endpoint := fmt.Sprintf("/api/v1/audit/%s", auditType)
    if err := w.storageClient.Post(ctx, endpoint, []byte(auditDataJSON)); err != nil {
        // Retry failed - increment retry count
        msg.Values["retry_count"] = retryCount + 1
        return fmt.Errorf("Data Storage write failed (attempt %d): %w", retryCount+1, err)
    }

    return nil
}

func (w *RetryWorker) checkBackoff(retryCount int, createdAt time.Time) error {
    // Exponential backoff: 1m, 5m, 15m, 1h, 4h, 24h
    backoffIntervals := []time.Duration{
        1 * time.Minute,
        5 * time.Minute,
        15 * time.Minute,
        1 * time.Hour,
        4 * time.Hour,
        24 * time.Hour,
    }

    if retryCount >= len(backoffIntervals) {
        // Max retries exceeded - move to dead letter
        return fmt.Errorf("max retries (%d) exceeded", len(backoffIntervals))
    }

    nextRetry := createdAt.Add(backoffIntervals[retryCount])
    if time.Now().Before(nextRetry) {
        return fmt.Errorf("backoff period not elapsed, next retry at %s", nextRetry)
    }

    return nil
}

func (w *RetryWorker) handleRetryFailure(ctx context.Context, msg *dlq.Message) {
    retryCount := int(msg.Values["retry_count"].(int64))

    if retryCount >= 6 {
        // Permanent failure - move to dead letter stream for manual investigation
        w.dlqClient.MoveToDeadLetter(ctx, msg)
        w.dlqClient.AckMessage(ctx, w.consumerGroup, msg.ID)
        log.Printf("Message %s moved to dead letter after %d retries", msg.ID, retryCount)
    }
    // Otherwise, message stays in DLQ for next retry attempt
}
```

---

## ğŸ“Š **Failure Scenarios & Recovery**

### **Scenario 1: Transient Network Error**

**Failure**: Connection timeout to Data Storage Service (e.g., network glitch)

**Recovery**:
1. Controller: Immediate retry (3 attempts, 100ms backoff)
2. If still fails: Write to DLQ
3. Retry Worker: Attempts after 1 minute
4. **Expected Time to Recovery**: 1-2 minutes
5. **Outcome**: âœ… Audit data persisted, zero loss

### **Scenario 2: Data Storage Service Down**

**Failure**: Data Storage Service pod crashed or restarting

**Recovery**:
1. Controller: Write to DLQ immediately (no retries)
2. DLQ accumulates messages (up to 10,000)
3. Retry Worker: Attempts after 1m, 5m, 15m, ...
4. Data Storage recovers after 10 minutes
5. **Expected Time to Recovery**: 10-15 minutes
6. **Outcome**: âœ… All audit data persisted after service recovery

### **Scenario 3: Validation Failure**

**Failure**: Audit data invalid (e.g., missing required fields)

**Recovery**:
1. Data Storage returns HTTP 400 (validation error)
2. Controller: **NO DLQ WRITE** (validation errors are bugs, not transient)
3. Log error for debugging
4. **Expected Time to Recovery**: N/A (requires code fix)
5. **Outcome**: âš ï¸ Audit data lost (this is a service bug)

**Mitigation**: Pre-write validation in controller to catch bugs before submission

### **Scenario 4: Database Full**

**Failure**: PostgreSQL disk space exhausted

**Recovery**:
1. Data Storage returns HTTP 507 (Insufficient Storage)
2. Controller: Write to DLQ
3. DLQ accumulates (Redis has enough capacity)
4. Ops team: Expand PostgreSQL disk
5. Retry Worker: Successful writes after disk expansion
6. **Expected Time to Recovery**: 1-4 hours (operator intervention)
7. **Outcome**: âœ… No audit loss, eventual consistency

### **Scenario 5: DLQ Full**

**Failure**: Redis DLQ reaches 10,000 message limit

**Recovery**:
1. Oldest messages auto-evicted (FIFO)
2. Alert fires: "DLQ depth >5,000"
3. Ops team: Investigate Data Storage Service issue
4. **Expected Time to Recovery**: Immediate (new messages accepted)
5. **Outcome**: âš ï¸ Oldest audit data lost (if Data Storage unavailable for extended period)

**Mitigation**: Alert at 5,000 messages to trigger investigation before loss occurs

---

## ğŸ”§ **Implementation Details**

### **Redis Streams Configuration**

```yaml
# DLQ Stream: audit-dlq
Stream Name: audit-dlq
Max Length: 10,000 (capped collection, FIFO eviction)
TTL: 7 days (messages older than 7 days auto-expire)
Consumer Group: audit-retry-workers
Consumers: 1-3 (scale based on DLQ depth)
```

### **Retry Strategy**

| Attempt | Backoff Interval | Cumulative Time | Rationale |
|---------|------------------|-----------------|-----------|
| 1 | 1 minute | 1m | Quick recovery for transient issues |
| 2 | 5 minutes | 6m | Network partition recovery |
| 3 | 15 minutes | 21m | Pod restart recovery |
| 4 | 1 hour | 1h 21m | Rolling upgrade recovery |
| 5 | 4 hours | 5h 21m | Extended outage |
| 6 | 24 hours | 29h 21m | Manual intervention required |
| 7+ | Dead Letter | - | Permanent failure, manual investigation |

### **Monitoring & Alerts**

```yaml
# Prometheus Metrics
kubernaut_audit_dlq_depth{} > 5000
  Alert: AuditDLQHigh
  Severity: warning
  Action: Investigate Data Storage Service health

kubernaut_audit_dlq_depth{} > 9000
  Alert: AuditDLQCritical
  Severity: critical
  Action: Immediate response - audit loss imminent

kubernaut_audit_write_failures_total{} > 100 (rate: 1m)
  Alert: AuditWriteFailureSpike
  Severity: critical
  Action: Investigate Data Storage Service or database

kubernaut_audit_dlq_messages_in_dead_letter{} > 0
  Alert: AuditPermanentFailures
  Severity: critical
  Action: Manual investigation required (data loss occurred)
```

### **Metrics to Track**

```go
// Prometheus metrics (pkg/datastorage/audit/metrics.go)
auditWriteAttemptsTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "kubernaut_audit_write_attempts_total",
        Help: "Total audit write attempts",
    },
    []string{"audit_type", "result"}, // result: success, dlq_fallback, validation_error
)

auditDLQDepth = promauto.NewGauge(prometheus.GaugeOpts{
    Name: "kubernaut_audit_dlq_depth",
    Help: "Current number of messages in audit DLQ",
})

auditRetryDuration = promauto.NewHistogramVec(
    prometheus.HistogramOpts{
        Name:    "kubernaut_audit_retry_duration_seconds",
        Help:    "Time from DLQ write to successful retry",
        Buckets: []float64{60, 300, 900, 3600, 14400, 86400}, // 1m, 5m, 15m, 1h, 4h, 24h
    },
    []string{"audit_type"},
)

auditDeadLetterCount = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "kubernaut_audit_dead_letter_total",
        Help: "Total messages moved to dead letter (permanent failures)",
    },
    []string{"audit_type", "failure_reason"},
)
```

---

## ğŸ¯ **Consequences**

### **âœ… Benefits**

1. **Audit Completeness**: ADR-032 "No Audit Loss" mandate satisfied
2. **Service Resilience**: Controllers don't block/fail on audit write failures
3. **Fault Tolerance**: Survives Data Storage Service outages up to 7 days
4. **Observability**: DLQ depth and retry metrics provide early warning
5. **V2.0 RAR Confidence**: 100% audit coverage (with eventual consistency)

### **âš ï¸ Trade-offs**

1. **Eventual Consistency**: Audit data may lag seconds to hours during outages
2. **Infrastructure Dependency**: Requires Redis (already in stack for Context API cache)
3. **Operational Complexity**: New component to monitor (DLQ depth, retry worker health)
4. **Limited Capacity**: DLQ caps at 10,000 messages (~10MB); oldest messages evicted if exceeded

### **âŒ Rejected Alternatives**

#### **Alternative A: Synchronous Retry with Backoff**

**Pattern**: Retry in controller reconciliation loop

**Rejected Reason**:
- âŒ Blocks reconciliation (violates service availability requirement)
- âŒ Retry storms during prolonged outages
- âŒ No persistent queue (retries lost if controller restarts)

#### **Alternative B: Direct PostgreSQL Fallback**

**Pattern**: Controller writes directly to PostgreSQL if Data Storage Service fails

**Rejected Reason**:
- âŒ Violates ADR-032 ("Only Data Storage Service accesses PostgreSQL")
- âŒ Requires all controllers to have PostgreSQL credentials (security risk)
- âŒ Duplicates database schema knowledge across 6 services (maintenance burden)
- âŒ No benefit over DLQ pattern

#### **Alternative C: No Fallback (Best-Effort Only)**

**Pattern**: If audit write fails, log error and continue

**Rejected Reason**:
- âŒ Violates ADR-032 "No Audit Loss" mandate
- âŒ V2.0 RAR generation unreliable (missing audit data)
- âŒ Compliance risk (incomplete 7+ year audit trail)

---

## ğŸ“š **Integration Points**

### **Services Using DLQ Pattern** (6 CRD Controllers)

1. **RemediationOrchestrator** â†’ `POST /api/v1/audit/orchestration`
2. **RemediationProcessor** â†’ `POST /api/v1/audit/signal-processing`
3. **AIAnalysis Controller** â†’ `POST /api/v1/audit/ai-decisions`
4. **WorkflowExecution Controller** â†’ `POST /api/v1/audit/executions`
5. **Notification Controller** â†’ `POST /api/v1/audit/notifications`
6. **Effectiveness Monitor** â†’ `POST /api/v1/audit/effectiveness`

### **Shared DLQ Client Library**

All 6 services use the same `pkg/datastorage/dlq/` client library for consistency.

---

## ğŸš€ **Deployment**

### **Redis DLQ Setup** (already in infrastructure)

```yaml
# deploy/redis-dlq.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-dlq
  namespace: kubernaut-system
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        volumeMounts:
        - name: data
          mountPath: /data
        command:
        - redis-server
        - --maxmemory 512mb
        - --maxmemory-policy allkeys-lru
        - --save ""  # No persistence (ephemeral DLQ)
---
apiVersion: v1
kind: Service
metadata:
  name: redis-dlq
  namespace: kubernaut-system
spec:
  ports:
  - port: 6379
  selector:
    app: redis-dlq
```

### **Async Retry Worker Deployment**

```yaml
# deploy/audit-retry-worker.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: audit-retry-worker
  namespace: kubernaut-system
spec:
  replicas: 1  # Scale to 3 if DLQ depth > 5000
  template:
    spec:
      containers:
      - name: retry-worker
        image: kubernaut/audit-retry-worker:v1.0
        env:
        - name: REDIS_ADDR
          value: "redis-dlq:6379"
        - name: DATA_STORAGE_URL
          value: "http://data-storage:8080"
        - name: CONSUMER_GROUP
          value: "audit-retry-workers"
        - name: CONSUMER_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
```

---

## ğŸ“– **Usage Example**

### **Controller Integration**

```go
// internal/controller/aianalysis/reconciler.go
package controller

import (
    "github.com/jordigilh/kubernaut/pkg/datastorage/audit"
    "github.com/jordigilh/kubernaut/pkg/datastorage/dlq"
)

type AIAnalysisReconciler struct {
    client.Client
    auditClient *audit.Client
}

func (r *AIAnalysisReconciler) SetupWithManager(mgr ctrl.Manager) error {
    // Create DLQ client
    dlqClient, err := dlq.NewClient("redis-dlq:6379")
    if err != nil {
        return err
    }

    // Create audit client with DLQ fallback
    r.auditClient = audit.NewClient("http://data-storage:8080", dlqClient)

    return ctrl.NewControllerManagedBy(mgr).
        For(&v1alpha1.AIAnalysis{}).
        Complete(r)
}

func (r *AIAnalysisReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    // ... business logic ...

    // Write audit (non-blocking, DLQ fallback on failure)
    auditData := &audit.AIAnalysisAudit{
        ID:            string(analysis.UID),
        Fingerprint:   analysis.Spec.AlertFingerprint,
        Confidence:    analysis.Status.Confidence,
        // ... other fields ...
    }

    if err := r.auditClient.WriteAIAnalysisAudit(ctx, auditData); err != nil {
        // Log error but DO NOT FAIL reconciliation
        r.Log.Error(err, "Audit write failed (may be in DLQ)", "analysisID", analysis.UID)
    }

    return ctrl.Result{}, nil
}
```

---

## âœ… **Validation Criteria**

- [x] Decision documented (Dead Letter Queue with async retry)
- [x] Architecture diagram provided
- [x] 5 failure scenarios with recovery paths defined
- [x] Retry strategy with exponential backoff specified
- [x] Monitoring metrics and alerts defined
- [x] Redis Streams configuration documented
- [x] Deployment manifests provided
- [x] Integration example code provided
- [x] User approval received (Decision 2c)
- [ ] Implementation: Create `pkg/datastorage/dlq/` package (Phase 1-3)
- [ ] Implementation: Create `cmd/audit-retry-worker/` (Phase 1-3)
- [ ] Testing: DLQ client unit tests (Phase 1-3)
- [ ] Testing: End-to-end retry scenarios (Phase 1-3)

---

## ğŸ”— **Related Documentation**

- **Authority**: ADR-032 v1.1 - Data Access Layer Isolation
- **Database Schema**: `migrations/010_audit_write_api.sql`
- **Implementation Plan**: `docs/services/stateless/data-storage/implementation/IMPLEMENTATION_PLAN_V4.7.md`
- **Service Integration**: Phase 0 Day 0.3 service integration checklist (to be created)

---

## âœ… **Phase 0 Day 0.1 - Task 3 Complete**

**Deliverable**: âœ… DD-009 Error Recovery ADR documented
**Validation**: User-approved Decision 2c (Dead Letter Queue with async retry)
**Confidence**: 100%

---

**Document Version**: 1.0
**Status**: âœ… GAP #5 RESOLVED
**Last Updated**: 2025-11-02

