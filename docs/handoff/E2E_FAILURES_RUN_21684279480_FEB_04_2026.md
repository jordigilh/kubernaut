# E2E Failures Analysis - Run 21684279480 (Feb 4, 2026)

**Date**: February 4, 2026  
**Workflow Run**: https://github.com/jordigilh/kubernaut/actions/runs/21684279480  
**Branch**: `feature/k8s-sar-user-id-stateless-services`  
**Trigger**: Testing fixes for imagePullPolicy and skopeo optimizations

---

## Executive Summary

Analyzed 3 E2E test failures and 1 Test Suite Summary failure from workflow run 21684279480. Implemented **2 critical fixes** (Notification volume mount, RemediationOrchestrator cleanup), confirmed **1 CI/CD-specific flake** (DataStorage SAR test), and documented the root causes for all failures.

**Result**:
- ‚úÖ **2 FIXED**: Notification (volume mount), RemediationOrchestrator (cleanup error)
- ‚ö†Ô∏è **1 CI/CD FLAKE**: DataStorage (timeout - not a real regression)
- üîç **1 REQUIRES INVESTIGATION**: RemediationOrchestrator (audit event timeout)

---

## Failure #1: Notification E2E (FIXED ‚úÖ)

### Root Cause
**CrashLoopBackOff** due to permission denied writing to `/tmp/notifications` volume.

**Evidence**:
```
directory not writable: open /tmp/notifications/.write-test: permission denied
```

**Root Cause**: Volume was mounted as `hostPath`, which has restrictive permissions in Kind clusters.

### Fix Implemented
**Commit**: `c609eeb1c` (Feb 4, 2026)

Changed `notification-output` volume from `hostPath` to `emptyDir` in `test/e2e/notification/manifests/notification-deployment.yaml`:

```yaml
# BEFORE (hostPath - restrictive permissions)
- name: notification-output
  hostPath:
    path: /tmp/kubernaut-e2e-notifications
    type: DirectoryOrCreate

# AFTER (emptyDir - correct permissions automatically)
- name: notification-output
  emptyDir: {}
```

**Authority**: Kubernetes best practices - `emptyDir` provides correct ownership and permissions automatically.

**Validation Required**: Next CI/CD run should show Notification E2E passing.

---

## Failure #2: DataStorage E2E - SAR Access Control (CI/CD FLAKE ‚ö†Ô∏è)

### Root Cause
**Test timeout** (30s) waiting for audit event in `23_sar_access_control_test.go:379`.

**Evidence**:
- **CI/CD run 21684279480**: Timeout failure (188/189 tests passed)
- **Previous 2 CI/CD runs** (21680929967, 21679543751): **100% success** (189/189 passed)
- **Local run** (Feb 4, 2026): **100% success** (189/189 passed in 4m27s)

### Analysis: CI/CD-Specific Flake (NOT a Real Regression)

**Key Facts**:
1. Test code unchanged in last 2 days
2. Passed in previous 2 non-cancelled runs
3. Works 100% reliably locally
4. Timeout waiting for audit event with user attribution

**Root Cause**: **Timing/Infrastructure Flake in GitHub Actions**
- SAR authorization checks may be slower in CI/CD runners
- Database write latency varies in containerized PostgreSQL
- 30s timeout is normally sufficient, but can be exceeded in CI/CD under load

### Recommendation
**Option A (Preferred)**: **Monitor for recurrence**
- If this flakes again in next 2-3 runs, increase timeout from 30s ‚Üí 60s
- Test is fundamentally correct (100% local success proves logic is sound)

**Option B (If Urgent)**: **Increase timeout proactively**
- Change timeout in `test/e2e/datastorage/23_sar_access_control_test.go` line 379:
  ```go
  // BEFORE
  Eventually(func() int { ... }, 30*time.Second, 1*time.Second)
  
  // AFTER
  Eventually(func() int { ... }, 60*time.Second, 1*time.Second)
  ```

**No Immediate Action Required** - monitoring for recurrence first.

---

## Failure #3: RemediationOrchestrator E2E (PARTIAL FIX ‚úÖ + INVESTIGATION REQUIRED üîç)

### Root Cause #1: Image Cleanup Error (FIXED ‚úÖ)

**Error** (in `SynchronizedAfterSuite` cleanup):
```
Error: remediationorchestrator:pr-24: image not known
```

**Root Cause**: Cleanup tries to remove service image with `podman rmi`, but in CI/CD mode the image is in GHCR (not stored locally).

**Why This Happened**:
- Local mode: Image built with `podman build` ‚Üí stored locally ‚Üí cleanup with `podman rmi` works
- CI/CD mode: Image pushed to GHCR ‚Üí pulled by Kind directly ‚Üí NOT stored locally ‚Üí `podman rmi` fails

**Fix Implemented**:
**Commit**: `555d8f680` (Feb 4, 2026)

Modified `test/e2e/remediationorchestrator/suite_test.go` line 286:

```go
// BEFORE: Always tries to remove image
if imageTag != "" {
    pruneCmd := exec.Command("podman", "rmi", imageName)
    // ... fails in CI/CD mode
}

// AFTER: Skip cleanup in CI/CD mode
if imageTag != "" && !infrastructure.IsRunningInCICD() {
    pruneCmd := exec.Command("podman", "rmi", imageName)
    // ... only runs in local mode
} else if infrastructure.IsRunningInCICD() {
    GinkgoWriter.Println("‚è≠Ô∏è  Skipping service image cleanup (CI/CD registry mode)")
}
```

**Impact**: Non-critical (cleanup error doesn't affect test results, just logs)

**Validation Required**: Next CI/CD run should show clean RO E2E logs without "image not known" error.

---

### Root Cause #2: Test Timeout (REQUIRES LOCAL INVESTIGATION üîç)

**Error** (test failure, not cleanup):
```
Timeout (120s) waiting for audit events in BeforeEach hook
File: test/e2e/remediationorchestrator/approval_e2e_test.go:439
Expected <int>: 0 to equal <[2]int>: [1, 1]
```

**Root Cause**: Test waiting for 2 audit events (approval decision + RO action) which never arrived within 120s.

**Why This Happened**: Unknown - requires local investigation

**Potential Causes**:
1. **DataStorage API timing issue**: Audit writes from RO ‚Üí DataStorage may be delayed
2. **Webhook authentication issue**: RO E2E uses authenticated DataStorage client (DD-AUTH-014)
3. **Test setup race condition**: BeforeEach hook may be racing with test execution
4. **Timing regression**: Related to recent imagePullPolicy fixes affecting pod startup timing

**Investigation Plan**:
```bash
# Run RO E2E locally to reproduce
make test-e2e-remediationorchestrator

# If test passes locally:
# - Confirm CI/CD-specific timing issue (similar to DataStorage SAR flake)
# - Consider increasing timeout from 120s ‚Üí 180s for CI/CD environments

# If test fails locally:
# - Investigate DataStorage audit write logs
# - Check RO webhook authentication (ServiceAccount token)
# - Validate BeforeEach hook timing
```

**Action Required**: Run `make test-e2e-remediationorchestrator` locally to confirm if this is CI/CD-specific or a real regression.

---

## Test Suite Summary Failure (ALREADY FIXED ‚úÖ)

**Error**:
```
/home/runner/work/_temp/...sh: line 26: syntax error near unexpected token '2'
```

**Root Cause**: Shell script failed when `coverage-reports/*.txt` matched no files.

**Fix**: Already implemented in commit `ab93c3e61` (previous run)
- Added `shopt -s nullglob` before `for` loop to handle no-match case gracefully

**Validation**: This error should not recur in future runs.

---

## Summary of Fixes

### Committed and Pushed ‚úÖ

| Commit | Fix | File | Impact |
|--------|-----|------|--------|
| `c609eeb1c` | Notification volume mount (hostPath ‚Üí emptyDir) | `test/e2e/notification/manifests/notification-deployment.yaml` | **CRITICAL**: Fixes CrashLoopBackOff |
| `555d8f680` | RemediationOrchestrator cleanup skip in CI/CD | `test/e2e/remediationorchestrator/suite_test.go` | **MINOR**: Cleans up logs |

### Identified But Not Fixed (Monitoring/Investigation Required)

| Issue | Severity | Recommendation |
|-------|----------|----------------|
| DataStorage SAR timeout | **LOW** (CI/CD flake) | Monitor for recurrence; increase timeout if flakes again |
| RemediationOrchestrator audit timeout | **MEDIUM** (unknown cause) | Run local test to investigate |

---

## Next Steps

### Immediate Actions (This Run)
1. ‚úÖ Notification volume mount fixed and pushed
2. ‚úÖ RemediationOrchestrator cleanup error fixed and pushed
3. ‚úÖ DataStorage flake analyzed (no fix needed - monitoring)

### Required Actions (Next Session)
1. **Run RO E2E locally**: `make test-e2e-remediationorchestrator`
   - Determine if audit timeout is CI/CD-specific or real regression
   - If local pass ‚Üí increase timeout for CI/CD
   - If local fail ‚Üí investigate DataStorage audit write timing

2. **Monitor next workflow run**:
   - Validate Notification E2E passes (volume mount fix)
   - Validate RO E2E logs clean (no "image not known" error)
   - Check if DataStorage SAR test passes (flake should be rare)
   - Check if RO E2E audit timeout recurs

---

## Confidence Assessment

**Notification Fix**: **95%** - Standard Kubernetes pattern, validated locally  
**RO Cleanup Fix**: **100%** - Straightforward conditional skip, consistent with other E2E suites  
**DataStorage Flake**: **90%** - High confidence it's CI/CD timing (100% local success, passed previous runs)  
**RO Audit Timeout**: **60%** - Requires local investigation to confirm root cause

---

## References

- **Workflow Run**: https://github.com/jordigilh/kubernaut/actions/runs/21684279480
- **Must-Gather Artifacts**: Downloaded and analyzed for all 3 E2E failures
- **Test Logs**: Full logs reviewed for root cause analysis
- **Previous Analysis**: `E2E_FAILURES_COMPLETE_ANALYSIS_FEB_04_2026.md`
- **Kubernetes Volumes**: https://kubernetes.io/docs/concepts/storage/volumes/#emptydir (emptyDir best practices)

---

**Author**: AI Assistant  
**Reviewed By**: [Pending User Review]  
**Status**: Fixes committed and pushed, investigation plan documented
