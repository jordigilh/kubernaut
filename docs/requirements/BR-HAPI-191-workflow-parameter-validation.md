# BR-HAPI-191: Workflow Parameter Validation in Chat Session

**Business Requirement ID**: BR-HAPI-191
**Category**: HolmesGPT API Service
**Priority**: P1
**Target Version**: V1
**Status**: âœ… Approved
**Date**: December 1, 2025

---

## ğŸ“‹ **Business Need**

### **Problem Statement**

When HolmesGPT-API selects a workflow and suggests parameters, those parameters must be validated against the workflow's parameter schema before execution. Currently, validation happens **after** the LLM chat session ends, which creates significant problems.

**Current Limitations**:
- âŒ Parameter validation happens in downstream services (after LLM session)
- âŒ If validation fails, the LLM context is lost
- âŒ Must restart Root Cause Analysis from scratch
- âŒ LLM non-determinism means repeated attempts may produce different results
- âŒ Wasted compute/tokens on failed remediations

**Impact**:
- Users wait for entire execution to discover parameter errors
- RCA must be repeated on validation failures (expensive)
- Lower remediation success rate due to parameter mismatches
- Poor user experience when workflows fail on invalid parameters

---

## ğŸ¯ **Business Objective**

**Enable HolmesGPT-API to validate workflow parameters INSIDE the LLM chat session, allowing the LLM to self-correct before the session ends.**

### **Success Criteria**

1. âœ… New tool `validate_workflow_parameters` available to LLM during investigation
2. âœ… Tool fetches workflow parameter schema from Data Storage
3. âœ… Validates: required fields, types, enums, numeric ranges
4. âœ… Returns actionable error messages for LLM self-correction
5. âœ… LLM can retry validation up to 3 times with corrected parameters
6. âœ… If LLM cannot self-correct, flags for human review (not silent failure)
7. âœ… >95% of parameter validation errors caught before execution

---

## ğŸ“Š **Use Cases**

### **Use Case 1: LLM Self-Correction on Invalid Parameters**

**Scenario**: LLM selects OOMKill recovery workflow but suggests invalid MEMORY_LIMIT format.

**Current Flow** (Without BR-HAPI-191):
```
1. LLM performs RCA â†’ identifies OOMKilled
2. LLM selects workflow: oomkill-increase-memory-v1
3. LLM suggests: {"MEMORY_LIMIT": "2 gigabytes"}  // Invalid format
4. Chat session ends
5. AIAnalysis creates WorkflowExecution CRD
6. Workflow Engine validates parameters
7. âŒ Validation fails: "MEMORY_LIMIT must match K8s format (e.g., '2Gi')"
8. âŒ Entire RCA must be repeated
9. âŒ May get different workflow selection on retry
```

**Desired Flow with BR-HAPI-191**:
```
1. LLM performs RCA â†’ identifies OOMKilled
2. LLM selects workflow: oomkill-increase-memory-v1
3. LLM suggests: {"MEMORY_LIMIT": "2 gigabytes"}
4. LLM calls validate_workflow_parameters(workflow_id, params)
5. Tool returns: {
     "status": "invalid",
     "errors": ["MEMORY_LIMIT must match K8s format (e.g., '512Mi', '2Gi')"]
   }
6. LLM self-corrects: {"MEMORY_LIMIT": "2Gi"}
7. LLM calls validate_workflow_parameters again
8. Tool returns: {"status": "valid"}
9. âœ… Chat session returns validated parameters
10. âœ… Workflow executes successfully
```

### **Use Case 2: Missing Required Parameter**

**Scenario**: LLM forgets to include NAMESPACE parameter.

```
1. LLM selects workflow requiring: NAMESPACE (required), TARGET_NAME (required)
2. LLM suggests: {"TARGET_NAME": "api-server"}  // Missing NAMESPACE
3. LLM calls validate_workflow_parameters
4. Tool returns: {"errors": ["Missing required parameter: NAMESPACE"]}
5. LLM corrects: {"NAMESPACE": "production", "TARGET_NAME": "api-server"}
6. Validation passes
7. âœ… Workflow executes with correct parameters
```

### **Use Case 3: Enum Value Mismatch**

**Scenario**: Workflow requires RESTART_POLICY âˆˆ {Always, OnFailure, Never}.

```
1. LLM suggests: {"RESTART_POLICY": "always"}  // Wrong case
2. Validation fails: "RESTART_POLICY must be one of [Always, OnFailure, Never]"
3. LLM corrects: {"RESTART_POLICY": "Always"}
4. âœ… Validation passes
```

### **Use Case 4: LLM Hallucinated Credentials Stripped (v1.3, Issue #241)**

**Scenario**: Workflow schema declares only `TARGET_NAMESPACE` and `TARGET_RESOURCE_NAME`, but the LLM also provides `GIT_PASSWORD` and `GIT_USERNAME`.

```
1. LLM selects workflow: fix-certificate-gitops-v1
2. LLM suggests: {
     "TARGET_NAMESPACE": "demo-cert-gitops",
     "TARGET_RESOURCE_NAME": "demo-app-cert",
     "GIT_PASSWORD": "kubernaut-token",    // Hallucinated - NOT in schema
     "GIT_USERNAME": "kubernaut"            // Hallucinated - NOT in schema
   }
3. HAPI validates declared params (TARGET_NAMESPACE, TARGET_RESOURCE_NAME) âœ…
4. HAPI strips undeclared params: GIT_PASSWORD, GIT_USERNAME removed
5. âš ï¸ Warning logged: "Stripped 2 undeclared parameters: GIT_PASSWORD, GIT_USERNAME"
6. âœ… Validation passes with params: {"TARGET_NAMESPACE": "demo-cert-gitops", "TARGET_RESOURCE_NAME": "demo-app-cert"}
7. âœ… Credentials come from DD-WE-006 dependency mounts, not LLM params
```

### **Use Case 5: Workflow Without Parameter Schema (v1.3)**

**Scenario**: Workflow has no `parameters` section in its schema.

```
1. LLM selects a workflow with no parameter schema
2. LLM suggests: {"SOME_PARAM": "value"}
3. HAPI finds no schema â€” nothing is declared
4. HAPI strips ALL params (nothing declared = nothing allowed)
5. âœ… Validation passes with params: {}
```

---

## ğŸ”§ **Technical Requirements**

### **TR-1: validate_workflow_parameters Tool**

```python
class ValidateWorkflowParametersTool(Tool):
    name = "validate_workflow_parameters"

    parameters = {
        "workflow_id": ToolParameter(type="string", required=True),
        "parameters": ToolParameter(type="object", required=True)
    }

    def invoke(self, workflow_id: str, parameters: dict) -> ToolResult:
        # Fetch schema from Data Storage
        # Validate parameters
        # Return errors or success
```

### **TR-2: Data Storage Schema Endpoint**

HolmesGPT-API must be able to fetch workflow parameter schema:

```
GET /api/v1/workflows/{workflow_id}/schema
â†’ Returns parameter definitions (name, type, required, enum, min, max)
```

### **TR-3: LLM Prompt Update**

Add instruction to LLM system prompt:

```
After selecting a workflow, you MUST validate parameters using
validate_workflow_parameters before returning your recommendation.
If validation fails, correct the parameters and retry (max 3 attempts).
```

### **TR-4: Retry Limit**

- Maximum 3 validation attempts
- After 3 failures, return `needs_human_review: true`

---

## ğŸ“ˆ **Metrics & KPIs**

| Metric | Target | Current |
|--------|--------|---------|
| Parameter validation errors caught before execution | >95% | 0% |
| RCA restarts due to parameter errors | <5% | Unknown |
| Average validation retries per request | <1.5 | N/A |
| Human review escalations due to validation | <2% | N/A |

---

## ğŸ”— **Dependencies**

| Dependency | Service | Status |
|------------|---------|--------|
| Workflow schema endpoint | Data Storage | ğŸŸ¡ Needs implementation |
| `validate_workflow_parameters` tool | HolmesGPT-API | ğŸŸ¡ Needs implementation |
| LLM prompt update | HolmesGPT-API | ğŸŸ¡ Needs implementation |

---

## ğŸ“ **Design Decision**

**Reference**: [DD-HAPI-002: Workflow Parameter Validation Architecture](../../architecture/decisions/DD-HAPI-002-workflow-parameter-validation.md)

---

## ğŸ”„ **Related Requirements**

| BR ID | Description | Relationship |
|-------|-------------|--------------|
| BR-WE-001 | Defense-in-Depth Parameter Validation | WE validates as safety net |
| BR-HAPI-046-050 | Data Storage Workflow Tool | Prerequisite - selects workflow |
| BR-WORKFLOW-001 | Workflow Registry Management | Provides workflow definitions |

---

## âœ… **Acceptance Criteria**

```gherkin
Feature: Workflow Parameter Validation in Chat Session

  Scenario: Valid parameters pass on first attempt
    Given LLM has selected workflow "oomkill-recovery-v1"
    And LLM provides parameters {"MEMORY_LIMIT": "2Gi", "NAMESPACE": "production"}
    When LLM calls validate_workflow_parameters
    Then validation returns status "valid"
    And no retries are needed

  Scenario: Invalid parameters trigger self-correction
    Given LLM has selected workflow "oomkill-recovery-v1"
    And LLM provides parameters {"MEMORY_LIMIT": "2 gigabytes"}
    When LLM calls validate_workflow_parameters
    Then validation returns status "invalid" with errors
    And LLM corrects parameters to {"MEMORY_LIMIT": "2Gi"}
    And second validation returns status "valid"

  Scenario: Missing required parameter detected
    Given workflow requires parameter "NAMESPACE" as required
    And LLM omits "NAMESPACE" from parameters
    When LLM calls validate_workflow_parameters
    Then validation returns error "Missing required parameter: NAMESPACE"

  Scenario: Max retries exceeded escalates to human
    Given LLM fails validation 3 times
    When LLM attempts fourth validation
    Then response includes "needs_human_review": true

  Scenario: Undeclared parameters stripped silently (v1.3, Issue #241)
    Given workflow schema declares parameters ["TARGET_NAMESPACE"]
    And LLM provides parameters {"TARGET_NAMESPACE": "prod", "GIT_PASSWORD": "secret"}
    When HAPI validates the workflow response
    Then "GIT_PASSWORD" is removed from the parameters dict
    And parameters dict contains only {"TARGET_NAMESPACE": "prod"}
    And validation returns status "valid"
    And a warning is logged for stripped parameter "GIT_PASSWORD"

  Scenario: No-schema workflow has all parameters stripped (v1.3)
    Given workflow has no parameters schema
    And LLM provides parameters {"ANY_PARAM": "value"}
    When HAPI validates the workflow response
    Then parameters dict is empty
    And validation returns status "valid"
```

---

## ğŸ“… **Timeline**

| Phase | Duration | Deliverable |
|-------|----------|-------------|
| Design | Complete | DD-HAPI-002 |
| Data Storage schema endpoint | 1 day | GET /workflows/{id}/schema |
| HolmesGPT-API tool implementation | 2 days | validate_workflow_parameters |
| LLM prompt update | 0.5 day | System prompt modification |
| Testing | 1 day | Unit + E2E tests |
| **Total** | **~5 days** | |

---

## Changelog

| Version | Date | Changes |
|---------|------|---------|
| 1.1 | 2026-03-02 | Added Use Cases 4-5 and acceptance criteria for undeclared parameter stripping (Issue #241, DD-HAPI-002 v1.3) |
| 1.0 | 2025-12-01 | Initial requirement |

