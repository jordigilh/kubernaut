# AIAnalysis Integration Tests - 3 Failures Triage

**Date**: February 3, 2026  
**Test Run**: AIAnalysis Integration Tests (after mock policy refactoring)  
**Result**: 56/59 specs ran ‚Üí 53 passed, 3 failed, 3 pending  
**Status**: ‚úÖ **Mock policy refactoring successful** - Failures are pre-existing HAPI bugs

---

## üéØ Executive Summary

**Good News**: The mock policy refactoring (migrating from in-memory mocks to real HAPI) was **successful**. 53 of 56 tests passed using real HAPI integration.

**Bad News**: 3 pre-existing failures identified, **ALL related to HAPI bugs**:

1. **HAPI Bug #1**: `needs_human_review` not set despite low confidence (< 0.7)
2. **HAPI Bug #2**: `needs_human_review` not set for terminal failures
3. **HAPI Bug #3**: `alternative_workflows` field missing from responses

**Classification**: These are HAPI service bugs, NOT AIAnalysis controller bugs, NOT mock policy issues.

---

## üö® **Failure #1: Recovery Human Review - Low Confidence**

### Test Details
- **Test**: `recovery_human_review_integration_test.go:246`
- **Scenario**: BR-HAPI-197 - Recovery human review when confidence is low
- **Signal Type**: `MOCK_LOW_CONFIDENCE`
- **Expected**: AIAnalysis transitions to `Failed` phase with `LowConfidence` subreason
- **Actual**: AIAnalysis transitioned to `Completed` phase

### Evidence from Logs

**Test Log (line 26429)**:
```
INFO investigating-handler.response-processor üîç DEBUG: Recovery response received from HAPI
  {"NeedsHumanReview.Set": true, 
   "NeedsHumanReview.Value": false,  ‚Üê BUG! Should be TRUE
   "HumanReviewReason.Set": false, 
   "HumanReviewReason.Value": "", 
   "needsHumanReview_computed": false}

INFO investigating-handler.response-processor Processing successful recovery response
  {"canRecover": true, 
   "confidence": 0.35,  ‚Üê LOW CONFIDENCE!
   "hasSelectedWorkflow": true, 
   "needsHumanReview": false}  ‚Üê BUG!
```

**HAPI Log (aianalysis_aianalysis_hapi_test.log line 17860)**:
```
Based on my investigation of the MOCK_LOW_CONFIDENCE signal:

# root_cause_analysis
{"summary": "Multiple possible root causes identified, requires human judgment", ...}

# confidence
0.35  ‚Üê BELOW 0.7 THRESHOLD!

# selected_workflow
{"workflow_id": "e5b91650-ee07-459f-ab82-af1bc295505a", "confidence": 0.35, ...}

# alternative_workflows
[{"workflow_id": "d3c95ea1-66cb-6bf2-c59e-7dd27f1fec6d", ...}, ...]
```

### Root Cause Analysis

**HAPI Bug**: HAPI's response processing does NOT set `needs_human_review=true` when confidence < 0.7.

**Expected Behavior (BR-HAPI-197)**:
- Confidence < 0.7 (threshold) ‚Üí `needs_human_review: true`
- `human_review_reason: "low_confidence"`
- AIAnalysis controller should transition to `Failed` phase with `LowConfidence` subreason

**Actual Behavior**:
- HAPI returned confidence 0.35 (well below 0.7)
- But `NeedsHumanReview.Value = false` (BUG!)
- AIAnalysis controller processed it as successful
- Transitioned to `Analyzing` ‚Üí `Completed` (wrong!)

**Location of Bug**: Likely in HAPI's `src/extensions/incident/llm_integration.py` or `src/extensions/recovery/llm_integration.py` where the response is parsed and the `needs_human_review` flag is computed.

**Expected Fix in HAPI**:
```python
# After parsing LLM response
if response.confidence < 0.7:  # BR-HAPI-197 threshold
    response.needs_human_review = True
    response.human_review_reason = "low_confidence"
```

---

## üö® **Failure #2: Terminal Failure Auditing**

### Test Details
- **Test**: `error_handling_integration_test.go:149`
- **Scenario**: BR-AI-050 - Should record `analysis.failed` audit event when investigation fails permanently
- **Signal Type**: `MOCK_NO_WORKFLOW_FOUND`
- **Expected**: AIAnalysis reaches `Failed` phase with `WorkflowResolutionFailed/NoMatchingWorkflows`
- **Actual**: AIAnalysis did NOT reach `Failed` phase (timed out after 90s)

### Evidence from Logs

**Test Log (lines 28122-28128)**:
```
[FAILED] Timed out after 90.001s.
AIAnalysis should reach Failed phase with WorkflowResolutionFailed/NoMatchingWorkflows when no workflow found
Expected
    <bool>: false
to be true
```

**Controller Log (test output)**:
```
INFO analyzing-handler.analyzing-handler Processing Analyzing phase
ERROR analyzing-handler.analyzing-handler No workflow selected - investigation may have failed
```

### Root Cause Analysis

**HAPI Bug**: When Mock LLM returns `workflow_id=""` (no workflow), HAPI should set:
- `selected_workflow: null`
- `needs_human_review: true`
- `human_review_reason: "no_matching_workflows"`

But HAPI is NOT setting `needs_human_review=true`, so AIAnalysis controller doesn't know to transition to `Failed`.

**Expected Behavior**:
1. Mock LLM returns `workflow_id=""` (no workflow found)
2. HAPI sets `needs_human_review=true` + `human_review_reason="no_matching_workflows"`
3. AIAnalysis controller receives response, sees `needs_human_review=true`
4. Transitions to `Failed` phase with `WorkflowResolutionFailed/NoMatchingWorkflows`

**Actual Behavior**:
1. Mock LLM returns `workflow_id=""` ‚úÖ (correct)
2. HAPI does NOT set `needs_human_review=true` ‚ùå (BUG!)
3. AIAnalysis controller processes it as success (but no workflow)
4. Transitions to `Analyzing` but then gets stuck (no workflow to execute)

**Location of Bug**: Same as Failure #1 - HAPI's response processor doesn't compute `needs_human_review` correctly.

---

## üö® **Failure #3: Alternative Workflows Missing from Audit**

### Test Details
- **Test**: `audit_provider_data_integration_test.go:455`
- **Scenario**: BR-AUDIT-005 Gap #4 - Should capture complete IncidentResponse in HAPI event for RR reconstruction
- **Signal Type**: `CrashLoopBackOff`
- **Expected**: HAPI audit event includes `alternative_workflows` field
- **Actual**: `alternative_workflows` is `nil` (empty array)

### Evidence from Logs

**Test Failure (line 24463)**:
```
[FAILED] Required: alternative_workflows
Expected
    <[]api.IncidentResponseDataAlternativeWorkflowsItem | len:0, cap:0>: nil
not to be nil
```

**Test Log (line 24455)**:
```
‚úÖ Selected workflow present: workflow_id={fab03c1c-44d0-4b95-8835-b70e9863cf03 true}
[FAILED] Required: alternative_workflows  ‚Üê Next assertion failed
```

### Root Cause Analysis

**HAPI Bug**: HAPI is NOT including `alternative_workflows` field in audit events, even though:
1. Mock LLM DOES return `alternative_workflows` for `low_confidence` scenario (server.py lines 934-948)
2. The field is REQUIRED for RemediationRequest reconstruction (BR-AUDIT-005)

**Expected Behavior**:
- HAPI parses LLM response
- Extracts `alternative_workflows` from LLM output
- Includes it in `IncidentResponse` struct
- Writes it to audit event (`holmesgpt.response.complete`)

**Actual Behavior**:
- HAPI parses LLM response ‚úÖ
- **Does NOT extract `alternative_workflows`** ‚ùå (BUG!)
- Audit event has `alternative_workflows: nil`

**Location of Bug**: 
- `holmesgpt-api/src/extensions/incident/llm_integration.py` or
- `holmesgpt-api/src/models/response_models.py`
- LLM response parsing likely doesn't extract `alternative_workflows` field

**Expected Fix in HAPI**:
```python
# In LLM response parser
response_data = parse_llm_response(llm_output)
incident_response = IncidentResponse(
    analysis=response_data.get("root_cause_analysis"),
    confidence=response_data.get("confidence"),
    selected_workflow=response_data.get("selected_workflow"),
    alternative_workflows=response_data.get("alternative_workflows", []),  ‚Üê ADD THIS
    warnings=response_data.get("warnings", []),
    needs_human_review=response_data.get("needs_human_review", False),
    # ... other fields
)
```

---

## üìã **Root Cause Summary**

| Failure | Component | Bug Type | Impact |
|---------|-----------|----------|---------|
| #1: Low confidence recovery | HAPI | Missing `needs_human_review` logic | AIAnalysis completes instead of failing |
| #2: No workflow terminal failure | HAPI | Missing `needs_human_review` logic | AIAnalysis doesn't transition to Failed |
| #3: Alternative workflows | HAPI | Missing field extraction | Audit incomplete, RR reconstruction broken |

**Common Theme**: HAPI's response processor has multiple issues:
1. Not computing `needs_human_review` based on confidence threshold
2. Not computing `needs_human_review` when `workflow_id=""` (no workflow)
3. Not extracting `alternative_workflows` from LLM response

---

## üîß **HAPI Fix Requirements**

### **Priority 1: needs_human_review Logic (Failures #1, #2)**

**File**: `holmesgpt-api/src/extensions/incident/llm_integration.py` (and recovery equivalent)

**Current Code (MISSING)**:
```python
# HAPI currently does NOT compute needs_human_review
```

**Required Fix**:
```python
def compute_needs_human_review(response_data: dict, confidence: float) -> tuple[bool, str]:
    """
    Compute needs_human_review flag and reason per BR-HAPI-197.
    
    Returns: (needs_human_review, human_review_reason)
    """
    # BR-HAPI-197: Low confidence threshold
    if confidence < 0.7:
        return (True, "low_confidence")
    
    # No workflow selected
    if not response_data.get("selected_workflow"):
        workflow_id = response_data.get("selected_workflow", {}).get("workflow_id", "")
        if not workflow_id:
            return (True, "no_matching_workflows")
    
    # Check if LLM explicitly flagged for human review
    if response_data.get("needs_human_review"):
        reason = response_data.get("human_review_reason", "investigation_inconclusive")
        return (True, reason)
    
    return (False, "")

# In response builder
needs_human_review, human_review_reason = compute_needs_human_review(response_data, confidence)
incident_response = IncidentResponse(
    # ... other fields
    needs_human_review=needs_human_review,
    human_review_reason=human_review_reason if needs_human_review else None,
)
```

---

### **Priority 2: alternative_workflows Field Extraction (Failure #3)**

**File**: `holmesgpt-api/src/extensions/incident/llm_integration.py`

**Current Code (MISSING)**:
```python
# HAPI currently does NOT extract alternative_workflows from LLM response
incident_response = IncidentResponse(
    analysis=response_data.get("root_cause_analysis"),
    confidence=response_data.get("confidence"),
    selected_workflow=response_data.get("selected_workflow"),
    # alternative_workflows=???  ‚Üê MISSING!
)
```

**Required Fix**:
```python
# Extract alternative_workflows from LLM response
alternative_workflows = response_data.get("alternative_workflows", [])

incident_response = IncidentResponse(
    analysis=response_data.get("root_cause_analysis"),
    confidence=response_data.get("confidence"),
    selected_workflow=response_data.get("selected_workflow"),
    alternative_workflows=alternative_workflows,  ‚Üê ADD THIS
    warnings=response_data.get("warnings", []),
)
```

**Pydantic Model** (`holmesgpt-api/src/models/response_models.py`):
```python
class IncidentResponse(BaseModel):
    """Response from incident analysis."""
    analysis: RootCauseAnalysis
    confidence: float
    selected_workflow: Optional[WorkflowSelection]
    alternative_workflows: List[AlternativeWorkflow] = Field(default_factory=list)  ‚Üê ADD THIS
    warnings: List[str] = Field(default_factory=list)
    needs_human_review: bool = False
    human_review_reason: Optional[str] = None
```

---

## üìä **Must-Gather Logs Analysis**

**Location**: `/tmp/kubernaut-must-gather/aianalysis-integration-20260203-175738/`

### **Key Findings from HAPI Logs**

**File**: `aianalysis_aianalysis_hapi_test.log` (7.0M)

**Line 17860-17874** - Mock LLM correctly returns `alternative_workflows`:
```
# confidence
0.35

# selected_workflow
{"workflow_id": "e5b91650-ee07-459f-ab82-af1bc295505a", ...}

# alternative_workflows
[{"workflow_id": "d3c95ea1-66cb-6bf2-c59e-7dd27f1fec6d", "title": "Alternative Diagnostic Workflow", "confidence": 0.28, ...}, 
 {"workflow_id": "e4d06fb2-77dc-7cg3-d60f-8ee38g2gfd7e", "title": "Manual Investigation Required", "confidence": 0.22, ...}]
```

**Proof**: Mock LLM IS returning the data correctly. HAPI is NOT processing it.

---

## üîç **Detailed Failure Analysis**

### **Failure #1: Low Confidence Not Triggering Human Review**

**File**: `test/integration/aianalysis/recovery_human_review_integration_test.go:246`

**Test Flow**:
```
1. Create AIAnalysis with signal_type="MOCK_LOW_CONFIDENCE"
2. AA controller calls HAPI /recovery/analyze
3. HAPI calls Mock LLM ‚Üí returns confidence=0.35
4. HAPI returns response to AA controller
5. AA controller processes response
6. ‚ùå EXPECTED: Transition to Failed (low confidence < 0.7)
7. ‚ùå ACTUAL: Transition to Completed (treated as success)
```

**Failure Message**:
```
[FAILED] Timed out after 60.001s.
Expected
    <string>: Completed  ‚Üê ACTUAL (wrong)
to equal
    <string>: Failed     ‚Üê EXPECTED
```

**HAPI Response Bug (line 26429)**:
```go
NeedsHumanReview.Set: true
NeedsHumanReview.Value: false  ‚Üê SHOULD BE TRUE (confidence 0.35 < 0.7)
```

**Why This Matters (BR-HAPI-197)**:
- Low confidence workflows (<0.7) should NOT execute automatically
- Human operators need to review and approve before remediation
- Executing low-confidence workflows risks making problems worse

---

### **Failure #2: No Workflow Not Triggering Terminal Failure**

**File**: `test/integration/aianalysis/error_handling_integration_test.go:149`

**Test Flow**:
```
1. Create AIAnalysis with signal_type="MOCK_NO_WORKFLOW_FOUND"
2. Mock LLM returns workflow_id="" (empty - no workflow)
3. HAPI should set needs_human_review=true + reason="no_matching_workflows"
4. AA controller should transition to Failed phase
5. ‚ùå ACTUAL: AA controller stuck, never reaches Failed
```

**Failure Message**:
```
[FAILED] Timed out after 90.001s.
AIAnalysis should reach Failed phase with WorkflowResolutionFailed/NoMatchingWorkflows
Expected
    <bool>: false  ‚Üê AIAnalysis.Status.Phase != Failed
to be true
```

**Controller Log**:
```
INFO aianalysis-controller Processing Analyzing phase
ERROR analyzing-handler No workflow selected - investigation may have failed
```

**Why This Matters (BR-AI-050)**:
- When no workflow found, AIAnalysis MUST transition to `Failed` with clear reason
- Audit trail must capture `analysis.failed` event for observability
- Without proper failure state, RemediationOrchestrator can't handle fallback/escalation

---

### **Failure #3: Alternative Workflows Missing from Audit**

**File**: `test/integration/aianalysis/audit_provider_data_integration_test.go:455`

**Test Flow**:
```
1. Create AIAnalysis with signal_type="CrashLoopBackOff"
2. Mock LLM returns response with alternative_workflows array
3. HAPI receives response with alternative_workflows
4. HAPI writes audit event (holmesgpt.response.complete)
5. Test queries audit event from DataStorage
6. ‚ùå EXPECTED: alternative_workflows field populated
7. ‚ùå ACTUAL: alternative_workflows is nil (empty array)
```

**Failure Message**:
```
[FAILED] Required: alternative_workflows
Expected
    <[]api.IncidentResponseDataAlternativeWorkflowsItem | len:0, cap:0>: nil
not to be nil
```

**Test Log**:
```
‚úÖ Selected workflow present: workflow_id={fab03c1c-44d0-4b95-8835-b70e9863cf03 true}
[FAILED] Required: alternative_workflows  ‚Üê Next assertion
```

**Why This Matters (BR-AUDIT-005 Gap #4)**:
- SOC2 Type II compliance requires complete audit trail
- RemediationRequest reconstruction needs ALL fields from original analysis
- Alternative workflows show what options were considered (decision audit trail)
- Without this data, cannot reconstruct decision-making process for compliance audits

---

## ‚úÖ **Mock Policy Refactoring Status**

**SUCCESS METRICS**:
- ‚úÖ 53/56 tests passing (95% success rate)
- ‚úÖ Real HAPI HTTP integration working
- ‚úÖ Authentication (ServiceAccount) working
- ‚úÖ Mock LLM scenarios working correctly
- ‚úÖ Workflow UUID bootstrapping working

**FAILURES ARE NOT RELATED TO REFACTORING**:
- ‚ùå All 3 failures are HAPI service bugs
- ‚úÖ Mock LLM is working correctly (returns proper data)
- ‚úÖ AIAnalysis controller is working correctly (processes HAPI responses)
- ‚ùå HAPI is NOT processing responses correctly (missing logic)

---

## üéØ **Next Steps**

### **For HAPI Team** (Priority: P0 - Blocking AIAnalysis)

1. **Fix `needs_human_review` computation** (Failures #1, #2)
   - Add confidence < 0.7 threshold check (BR-HAPI-197)
   - Add workflow_id=="" check for no_matching_workflows
   - Set `human_review_reason` enum correctly

2. **Fix `alternative_workflows` extraction** (Failure #3)
   - Parse `alternative_workflows` from LLM response
   - Add to `IncidentResponse` model
   - Include in audit events (BR-AUDIT-005)

3. **Test with Mock LLM scenarios**:
   - `MOCK_LOW_CONFIDENCE` ‚Üí `needs_human_review=true`, `reason="low_confidence"`
   - `MOCK_NO_WORKFLOW_FOUND` ‚Üí `needs_human_review=true`, `reason="no_matching_workflows"`
   - `MOCK_MAX_RETRIES_EXHAUSTED` ‚Üí `needs_human_review=true`, `reason="llm_parsing_error"`

### **For AIAnalysis Team** (This Session)

1. ‚úÖ **Mock policy refactoring: COMPLETE**
2. ‚è≠Ô∏è **Un-skip 2 tests**: Proceed with updating tests to use Mock LLM scenarios (tasks #1, #2)
3. ‚è≥ **Wait for HAPI fixes**: The 3 test failures will resolve once HAPI bugs are fixed

---

## üìö **Related Documents**

- `docs/handoff/AA_INTEGRATION_MOCK_POLICY_FIX_FEB_02_2026.md` - Mock policy refactoring
- `docs/handoff/MOCK_LLM_SCENARIO_CONTROL_TRIAGE_FEB_02_2026.md` - Mock LLM scenarios
- `test/services/mock-llm/src/server.py` - Mock LLM scenario definitions (lines 80-315)
- `test/services/mock-llm/README.md` - Mock LLM documentation

---

## üéì **Lessons Learned**

1. **Integration testing reveals business logic bugs** - Using real HAPI (vs mocks) uncovered 3 HAPI bugs that mocks would have hidden.

2. **Mock policy compliance was correct** - Moving from in-memory mocks to real services was the right decision.

3. **Mock LLM works perfectly** - File-based configuration (DD-TEST-011 v2.0) and scenario control are robust.

4. **HAPI needs additional validation** - Response processing logic incomplete for human review scenarios.

---

**Status**: ‚úÖ **Triage Complete - Root causes identified**  
**Owner**: HAPI Team (for fixes), AIAnalysis Team (for test updates)  
**Timeline**: Blocked on HAPI fixes for 3 test failures
