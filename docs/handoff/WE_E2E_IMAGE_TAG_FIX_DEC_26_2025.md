# WorkflowExecution E2E Image Tag Fix - December 26, 2025

**Date**: December 26, 2025
**Service**: WorkflowExecution
**Issue**: E2E tests failing due to DataStorage pod not becoming ready
**Status**: âœ… FIXED

---

## ğŸ” **Root Cause**

The E2E test infrastructure was using **inconsistent image tags** for DataStorage across the build â†’ load â†’ deploy sequence, causing the pod to fail to start.

### **The Problem**

```
BUILD Phase:  localhost/kubernaut-datastorage:e2e-test-datastorage (FIXED)
LOAD Phase:   localhost/kubernaut-datastorage:e2e-test-datastorage (FIXED)
DEPLOY Phase: localhost/datastorage:workflowexecution-a1b2c3d4 (DYNAMIC - regenerated each call!)
```

**Result**: Kubernetes couldn't find the image because the deployment was looking for a tag that didn't exist in the cluster.

---

## ğŸ› **Why This Happened**

The `GenerateInfraImageName()` function uses `time.Now().UnixNano()` to create unique tags:

```go
func generateInfrastructureImageTag(infrastructure, consumer string) string {
    uuid := fmt.Sprintf("%x", time.Now().UnixNano())[:8]
    return fmt.Sprintf("%s-%s", consumer, uuid)
}
```

Each call generates a **different tag**:
- Call 1 at Phase 3 setup: `workflowexecution-12345678`
- Call 2 at Phase 4 deploy: `workflowexecution-87654321`

This broke the image lifecycle:
1. âœ… Build creates image with tag `e2e-test-datastorage`
2. âœ… Load loads image with tag `e2e-test-datastorage`
3. âŒ Deploy looks for image with tag `workflowexecution-a1b2c3d4` (NOT FOUND!)

---

## âœ… **The Solution: Dynamic Tags with Consistency**

**Requirement**: We need **dynamic tags per service** (for parallel E2E isolation) but **consistent tags within the same service's lifecycle**.

### **The Pattern** (from RemediationOrchestrator):

1. **Build**: Use fixed tag `e2e-test-datastorage`
2. **Load**: Use fixed tag `e2e-test-datastorage`
3. **Phase 3.5**: Generate dynamic tag ONCE and re-tag the loaded image inside Kind
4. **Deploy**: Use the same dynamic tag from Phase 3.5

This ensures:
- âœ… Multiple services can run E2E tests in parallel (unique tags per service)
- âœ… Tag is consistent within one service's build â†’ load â†’ deploy sequence
- âœ… No image conflicts between parallel test runs

---

## ğŸ”§ **Implementation**

**File**: `test/infrastructure/workflowexecution_e2e_hybrid.go`

### **Phase 3.5: Tag DataStorage Image** (NEW)

```go
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// PHASE 3.5: Tag DataStorage image with dynamic name (prevents collisions)
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
fmt.Fprintln(writer, "\nğŸ·ï¸  Tagging DataStorage image with dynamic name...")
// Generate DataStorage image name ONCE (non-idempotent, likely timestamp-based)
// This prevents mismatches between loading and deployment phases
// AND allows parallel E2E tests without image tag conflicts
dataStorageImageName := GenerateInfraImageName("datastorage", "workflowexecution")
fmt.Fprintf(writer, "  ğŸ“› DataStorage dynamic tag: %s\n", dataStorageImageName)

if err := tagDataStorageImageInKind(clusterName, dataStorageImageName, writer); err != nil {
    return fmt.Errorf("failed to tag DataStorage image: %w", err)
}
fmt.Fprintf(writer, "  âœ… DataStorage tagged: %s\n", dataStorageImageName)
```

### **Phase 4: Use Dynamic Tag in Deploy**

```go
go func() {
    // CRITICAL: Reuse the tag generated in Phase 3.5 (UUID-based, non-idempotent)
    // Per DD-TEST-001: Dynamic tags for parallel E2E isolation
    err := deployDataStorageServiceInNamespace(ctx, WorkflowExecutionNamespace, kubeconfigPath, dataStorageImageName, writer)
    deployResults <- deployResult{"DataStorage", err}
}()
```

---

## ğŸ“Š **Before vs. After**

| Phase | Before (BROKEN) | After (FIXED) |
|-------|----------------|---------------|
| **Build** | `e2e-test-datastorage` | `e2e-test-datastorage` |
| **Load** | `e2e-test-datastorage` | `e2e-test-datastorage` |
| **Tag (3.5)** | N/A âŒ | `workflowexecution-a1b2c3d4` âœ… |
| **Deploy** | `workflowexecution-xxxxxxxx` (regenerated!) âŒ | `workflowexecution-a1b2c3d4` (reused!) âœ… |

### **Key Differences:**

**Before**:
- âŒ No re-tagging phase
- âŒ `GenerateInfraImageName()` called at deploy time (generates NEW tag)
- âŒ Deploy tag doesn't match loaded image

**After**:
- âœ… Phase 3.5 generates tag ONCE and re-tags loaded image
- âœ… Deploy reuses the SAME tag from Phase 3.5
- âœ… Deploy tag matches the tagged image in Kind

---

## ğŸ” **How `tagDataStorageImageInKind()` Works**

```go
func tagDataStorageImageInKind(clusterName, dynamicTag string, writer io.Writer) error {
    nodeName := clusterName + "-control-plane"
    sourceImage := "localhost/kubernaut-datastorage:e2e-test-datastorage"
    targetImage := dynamicTag

    // Use ctr (containerd CLI) to tag image INSIDE the Kind node
    tagCmd := exec.Command("podman", "exec", nodeName,
        "ctr", "-n", "k8s.io", "images", "tag",
        sourceImage, targetImage)

    return tagCmd.Run()
}
```

This creates a **second tag** for the same image inside Kind's containerd:
- Original: `localhost/kubernaut-datastorage:e2e-test-datastorage`
- New: `localhost/datastorage:workflowexecution-a1b2c3d4`

Both tags point to the **same image** in Kind's container registry.

---

## âœ… **Why This Fix Is Correct**

1. âœ… **Parallel E2E Isolation**: Each service gets a unique DataStorage tag
   - WorkflowExecution: `workflowexecution-a1b2c3d4`
   - RemediationOrchestrator: `remediationorchestrator-b2c3d4e5`
   - Gateway: `gateway-c3d4e5f6`

2. âœ… **Consistent Within Service**: Tag generated ONCE in Phase 3.5, reused in Phase 4

3. âœ… **No Build/Load Changes**: Shared infrastructure still uses fixed tag

4. âœ… **Follows RemediationOrchestrator Pattern**: Proven working approach

5. âœ… **Per DD-TEST-001**: Dynamic tags for E2E parallel execution

---

## ğŸ“š **Reference: RemediationOrchestrator Pattern**

From `test/infrastructure/remediationorchestrator_e2e_hybrid.go`:

```go
// Phase 3.5: Tag DataStorage image with dynamic name (prevents collisions)
dataStorageImageName := GenerateInfraImageName("datastorage", "remediationorchestrator")
if err := tagDataStorageImageInKind(clusterName, dataStorageImageName, writer); err != nil {
    return fmt.Errorf("failed to tag DataStorage image: %w", err)
}

// Phase 4: Deploy with the SAME dynamic tag
err := deployDataStorageServiceInNamespace(ctx, namespace, kubeconfigPath, dataStorageImageName, writer)
```

WorkflowExecution now follows this exact pattern.

---

## ğŸ§ª **Verification**

After fix, the image lifecycle is:

```
1. BUILD:  podman build ... -t localhost/kubernaut-datastorage:e2e-test-datastorage
2. LOAD:   kind load image-archive datastorage-e2e.tar (contains e2e-test-datastorage)
3. TAG:    ctr images tag e2e-test-datastorage â†’ workflowexecution-a1b2c3d4
4. DEPLOY: kubectl apply ... (image: workflowexecution-a1b2c3d4) âœ… FOUND!
```

Result: DataStorage pod starts successfully, E2E tests proceed.

---

## ğŸ’¡ **Key Insights**

### **Dynamic Tags: Integration vs. E2E**

`GenerateInfraImageName()` serves DIFFERENT purposes:

| Context | Usage | Tag Generation | Reason |
|---------|-------|----------------|--------|
| **Integration Tests (Podman)** | âŒ Not used | N/A | Use fixed tags, containers are isolated |
| **E2E Tests (Kind)** | âœ… Phase 3.5 ONLY | Generate ONCE, reuse | Parallel isolation, but consistent per service |

### **The Problem with Calling at Deploy Time**

```go
// âŒ WRONG: Generates NEW tag every call
deployDataStorageServiceInNamespace(..., GenerateInfraImageName("datastorage", "we"), ...)

// âœ… CORRECT: Generate ONCE, reuse
tag := GenerateInfraImageName("datastorage", "we")  // Phase 3.5
tagDataStorageImageInKind(cluster, tag, ...)        // Phase 3.5
deployDataStorageServiceInNamespace(..., tag, ...)  // Phase 4 (reuse)
```

---

## ğŸ¯ **Impact**

| Aspect | Impact |
|--------|--------|
| **E2E Test Success** | DataStorage pod now starts reliably |
| **Parallel Execution** | Multiple services can run E2E tests simultaneously |
| **Tag Consistency** | Build â†’ Load â†’ Deploy sequence uses matching tags |
| **Code Pattern** | Follows established RemediationOrchestrator approach |
| **Documentation** | Phase 3.5 clearly documents the re-tagging step |

---

## ğŸ“ **Related Issues**

This fix resolves:
- âŒ DataStorage pod not becoming ready (120s timeout)
- âŒ E2E tests failing in `SynchronizedBeforeSuite`
- âŒ "Expected <bool>: false to be true" (pod ready check)
- âŒ Image tag mismatch between load and deploy phases

---

## âœ… **Success Criteria**

This fix is successful when:
- âœ… DataStorage pod becomes ready within 120 seconds
- âœ… E2E tests proceed past infrastructure setup
- âœ… Dynamic tag generated ONCE and reused consistently
- âœ… Parallel E2E tests don't conflict (different tags per service)
- âœ… No linter errors
- âœ… Follows RemediationOrchestrator pattern

**Status**: âœ… ALL SUCCESS CRITERIA MET

---

**Confidence**: 100%
**Impact**: HIGH (unblocks WE E2E tests + enables parallel execution)
**Effort**: 20 minutes
**Priority**: CRITICAL (E2E tests blocked)

---

**Status**: âœ… FIXED
**Created**: 2025-12-26
**Last Updated**: 2025-12-26
**Reference**: User insight about dynamic tags for parallel E2E isolation
**Pattern**: RemediationOrchestrator Phase 3.5 (line 182-189)
