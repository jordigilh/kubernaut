package intelligence

import (
	"fmt"
	"runtime"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/sirupsen/logrus"

	"github.com/jordigilh/kubernaut/pkg/intelligence/clustering"
	"github.com/jordigilh/kubernaut/pkg/intelligence/learning"
	"github.com/jordigilh/kubernaut/pkg/intelligence/patterns"
	sharedtypes "github.com/jordigilh/kubernaut/pkg/shared/types"
	"github.com/jordigilh/kubernaut/pkg/workflow/engine"
)

// Suite structure moved to intelligence_suite_test.go - generated by ginkgo bootstrap

var _ = Describe("Intelligence Performance - Business Requirements Testing", func() {
	var (
		clusteringEngine     *clustering.ClusteringEngine
		statisticalValidator *learning.StatisticalValidator
		config               *patterns.PatternDiscoveryConfig
		logger               *logrus.Logger
	)

	BeforeEach(func() {
		logger = logrus.New()
		logger.SetLevel(logrus.WarnLevel)

		config = &patterns.PatternDiscoveryConfig{
			MinExecutionsForPattern: 5,
			MaxHistoryDays:          30,
			SamplingInterval:        time.Hour,
			SimilarityThreshold:     0.85,
			ClusteringEpsilon:       0.3,
			MinClusterSize:          3,
			ModelUpdateInterval:     time.Hour,
			FeatureWindowSize:       20,
		}

		clusteringEngine = clustering.NewClusteringEngine(config, logger)

		learningConfig := &learning.PatternDiscoveryConfig{
			MinExecutionsForPattern: config.MinExecutionsForPattern,
			MaxHistoryDays:          config.MaxHistoryDays,
		}
		statisticalValidator = learning.NewStatisticalValidator(learningConfig, logger)
	})

	// BR-PERF-001: MUST meet performance benchmarks for real-time clustering analysis
	Context("BR-PERF-001: Real-Time Clustering Performance", func() {
		It("should perform clustering within business-required latency thresholds", func() {
			// Given: Large dataset requiring real-time clustering for business operations
			largeDataset := createLargeClusteringDataset(500)
			businessLatencyRequirement := 10 * time.Second

			// When: Performing clustering under business time constraints
			startTime := time.Now()
			result, err := clusteringEngine.ClusterExecutionPatterns(largeDataset)
			clusteringLatency := time.Since(startTime)

			// Then: Should meet business latency requirements for real-time operations
			Expect(err).ToNot(HaveOccurred(), "Clustering should succeed under performance pressure")
			Expect(clusteringLatency).To(BeNumerically("<=", businessLatencyRequirement),
				"Should complete clustering within %v for business real-time requirements", businessLatencyRequirement)

			// **Business Value**: Verify performance enables real-time business decisions
			Expect(result.Silhouette).To(BeNumerically(">=", 0.0), "Should maintain clustering quality under performance pressure")
			Expect(result.AnalyzedPoints).To(Equal(len(largeDataset)), "Should analyze all data points efficiently")
		})

		It("should handle varying dataset sizes efficiently", func() {
			// Given: Datasets of different sizes to test business scalability
			dataSizes := []int{50, 100, 200}

			var previousDuration time.Duration

			for i, dataSize := range dataSizes {
				// When: Processing datasets of increasing size for business growth
				dataset := createLargeClusteringDataset(dataSize)

				startTime := time.Now()
				result, err := clusteringEngine.ClusterExecutionPatterns(dataset)
				currentDuration := time.Since(startTime)

				// Then: Should maintain functional correctness at business scale
				Expect(err).ToNot(HaveOccurred(), "Should handle dataset size %d for business scalability", dataSize)
				Expect(result.AnalyzedPoints).To(Equal(dataSize), "Should process all data points at scale")
				Expect(result.Silhouette).To(BeNumerically(">=", 0.0), "Should maintain clustering quality at business scale")

				// **Business Value**: Verify system scales functionally (no threshold testing)
				if i > 0 {
					growthRatio := float64(currentDuration) / float64(previousDuration)

					// Test business value: Performance metrics are reasonable and functional
					Expect(growthRatio).To(BeNumerically(">", 0), "Performance ratio should be positive (functional scaling)")
					Expect(growthRatio).To(BeNumerically("<", 100), "Performance ratio should be reasonable (no extreme degradation)")

					// Log performance for monitoring without enforcing brittle thresholds
					fmt.Printf("Dataset scaling from %d to %d: ratio %.2f (functional scaling verified)\n",
						dataSizes[i-1], dataSize, growthRatio)
				}

				previousDuration = currentDuration
			}
		})
	})

	// BR-PERF-002: MUST optimize memory usage for large-scale analytics
	Context("BR-PERF-002: Memory Usage Optimization", func() {
		It("should maintain acceptable memory usage during clustering", func() {
			// Given: Large dataset requiring memory-efficient clustering for business operations
			largeClusteringData := createLargeClusteringDataset(1000)
			businessMemoryLimit := int64(200 * 1024 * 1024) // 200MB business limit

			// When: Performing memory-intensive clustering operations
			var memBefore, memAfter runtime.MemStats
			runtime.GC()
			runtime.ReadMemStats(&memBefore)

			result, err := clusteringEngine.ClusterExecutionPatterns(largeClusteringData)

			runtime.GC()
			runtime.ReadMemStats(&memAfter)
			memoryUsed := int64(memAfter.Alloc - memBefore.Alloc)

			// Then: Should maintain memory usage within business limits
			Expect(err).ToNot(HaveOccurred(), "Large-scale clustering should succeed within memory constraints")
			Expect(memoryUsed).To(BeNumerically("<=", businessMemoryLimit),
				"Should use <= %d MB memory for business operational limits", businessMemoryLimit/(1024*1024))

			// **Business Value**: Verify memory efficiency enables business scale operations
			Expect(result.Silhouette).To(BeNumerically(">=", 0.0), "Should maintain clustering quality despite memory constraints")
			Expect(result.AnalyzedPoints).To(Equal(len(largeClusteringData)), "Should process all data points within memory limits")
		})

		It("should handle statistical validation efficiently", func() {
			// Given: Large statistical validation workload
			largeValidationDataset := createLargeStatisticalDataset(500)
			businessMemoryThreshold := int64(100 * 1024 * 1024) // 100MB threshold

			// When: Performing statistical validation
			var memBefore, memAfter runtime.MemStats
			runtime.GC()
			runtime.ReadMemStats(&memBefore)

			assumptions := statisticalValidator.ValidateStatisticalAssumptions(largeValidationDataset)

			runtime.GC()
			runtime.ReadMemStats(&memAfter)
			memoryUsed := int64(memAfter.Alloc - memBefore.Alloc)

			// Then: Should handle large datasets efficiently for business operations
			Expect(len(assumptions.Assumptions)).To(BeNumerically(">=", 0), "Should return validation results")
			Expect(memoryUsed).To(BeNumerically("<=", businessMemoryThreshold),
				"Should use <= %d MB for business memory efficiency", businessMemoryThreshold/(1024*1024))

			// **Business Value**: Verify memory efficiency supports large business datasets
			Expect(len(assumptions.Assumptions)).To(BeNumerically(">=", 0), "Should provide statistical assumptions despite large dataset")
		})
	})

	// BR-PERF-003: MUST provide performance monitoring and optimization insights
	Context("BR-PERF-003: Performance Monitoring and Optimization", func() {
		It("should provide performance insights for business monitoring", func() {
			// Given: Analytics operations requiring business performance assessment
			monitoringDataset := createPerformanceMonitoringDataset(200)

			// When: Executing operations with performance tracking
			operationMetrics := make(map[string]time.Duration)

			// Clustering Performance
			clusteringData := convertToClusteringData(monitoringDataset)
			clusterStart := time.Now()
			clusters, err := clusteringEngine.ClusterExecutionPatterns(clusteringData)
			operationMetrics["clustering"] = time.Since(clusterStart)

			// Statistical Validation Performance
			statisticalData := createLargeStatisticalDataset(200)
			statStart := time.Now()
			statResult := statisticalValidator.ValidateStatisticalAssumptions(statisticalData)
			operationMetrics["statistical_validation"] = time.Since(statStart)

			// Then: Should provide performance insights for business optimization
			Expect(err).ToNot(HaveOccurred(), "Monitored operations should succeed")

			// **Business Value**: Verify performance tracking enables business optimization
			for operationName, duration := range operationMetrics {
				Expect(duration).To(BeNumerically(">", 0), "Should track execution time for %s", operationName)
				Expect(duration).To(BeNumerically("<=", 5*time.Second), "Should complete %s within reasonable time for business use", operationName)
			}

			// Validate results quality wasn't sacrificed for performance
			Expect(clusters.Silhouette).To(BeNumerically(">=", 0.0), "Should maintain clustering quality during monitoring")
			Expect(statResult.DataQualityScore).To(BeNumerically(">=", 0.5), "Should maintain statistical validation quality")
		})

		It("should detect performance degradation scenarios", func() {
			// Given: Performance baseline and potentially degraded performance scenarios
			baselineDataset := convertToClusteringData(createPerformanceMonitoringDataset(150))
			stressedDataset := convertToClusteringData(createDegradedPerformanceDataset(150))

			// When: Comparing performance across different scenarios for business insights
			baselineStart := time.Now()
			baselineClusters, _ := clusteringEngine.ClusterExecutionPatterns(baselineDataset)
			baselineLatency := time.Since(baselineStart)

			stressedStart := time.Now()
			stressedClusters, _ := clusteringEngine.ClusterExecutionPatterns(stressedDataset)
			stressedLatency := time.Since(stressedStart)

			// Then: Should provide performance comparison for business analysis
			degradationRatio := float64(stressedLatency) / float64(baselineLatency)

			// **Business Value**: Verify performance analysis enables business optimization decisions
			Expect(degradationRatio).To(BeNumerically(">", 0.0), "Should measure performance differences")

			// Even under stress, should maintain reasonable performance
			Expect(stressedLatency).To(BeNumerically("<=", 10*time.Second), "Should maintain acceptable performance under stress")

			// Quality should be maintained
			Expect(baselineClusters.Silhouette).To(BeNumerically(">=", 0.0), "Should maintain clustering quality under normal conditions")
			Expect(stressedClusters.Silhouette).To(BeNumerically(">=", 0.0), "Should maintain clustering quality under stress")

			// Business requirement: Quality degradation should be within acceptable bounds
			qualityDegradation := baselineClusters.Silhouette - stressedClusters.Silhouette
			Expect(qualityDegradation).To(BeNumerically("<=", 0.3), "Quality degradation should be within business tolerance")
		})
	})
})

// Helper functions for creating performance test data

func createLargeExecutionDataset(size int) []*engine.RuntimeWorkflowExecution {
	executions := make([]*engine.RuntimeWorkflowExecution, size)

	for i := 0; i < size; i++ {
		executions[i] = &engine.RuntimeWorkflowExecution{
			WorkflowExecutionRecord: sharedtypes.WorkflowExecutionRecord{
				ID:         fmt.Sprintf("perf-exec-%d", i),
				WorkflowID: fmt.Sprintf("perf-workflow-%d", i%10),
				StartTime:  time.Now().Add(-time.Duration(i) * time.Minute),
			},
		}

		endTime := executions[i].StartTime.Add(time.Duration(30+i%60) * time.Minute)
		executions[i].EndTime = &endTime
		executions[i].Duration = executions[i].EndTime.Sub(executions[i].StartTime)
	}

	return executions
}

func createLargeClusteringDataset(size int) []*engine.EngineWorkflowExecutionData {
	data := make([]*engine.EngineWorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		cluster := i % 8 // Create 8 distinct clusters

		data[i] = &engine.EngineWorkflowExecutionData{
			ExecutionID: fmt.Sprintf("cluster-exec-%d", i),
			WorkflowID:  fmt.Sprintf("cluster-workflow-%d", cluster),
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Minute),
			Duration:    time.Duration(20+cluster*10) * time.Minute,
			Success:     i%5 != 0, // 80% success rate
			Metrics: map[string]float64{
				"cpu_usage":    float64(20 + cluster*15),
				"memory_usage": float64(30 + cluster*20),
			},
			Metadata: map[string]interface{}{
				"cluster_group": cluster,
				"environment":   "production",
			},
		}
	}

	return data
}

func createLargeStatisticalDataset(size int) []*sharedtypes.WorkflowExecutionData {
	data := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		data[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("stat-exec-%d", i),
			WorkflowID:  fmt.Sprintf("stat-workflow-%d", i%5),
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    time.Duration(60+i*2) * time.Minute,
			Success:     i%4 != 0, // 75% success rate
			Metrics: map[string]float64{
				"execution_time": float64(60 + i*2),
				"cpu_usage":      float64(25 + i%40),
			},
			Metadata: map[string]interface{}{
				"environment": "production",
			},
		}
	}

	return data
}

func createPerformanceMonitoringDataset(size int) []*engine.RuntimeWorkflowExecution {
	executions := make([]*engine.RuntimeWorkflowExecution, size)

	for i := 0; i < size; i++ {
		executions[i] = &engine.RuntimeWorkflowExecution{
			WorkflowExecutionRecord: sharedtypes.WorkflowExecutionRecord{
				ID:         fmt.Sprintf("monitor-exec-%d", i),
				WorkflowID: fmt.Sprintf("monitor-workflow-%d", i%4),
				StartTime:  time.Now().Add(-time.Duration(i*2) * time.Minute),
			},
		}

		endTime := executions[i].StartTime.Add(time.Duration(35+i%25) * time.Minute)
		executions[i].EndTime = &endTime
		executions[i].Duration = executions[i].EndTime.Sub(executions[i].StartTime)
	}

	return executions
}

func createDegradedPerformanceDataset(size int) []*engine.RuntimeWorkflowExecution {
	executions := make([]*engine.RuntimeWorkflowExecution, size)

	for i := 0; i < size; i++ {
		executions[i] = &engine.RuntimeWorkflowExecution{
			WorkflowExecutionRecord: sharedtypes.WorkflowExecutionRecord{
				ID:         fmt.Sprintf("degraded-exec-%d", i),
				WorkflowID: fmt.Sprintf("degraded-workflow-%d", i%6),
				StartTime:  time.Now().Add(-time.Duration(i*3) * time.Minute),
			},
		}

		// Simulate degraded performance with longer execution times
		endTime := executions[i].StartTime.Add(time.Duration(80+i%40) * time.Minute) // Longer times
		executions[i].EndTime = &endTime
		executions[i].Duration = executions[i].EndTime.Sub(executions[i].StartTime)
	}

	return executions
}

func convertToClusteringData(executions []*engine.RuntimeWorkflowExecution) []*engine.EngineWorkflowExecutionData {
	data := make([]*engine.EngineWorkflowExecutionData, len(executions))

	for i, exec := range executions {
		data[i] = &engine.EngineWorkflowExecutionData{
			ExecutionID: exec.ID,
			WorkflowID:  exec.WorkflowID,
			Timestamp:   exec.StartTime,
			Duration:    exec.Duration,
			Success:     exec.EndTime != nil,
			Metrics: map[string]float64{
				"execution_time": exec.Duration.Seconds(),
			},
			Metadata: map[string]interface{}{
				"workflow_id": exec.WorkflowID,
			},
		}
	}

	return data
}
