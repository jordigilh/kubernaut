# Data Storage Service - Test Coverage Gap Analysis

**Date**: 2025-12-12
**Status**: üìã **GAP ANALYSIS** - New edge cases identified for business outcome validation
**Confidence**: 90%
**Scope**: All 3 test tiers (Unit, Integration, E2E)
**Authority**: Cross-referenced with authoritative BR documentation and design decisions

---

## üéØ **Executive Summary**

After successfully fixing all 613 tests (463 unit + 138 integration + 12 E2E), this analysis identifies **27 NEW test scenarios** covering business outcomes not yet validated in current test coverage.

### **Gap Distribution**

| Category | New Scenarios | Priority | Estimated Effort |
|----------|---------------|----------|------------------|
| **Self-Auditing Edge Cases** | 6 | P0-P1 | 3-4 hours |
| **Workflow Label Validation** | 5 | P0 | 2-3 hours |
| **DLQ & Error Recovery** | 4 | P0 | 3-4 hours |
| **Pagination & Query Edge Cases** | 4 | P1 | 2 hours |
| **Security & Sanitization** | 3 | P0 | 2 hours |
| **Cross-Service Integration** | 3 | P1 | 4-5 hours |
| **Performance & Limits** | 2 | P1 | 2-3 hours |
| **TOTAL** | **27 scenarios** | Mixed | **~20 hours** |

### **Confidence Assessment**

**High Confidence (90%)**:
- Gaps identified through systematic cross-reference of 45 BRs + 20+ design decisions
- All gaps have clear business outcomes and acceptance criteria
- Prioritization based on production risk and customer impact

---

## üìã **Gap Analysis by Business Outcome Category**

### **Category 1: Self-Auditing Edge Cases (BR-STORAGE-180-182)**

**Authoritative Reference**: DD-STORAGE-012 v4.0, BR-STORAGE-180-182

#### **Gap 1.1: Self-Audit During DLQ Fallback**
- **Priority**: P0
- **Business Outcome**: Ensure audit trail completeness when primary database is unavailable
- **Current Coverage**: ‚ùå **NOT TESTED**
  - Self-auditing tests assume PostgreSQL is available
  - DLQ tests don't verify meta-audit event generation
- **Missing Scenario**:
  ```
  GIVEN PostgreSQL is unavailable (scaled to 0)
  WHEN client writes audit event ‚Üí DLQ fallback (202 Accepted)
  THEN meta-audit event (datastorage.audit.written) should be generated with:
    - event_outcome: "accepted_dlq"
    - event_action: "dlq_fallback"
    - event_data.fallback_reason: "database_unavailable"
  ```
- **Acceptance Criteria**:
  - Meta-audit event created even when primary write fails
  - DLQ metadata captured in meta-audit event_data
  - No circular dependency (InternalAuditClient still functional)
- **Suggested Test**: Integration tier - `test/integration/datastorage/audit_self_auditing_dlq_test.go`
- **Estimated Effort**: 1-1.5 hours

#### **Gap 1.2: Self-Audit Batch Write Failure**
- **Priority**: P1
- **Business Outcome**: Track meta-audit batch write failures for observability
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - Single event failure tested, batch failure not tested
- **Missing Scenario**:
  ```
  GIVEN BufferedAuditStore has 100 pending meta-audit events
  WHEN batch write fails (PostgreSQL connection lost mid-batch)
  THEN:
    - Metric datastorage_self_audit_batch_write_failures_total increments
    - Error logged with batch_size context
    - Batch dropped (per ADR-038 non-blocking audit)
  ```
- **Acceptance Criteria**:
  - Prometheus metric correctly tracks batch failures
  - Service continues operation (non-blocking)
  - Error log includes batch metadata for debugging
- **Suggested Test**: Unit tier - `test/unit/datastorage/audit_self_auditing_batch_test.go`
- **Estimated Effort**: 45 minutes

#### **Gap 1.3: Concurrent Self-Audit Writes**
- **Priority**: P1
- **Business Outcome**: Verify thread-safety of BufferedAuditStore under concurrent load
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN 10 concurrent goroutines writing audit events
  WHEN each writes 100 events (1,000 total)
  THEN:
    - All meta-audit events generated correctly
    - No race conditions (verified by -race flag)
    - No data corruption (all 1,000 events persisted)
  ```
- **Acceptance Criteria**:
  - Test passes with `go test -race`
  - All meta-audit events have correct event_id uniqueness
  - No lost events (total count matches expected)
- **Suggested Test**: Integration tier - `test/integration/datastorage/audit_concurrent_test.go`
- **Estimated Effort**: 1 hour

#### **Gap 1.4: Self-Audit Buffer Overflow**
- **Priority**: P1
- **Business Outcome**: Validate buffer overflow protection prevents memory exhaustion
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN BufferedAuditStore with buffer size 1000
  WHEN client writes 2,000 events rapidly (faster than flush interval)
  THEN:
    - Oldest events dropped (FIFO eviction)
    - Metric datastorage_self_audit_events_dropped_total increments by ~1000
    - Service memory usage remains bounded
  ```
- **Acceptance Criteria**:
  - Buffer never exceeds configured size
  - Drop metric accurately tracks overflow
  - Service continues operation without OOM
- **Suggested Test**: Unit tier - `test/unit/datastorage/audit_buffer_overflow_test.go`
- **Estimated Effort**: 45 minutes

#### **Gap 1.5: Self-Audit Flush on Shutdown**
- **Priority**: P0
- **Business Outcome**: Ensure meta-audit events not lost during graceful shutdown
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN BufferedAuditStore has 50 pending meta-audit events
  WHEN graceful shutdown initiated (SIGTERM)
  THEN:
    - Flush triggered immediately (before 100ms interval)
    - All 50 events persisted to PostgreSQL
    - Shutdown completes within shutdownTimeout (30s)
  ```
- **Acceptance Criteria**:
  - Zero meta-audit events lost during shutdown
  - Flush occurs before database connection closed
  - Shutdown completes within configured timeout
- **Suggested Test**: Integration tier - `test/integration/datastorage/graceful_shutdown_self_audit_test.go`
- **Estimated Effort**: 1 hour

#### **Gap 1.6: Self-Audit Event Data Schema Validation**
- **Priority**: P1
- **Business Outcome**: Verify meta-audit event_data conforms to expected schema
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - event_data presence tested, schema not validated
- **Missing Scenario**:
  ```
  GIVEN client writes audit event with specific metadata
  WHEN meta-audit event generated
  THEN event_data JSON should contain:
    - original_event_id (UUID format)
    - original_event_type (string)
    - original_correlation_id (string)
    - write_duration_ms (integer > 0)
    - write_outcome (enum: "success", "dlq_fallback", "failed")
  ```
- **Acceptance Criteria**:
  - JSON schema validated using go-playground/validator
  - All required fields present
  - Field types and formats correct
- **Suggested Test**: Unit tier - `test/unit/datastorage/audit_event_data_schema_test.go`
- **Estimated Effort**: 30 minutes

---

### **Category 2: Workflow Label Validation (DD-WORKFLOW-001 v2.3)**

**Authoritative Reference**: DD-WORKFLOW-001 v2.3, BR-STORAGE-038-042

#### **Gap 2.1: Mandatory Label Completeness**
- **Priority**: P0
- **Business Outcome**: Reject workflow creation with missing mandatory labels
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - Happy path tested, all combinations of missing labels not tested
- **Missing Scenario**:
  ```
  GIVEN workflow creation request
  WHEN ANY of 5 mandatory labels is missing:
    - signal_type
    - severity
    - component
    - priority
    - environment
  THEN:
    - HTTP 400 Bad Request
    - RFC 7807 error with "detail" specifying missing field
    - Workflow NOT created in catalog
  ```
- **Acceptance Criteria**:
  - Test matrix: 5 scenarios (one per mandatory label)
  - Each scenario validates specific error message
  - Rollback successful (no partial writes)
- **Suggested Test**: Integration tier - `test/integration/datastorage/workflow_mandatory_labels_test.go`
- **Estimated Effort**: 45 minutes

#### **Gap 2.2: DetectedLabels Wildcard Validation**
- **Priority**: P0
- **Business Outcome**: Validate wildcard semantics for DetectedLabels string fields
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN workflow with DetectedLabels:
    - gitOpsTool: "*" (requires SOME value)
    - serviceMesh: (absent/empty - no requirement)
  WHEN workflow search filter specifies:
    - gitOpsTool: "argocd"
  THEN:
    - Workflow MATCHES (argocd satisfies "*" requirement)

  WHEN workflow search filter specifies:
    - gitOpsTool: "" (empty)
  THEN:
    - Workflow DOES NOT MATCH ("*" requires non-empty value)
  ```
- **Acceptance Criteria**:
  - Wildcard matching semantics per DD-WORKFLOW-001 v1.8
  - Test covers all DetectedLabels string fields: gitOpsTool, serviceMesh
  - Boolean fields (gitOpsManaged, pdbProtected, etc.) not affected by wildcard
- **Suggested Test**: Integration tier - `test/integration/datastorage/workflow_detected_labels_wildcard_test.go`
- **Estimated Effort**: 1 hour

#### **Gap 2.3: CustomLabels Validation Limits**
- **Priority**: P0
- **Business Outcome**: Enforce CustomLabels limits to prevent abuse
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN workflow creation with CustomLabels exceeding limits:
    - Max 10 keys (customer tries 15)
    - Max 5 values per key (customer tries 10)
    - Max 63 chars for key (customer tries 100)
    - Max 100 chars for value (customer tries 200)
  THEN:
    - HTTP 400 Bad Request
    - RFC 7807 error specifying which limit exceeded
    - Workflow NOT created
  ```
- **Acceptance Criteria**:
  - Test matrix: 4 scenarios (one per limit type)
  - Error messages clearly state exceeded limit
  - Limits per DD-WORKFLOW-001 v1.9
- **Suggested Test**: Unit tier - `test/unit/datastorage/workflow_custom_labels_limits_test.go`
- **Estimated Effort**: 45 minutes

#### **Gap 2.4: FailedDetections Array Validation**
- **Priority**: P1
- **Business Outcome**: Validate FailedDetections enum to prevent invalid field names
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN workflow search request with DetectedLabels.FailedDetections:
    - ["gitOpsManaged", "invalidFieldName", "pdbProtected"]
  THEN:
    - HTTP 400 Bad Request
    - Error: "FailedDetections contains invalid field name: invalidFieldName"
    - Valid enum: gitOpsManaged, gitOpsTool, networkIsolated, helmManaged,
                  stateful, hpaEnabled, pdbProtected, serviceMesh
  ```
- **Acceptance Criteria**:
  - Enum validation using go-playground/validator
  - All 8 valid DetectedLabels field names accepted
  - Any invalid field name rejected with clear error
- **Suggested Test**: Unit tier - `test/unit/datastorage/workflow_failed_detections_test.go`
- **Estimated Effort**: 30 minutes

#### **Gap 2.5: Mandatory Label Protection in CustomLabels**
- **Priority**: P0
- **Business Outcome**: Prevent CustomLabels from overriding mandatory system labels
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - Rego security wrapper tests exist, REST API validation not tested
- **Missing Scenario**:
  ```
  GIVEN workflow creation with CustomLabels attempting to override:
    - "signal_type": ["custom_value"] (system label)
    - "severity": ["custom_value"] (system label)
    - "component": ["custom_value"] (system label)
    - "priority": ["custom_value"] (system label)
    - "environment": ["custom_value"] (system label)
  THEN:
    - HTTP 400 Bad Request
    - Error: "CustomLabels cannot override mandatory system labels"
    - List of conflicting keys returned
  ```
- **Acceptance Criteria**:
  - 5 system labels protected (per DD-WORKFLOW-001 v1.9)
  - Security validation before Rego execution
  - Clear error message listing conflicting keys
- **Suggested Test**: Integration tier - `test/integration/datastorage/workflow_system_label_protection_test.go`
- **Estimated Effort**: 45 minutes

---

### **Category 3: DLQ & Error Recovery (BR-STORAGE-007, DD-009)**

**Authoritative Reference**: ADR-038, BR-STORAGE-007, DD-009 (DLQ Fallback)

#### **Gap 3.1: DLQ Recovery After PostgreSQL Restored**
- **Priority**: P0
- **Business Outcome**: Verify DLQ events automatically processed when database recovers
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - DLQ fallback tested, automatic recovery not tested
- **Missing Scenario**:
  ```
  GIVEN 50 audit events in DLQ (PostgreSQL was down)
  WHEN PostgreSQL becomes available again
  THEN within 30 seconds:
    - DLQ consumer processes all 50 events
    - All events persisted to PostgreSQL with original timestamps
    - DLQ stream emptied
    - Metric datastorage_dlq_events_recovered_total = 50
  ```
- **Acceptance Criteria**:
  - Automatic recovery (no manual intervention)
  - Original event timestamps preserved
  - DLQ consumer respects processing order (FIFO)
- **Suggested Test**: Integration tier - `test/integration/datastorage/dlq_recovery_test.go`
- **Estimated Effort**: 1.5 hours

#### **Gap 3.2: DLQ Overflow Protection**
- **Priority**: P1
- **Business Outcome**: Validate DLQ stream max length enforcement
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN DLQ stream max length 1000
  WHEN PostgreSQL down for extended period
  AND 1500 events written (DLQ fallback)
  THEN:
    - Oldest 500 events evicted (FIFO)
    - Newest 1000 events retained in DLQ
    - Metric datastorage_dlq_events_evicted_total = 500
    - Warning logged: "DLQ overflow - oldest events evicted"
  ```
- **Acceptance Criteria**:
  - DLQ never exceeds max length (1000)
  - Eviction metric accurate
  - Warning logged for observability
- **Suggested Test**: Integration tier - `test/integration/datastorage/dlq_overflow_test.go`
- **Estimated Effort**: 1 hour

#### **Gap 3.3: Partial Batch Write Failure**
- **Priority**: P1
- **Business Outcome**: Handle partial batch failures gracefully
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN batch of 10 audit events
  WHEN 7 writes succeed, 3 fail (e.g., validation errors)
  THEN:
    - 7 events persisted to PostgreSQL (HTTP 201)
    - 3 events sent to DLQ (HTTP 202)
    - Response includes:
      - success_count: 7
      - fallback_count: 3
      - failed_event_ids: [id1, id2, id3]
  ```
- **Acceptance Criteria**:
  - Successful writes committed (no rollback)
  - Failed events sent to DLQ
  - Detailed response for client debugging
- **Suggested Test**: Integration tier - `test/integration/datastorage/batch_partial_failure_test.go`
- **Estimated Effort**: 1 hour

#### **Gap 3.4: Redis Connection Failure During Write**
- **Priority**: P0
- **Business Outcome**: Gracefully degrade when Redis unavailable during DLQ fallback
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN PostgreSQL is down (scale=0)
  AND Redis is also down (DLQ unavailable)
  WHEN client writes audit event
  THEN:
    - HTTP 503 Service Unavailable
    - RFC 7807 error: "Audit persistence unavailable"
    - Metric datastorage_write_failures_total{reason="both_down"} increments
  ```
- **Acceptance Criteria**:
  - Service doesn't crash (graceful error)
  - Clear error message for client
  - Metric distinguishes "both_down" from other failures
- **Suggested Test**: Integration tier - `test/integration/datastorage/redis_and_postgres_down_test.go`
- **Estimated Effort**: 45 minutes

---

### **Category 4: Pagination & Query Edge Cases (BR-STORAGE-005, 006, DD-STORAGE-010)**

**Authoritative Reference**: DD-STORAGE-010 v1.0, BR-STORAGE-035 (V1.1 planned)

#### **Gap 4.1: Pagination Beyond Result Set**
- **Priority**: P1
- **Business Outcome**: Handle offset beyond total results gracefully
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - Within-bounds pagination tested, out-of-bounds not tested
- **Missing Scenario**:
  ```
  GIVEN 50 total audit events in database
  WHEN query with offset=100, limit=10
  THEN:
    - HTTP 200 OK (not 404)
    - Response: { "data": [], "pagination": { "total": 50, "has_more": false } }
    - Empty results, not error
  ```
- **Acceptance Criteria**:
  - Empty array returned (not error)
  - Pagination metadata correct (total reflects actual count)
  - Performance acceptable (no full table scan)
- **Suggested Test**: Integration tier - `test/integration/datastorage/query_offset_beyond_results_test.go`
- **Estimated Effort**: 30 minutes

#### **Gap 4.2: Cursor-Based Pagination Validation (V1.1)**
- **Priority**: P1 (V1.1 planned)
- **Business Outcome**: Validate cursor format and prevent cursor manipulation
- **Current Coverage**: ‚ùå **NOT TESTED** (V1.1 feature)
- **Missing Scenario**:
  ```
  GIVEN cursor-based pagination endpoint
  WHEN client provides invalid cursor:
    - Malformed base64
    - Tampered timestamp
    - Missing event_id
  THEN:
    - HTTP 400 Bad Request
    - Error: "Invalid cursor format"
    - Cursor structure: base64(event_timestamp + ":" + event_id)
  ```
- **Acceptance Criteria**:
  - Cursor validation before database query
  - Base64 decoding errors handled gracefully
  - Timestamp/event_id format validated
- **Suggested Test**: Integration tier - `test/integration/datastorage/cursor_pagination_validation_test.go` (V1.1)
- **Estimated Effort**: 1 hour (when V1.1 implemented)

#### **Gap 4.3: Query Performance with Large Offsets**
- **Priority**: P1
- **Business Outcome**: Validate acceptable performance for large offset values
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN 100,000 audit events in database
  WHEN query with offset=99000, limit=100
  THEN:
    - Query completes within 5 seconds (acceptable for admin queries)
    - Database uses index (not full table scan)
    - Metric datastorage_query_duration_seconds records latency
  ```
- **Acceptance Criteria**:
  - Query performance acceptable (<5s)
  - Database explain plan shows index usage
  - Prometheus metric tracks high-offset queries
- **Suggested Test**: Integration tier - `test/integration/datastorage/query_large_offset_performance_test.go`
- **Estimated Effort**: 1 hour

#### **Gap 4.4: Unicode Filter Parameter Injection**
- **Priority**: P0
- **Business Outcome**: Validate Unicode parameters don't bypass SQL injection protection
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - Unicode tested, but not combined with SQL injection attempts
- **Missing Scenario**:
  ```
  GIVEN query with Unicode namespace filter:
    - namespace: "prod-ÁéØÂ¢É-üî•'; DROP TABLE audit_events; --"
  THEN:
    - HTTP 200 OK (parameterized query prevents injection)
    - Query safely searches for literal string (including SQL keywords)
    - No database side effects (table not dropped)
  ```
- **Acceptance Criteria**:
  - Parameterized queries handle Unicode + SQL keywords
  - Unicode preserved through database round-trip
  - No SQL injection possible
- **Suggested Test**: Integration tier - `test/integration/datastorage/query_unicode_sql_injection_test.go`
- **Estimated Effort**: 30 minutes

---

### **Category 5: Security & Sanitization (BR-STORAGE-011, 025, 026)**

**Authoritative Reference**: BR-STORAGE-011, BR-STORAGE-025

#### **Gap 5.1: XSS via Workflow Description**
- **Priority**: P0
- **Business Outcome**: Prevent stored XSS attacks through workflow metadata
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - XSS sanitization tested for audit events, not workflow catalog
- **Missing Scenario**:
  ```
  GIVEN workflow creation with malicious description:
    - description: "<script>alert('XSS')</script>Workflow description"
  WHEN workflow retrieved via GET /api/v1/workflows/{id}
  THEN:
    - Script tags stripped
    - Response: "Workflow description" (sanitized)
    - No JavaScript execution in client
  ```
- **Acceptance Criteria**:
  - All HTML tags stripped from description
  - Event handlers removed (onclick, onerror, etc.)
  - Sanitization applied before persistence
- **Suggested Test**: Integration tier - `test/integration/datastorage/workflow_xss_prevention_test.go`
- **Estimated Effort**: 45 minutes

#### **Gap 5.2: SQL Injection via Workflow Search Filters**
- **Priority**: P0
- **Business Outcome**: Validate parameterized queries for workflow search filters
- **Current Coverage**: ‚ùå **NOT TESTED** for workflow search specifically
- **Missing Scenario**:
  ```
  GIVEN workflow search with malicious filter:
    - signal_type: "OOMKilled' OR '1'='1"
  THEN:
    - Parameterized query treats as literal string
    - Only workflows with exact signal_type match returned
    - No database side effects (no unauthorized data access)
  ```
- **Acceptance Criteria**:
  - All 5 mandatory label filters tested
  - DetectedLabels and CustomLabels filters tested
  - No SQL injection possible
- **Suggested Test**: Integration tier - `test/integration/datastorage/workflow_search_sql_injection_test.go`
- **Estimated Effort**: 45 minutes

#### **Gap 5.3: RFC 7807 Error Information Disclosure**
- **Priority**: P1
- **Business Outcome**: Ensure error responses don't leak sensitive information
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - RFC 7807 format tested, information disclosure not tested
- **Missing Scenario**:
  ```
  GIVEN database error during audit write:
    - PostgreSQL error: "duplicate key value violates unique constraint \"audit_events_pkey\""
  THEN RFC 7807 error should:
    - NOT include: PostgreSQL error details, stack traces, file paths
    - INCLUDE: Generic message "Database constraint violation"
    - INCLUDE: Appropriate HTTP status (409 Conflict)
  ```
- **Acceptance Criteria**:
  - Database error details sanitized
  - No stack traces in production responses
  - Errors logged server-side for debugging
- **Suggested Test**: Integration tier - `test/integration/datastorage/rfc7807_information_disclosure_test.go`
- **Estimated Effort**: 45 minutes

---

### **Category 6: Cross-Service Integration (ADR-032)**

**Authoritative Reference**: ADR-032, DD-HAPI-002, BR-STORAGE-039

#### **Gap 6.1: HolmesGPT-API Workflow Validation Flow**
- **Priority**: P1
- **Business Outcome**: Validate end-to-end workflow existence check flow
- **Current Coverage**: ‚ùå **NOT TESTED** - Workflow retrieval tested, but not HolmesGPT-API use case
- **Missing Scenario**:
  ```
  GIVEN HolmesGPT-API validate_workflow_exists tool
  WHEN AIAnalysis provides workflow_id: "invalid-uuid"
  THEN:
    - GET /api/v1/workflows/{invalid-uuid} returns HTTP 404
    - RFC 7807 error: "Workflow not found"
    - HolmesGPT-API returns error to AIAnalysis
    - AIAnalysis retries with different workflow
  ```
- **Acceptance Criteria**:
  - 404 response correctly formatted (RFC 7807)
  - Response time <100ms (fast validation)
  - Error message actionable for LLM
- **Suggested Test**: E2E tier - `test/e2e/datastorage/08_holmesgpt_workflow_validation_test.go`
- **Estimated Effort**: 1.5 hours

#### **Gap 6.2: Concurrent Workflow Search from Multiple Services**
- **Priority**: P1
- **Business Outcome**: Validate workflow search under concurrent load from multiple services
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN 5 concurrent clients (simulating Gateway, AIAnalysis, RemediationOrchestrator, etc.)
  WHEN each performs 20 workflow searches simultaneously (100 total)
  THEN:
    - All 100 searches complete successfully (HTTP 200)
    - Response times <500ms for 95th percentile
    - No connection pool exhaustion
    - No race conditions (verified by -race flag)
  ```
- **Acceptance Criteria**:
  - Test passes with `go test -race`
  - Connection pool configuration adequate
  - Prometheus metrics show healthy latency distribution
- **Suggested Test**: Integration tier - `test/integration/datastorage/workflow_search_concurrent_test.go`
- **Estimated Effort**: 1.5 hours

#### **Gap 6.3: Audit Event Correlation Across Services**
- **Priority**: P1
- **Business Outcome**: Validate correlation_id propagation through audit chain
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - Correlation tested per-service, not cross-service chain
- **Missing Scenario**:
  ```
  GIVEN remediation flow: Gateway ‚Üí AIAnalysis ‚Üí WorkflowExecution ‚Üí RemediationOrchestrator
  WHEN each service writes audit event with same correlation_id
  THEN query by correlation_id returns:
    - All 4 events in chronological order
    - Each event has correct service metadata
    - Timeline reconstructable from events
  ```
- **Acceptance Criteria**:
  - Correlation_id preserved across all services
  - Events retrievable in single query
  - Timestamps reflect actual execution order
- **Suggested Test**: E2E tier - `test/e2e/datastorage/09_audit_correlation_chain_test.go`
- **Estimated Effort**: 2 hours

---

### **Category 7: Performance & Limits (BR-STORAGE-007, 019)**

**Authoritative Reference**: BR-STORAGE-007, BR-STORAGE-027

#### **Gap 7.1: Workflow Search Result Set Limits**
- **Priority**: P1
- **Business Outcome**: Validate top_k limit enforcement in workflow search
- **Current Coverage**: ‚ö†Ô∏è **PARTIALLY TESTED** - Happy path with top_k=5 tested, boundary not tested
- **Missing Scenario**:
  ```
  GIVEN 100 workflows matching search criteria
  WHEN client requests top_k=1000 (excessive)
  THEN:
    - Response limited to max 100 (configurable limit)
    - Warning: "top_k clamped to maximum value"
    - Pagination link provided for remaining results
  ```
- **Acceptance Criteria**:
  - top_k clamped to reasonable maximum (e.g., 100)
  - Client informed of clamping
  - Performance acceptable even with max limit
- **Suggested Test**: Integration tier - `test/integration/datastorage/workflow_search_topk_limits_test.go`
- **Estimated Effort**: 45 minutes

#### **Gap 7.2: Audit Event Write Rate Limiting**
- **Priority**: P1
- **Business Outcome**: Validate service behavior under high write load
- **Current Coverage**: ‚ùå **NOT TESTED**
- **Missing Scenario**:
  ```
  GIVEN sustained write rate of 1000 events/second
  WHEN load continues for 60 seconds (60,000 events)
  THEN:
    - All events accepted (no writes rejected)
    - Average latency <10ms (p95 <50ms)
    - Database connection pool not exhausted
    - BufferedAuditStore flushes regularly (no buffer overflow)
  ```
- **Acceptance Criteria**:
  - Performance SLO met (10ms avg, 50ms p95)
  - No connection pool errors
  - Prometheus metrics show healthy throughput
- **Suggested Test**: Integration tier - `test/integration/datastorage/audit_write_performance_test.go`
- **Estimated Effort**: 1.5 hours

---

## üìä **Prioritized Implementation Roadmap**

### **Phase 1: Critical (P0) - Production Safety**
**Estimated Effort**: 10-12 hours
**Target**: Complete before V1.1 release

| Gap ID | Scenario | Tier | Effort |
|--------|----------|------|--------|
| 1.1 | Self-Audit During DLQ Fallback | Integration | 1-1.5h |
| 1.5 | Self-Audit Flush on Shutdown | Integration | 1h |
| 2.1 | Mandatory Label Completeness | Integration | 45m |
| 2.2 | DetectedLabels Wildcard Validation | Integration | 1h |
| 2.3 | CustomLabels Validation Limits | Unit | 45m |
| 2.5 | Mandatory Label Protection | Integration | 45m |
| 3.1 | DLQ Recovery After PostgreSQL Restored | Integration | 1.5h |
| 3.4 | Redis+PostgreSQL Both Down | Integration | 45m |
| 4.4 | Unicode SQL Injection | Integration | 30m |
| 5.1 | XSS via Workflow Description | Integration | 45m |
| 5.2 | SQL Injection via Workflow Search | Integration | 45m |

### **Phase 2: Important (P1) - Observability & Performance**
**Estimated Effort**: 8-10 hours
**Target**: V1.2 planning

| Gap ID | Scenario | Tier | Effort |
|--------|----------|------|--------|
| 1.2 | Self-Audit Batch Write Failure | Unit | 45m |
| 1.3 | Concurrent Self-Audit Writes | Integration | 1h |
| 1.4 | Self-Audit Buffer Overflow | Unit | 45m |
| 1.6 | Self-Audit Event Data Schema | Unit | 30m |
| 2.4 | FailedDetections Enum Validation | Unit | 30m |
| 3.2 | DLQ Overflow Protection | Integration | 1h |
| 3.3 | Partial Batch Write Failure | Integration | 1h |
| 4.1 | Pagination Beyond Result Set | Integration | 30m |
| 4.3 | Query Performance Large Offsets | Integration | 1h |
| 5.3 | RFC 7807 Information Disclosure | Integration | 45m |
| 6.1 | HolmesGPT-API Validation Flow | E2E | 1.5h |
| 6.2 | Concurrent Workflow Search | Integration | 1.5h |
| 6.3 | Audit Correlation Chain | E2E | 2h |
| 7.1 | Workflow Search top_k Limits | Integration | 45m |
| 7.2 | Audit Write Rate Limiting | Integration | 1.5h |

### **Phase 3: Future (V1.1+) - Feature Enhancements**
**Estimated Effort**: 1 hour
**Target**: When V1.1 features implemented

| Gap ID | Scenario | Tier | Effort |
|--------|----------|------|--------|
| 4.2 | Cursor-Based Pagination Validation | Integration | 1h |

---

## üéØ **Gap Analysis Confidence Assessment**

### **Confidence Breakdown**

| Confidence Level | Gap Count | Percentage | Rationale |
|------------------|-----------|------------|-----------|
| **High (95%)** | 20 gaps | 74% | Directly derived from authoritative BRs and DDs |
| **Medium (85%)** | 5 gaps | 19% | Inferred from cross-service integration patterns |
| **Low (70%)** | 2 gaps | 7% | Performance scenarios requiring production metrics |

### **Overall Confidence: 90%**

**Justification**:
- ‚úÖ All gaps cross-referenced with authoritative documentation
- ‚úÖ Business outcomes clearly defined
- ‚úÖ Acceptance criteria measurable and testable
- ‚úÖ Prioritization based on production risk assessment
- ‚ö†Ô∏è Some performance thresholds estimated (require production validation)

---

## üìã **Acceptance Criteria for Gap Resolution**

For each new test scenario:

1. **Test Passes Reliably**: No flaky tests (3 consecutive passes required)
2. **Business Outcome Validated**: Test verifies expected business behavior, not just technical function
3. **Documentation Updated**: Test purpose documented in test file comments
4. **BR Mapping**: Test annotated with relevant BR-STORAGE-XXX reference
5. **Metrics Validated**: If scenario involves metrics, Prometheus queries verified
6. **Error Paths Tested**: Both success and failure paths covered
7. **Cross-Referenced**: Test linked in BR documentation's "Test Coverage" section

---

## üîó **Related Documentation**

- **Business Requirements**: `docs/services/stateless/data-storage/BUSINESS_REQUIREMENTS.md` (v1.4)
- **Design Decisions**: `docs/architecture/decisions/DD-WORKFLOW-*.md`, `DD-STORAGE-*.md`
- **Self-Auditing**: `DD-STORAGE-012` v4.0 (Self-Auditing Implementation Plan)
- **Workflow Labels**: `DD-WORKFLOW-001` v2.3 (Mandatory Label Schema)
- **Hybrid Scoring**: `DD-WORKFLOW-004` v1.5 (Hybrid Weighted Label Scoring)
- **Pagination**: `DD-STORAGE-010` v1.0 (Query API Pagination Strategy)
- **Current Test Suite**: `docs/handoff/COMPLETE_DS_ALL_TESTS_PASSING.md`

---

## ‚úÖ **Next Steps**

1. **Review & Prioritization**: Team reviews gap analysis and confirms priorities
2. **Phase 1 Implementation**: Implement 11 P0 scenarios (10-12 hours)
3. **Test Execution**: Run full test suite (unit + integration + E2E)
4. **BR Documentation Update**: Add new test coverage to BR documentation
5. **Phase 2 Planning**: Schedule P1 scenarios for V1.2

---

**Last Updated**: 2025-12-12
**Next Review**: After Phase 1 implementation complete
**Confidence**: 90% (High confidence in gap identification and prioritization)
