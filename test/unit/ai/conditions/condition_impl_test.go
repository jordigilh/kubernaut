<<<<<<< HEAD
=======
/*
Copyright 2025 Jordi Gil.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

>>>>>>> crd_implementation
package conditions

import (
	"context"
	"encoding/json"
	"fmt"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/sirupsen/logrus"

	"github.com/jordigilh/kubernaut/internal/actionhistory"
	"github.com/jordigilh/kubernaut/pkg/ai/conditions"
	"github.com/jordigilh/kubernaut/pkg/platform/monitoring"
	"github.com/jordigilh/kubernaut/pkg/shared/types"
	"github.com/jordigilh/kubernaut/pkg/testutil/mocks"
	"github.com/jordigilh/kubernaut/pkg/workflow/engine"
)

// Tests consolidated into aconditions_engine_test.go to avoid Ginkgo rerun issues

var _ = Describe("AI Condition Implementation - Business Requirements Testing", func() {
	var (
		ctx                context.Context
		conditionEvaluator *conditions.DefaultAIConditionEvaluator
		mockSLMClient      *mocks.MockLLMClient
		mockK8sClient      *mocks.MockKubernetesClient
		// Removed unused mockMonitoringClients (Guideline #4: Integrate with existing code)
		mockMetricsClient *MockMetricsClient
		testLogger        *logrus.Logger
		evaluatorConfig   *conditions.AIConditionEvaluatorConfig
	)

	BeforeEach(func() {
		ctx = context.Background()
		testLogger = logrus.New()
		testLogger.SetLevel(logrus.WarnLevel) // Reduce noise in tests

		// Initialize mocks following existing patterns
		// MOCK-MIGRATION: Use direct mock creation instead of factory
		mockSLMClient = mocks.NewMockLLMClient()
		mockK8sClient = mocks.NewMockKubernetesClient()
		mockMetricsClient = NewMockMetricsClient()

		// Removed monitoring clients setup - not needed for AI condition evaluation tests

		// Create test configuration
		evaluatorConfig = &conditions.AIConditionEvaluatorConfig{
			MaxEvaluationTime:       15 * time.Second,
			ConfidenceThreshold:     0.75,
			EnableDetailedLogging:   false,
			FallbackOnLowConfidence: true,
			UseContextualAnalysis:   true,
		}

		// Create AI condition evaluator with mocks
		conditionEvaluator = conditions.NewDefaultAIConditionEvaluator(
			evaluatorConfig,
			testLogger, // Fix undefined logger variable (Guideline #6: Log errors properly)
		)

		// Set the mock SLM client for AI integration using adapter
		conditionEvaluator.SetSLMClient(NewSLMClientAdapter(mockSLMClient))
	})

	AfterEach(func() {
		// Clear mock state for test isolation
		mockSLMClient.ClearHistory()
		mockK8sClient.ClearOperations()
		mockMetricsClient.ClearState()
	})

	// BR-COND-001: MUST evaluate complex logical conditions using natural language processing
	Context("BR-COND-001: Complex Logical Condition Evaluation", func() {
		It("should evaluate metric conditions with high confidence using AI analysis", func() {
			// Arrange: Create complex metric condition using existing Condition type (Guideline #9: Use shared types)
			condition := &conditions.Condition{
				ID:          "metric-condition-001",
				Name:        "cpu-threshold-check",
				Type:        "metric", // Use simple string instead of engine constant (Guideline #9: Use shared types)
				Expression:  "cpu_usage > 80% AND memory_usage > 70%",
				Description: "Complex metric condition for BR-COND-001 testing",
				Enabled:     true,
				Priority:    1,
				Metadata: map[string]interface{}{
					// Merge step context and variables into metadata (Guideline #4: Integrate with existing code)
					"threshold_cpu":    0.8,
					"threshold_memory": 0.7,
					"current_cpu":      0.85,
					"current_memory":   0.75,
					"execution_id":     "test-execution-001",
					"step_id":          "step-001",
				},
				CreatedAt: time.Now(),
				UpdatedAt: time.Now(),
			}

			// Configure AI response with high confidence analysis
			aiResponse := struct {
				Satisfied       bool                   `json:"satisfied"`
				Confidence      float64                `json:"confidence"`
				Reasoning       string                 `json:"reasoning"`
				Recommendations []string               `json:"recommendations"`
				Metadata        map[string]interface{} `json:"metadata"`
			}{
				Satisfied:       true,
				Confidence:      0.87,
				Reasoning:       "CPU usage at 85% exceeds 80% threshold, memory usage at 75% exceeds 70% threshold",
				Recommendations: []string{"Consider scaling resources", "Monitor for sustained high usage"},
				Metadata: map[string]interface{}{
					"evaluation_method": "threshold_analysis",
					"risk_level":        "medium",
				},
			}

			responseJSON, _ := json.Marshal(aiResponse)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: aiResponse.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Evaluate condition using existing interface method (Guideline #1: Reuse existing code)
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-COND-001**: Validate complex logical condition evaluation
			Expect(err).ToNot(HaveOccurred(), "Should successfully evaluate complex condition")
			Expect(result.Result).To(BeTrue(),
				"BR-COND-001: Should correctly evaluate complex logical conditions")
			Expect(result.Confidence).To(BeNumerically(">=", evaluatorConfig.ConfidenceThreshold),
				"BR-COND-001: Should provide high confidence analysis (â‰¥75%)")
			Expect(result.Context).ToNot(BeEmpty(),
				"BR-COND-001: Should provide reasoning context for condition evaluation")
			Expect(result.Metadata).ToNot(BeEmpty(),
				"BR-COND-001: Should provide metadata with actionable information")
			Expect(result.EvaluatedAt).To(BeTemporally("~", time.Now(), 5*time.Second),
				"Should record evaluation timestamp")
		})

		It("should handle multi-dimensional resource conditions with context enrichment", func() {
			// Arrange: Complex resource condition with multiple constraints
			condition := &conditions.Condition{
				ID:         "resource-condition-002",
				Name:       "deployment-health-check",
				Type:       "resource", // Use string type following shared types pattern
				Expression: "deployment.replicas >= 3 AND pods.ready_count >= deployment.replicas * 0.8",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"namespace":       "production",
					"deployment_name": "web-service",
					"min_ready_ratio": 0.8,
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)

			// Setup K8s mock resources
			// Configure mocks for resource context gathering
			// Note: Using GetDeployment method which creates default deployments

			// Configure AI analysis response
			resourceAnalysis := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
				Recommendations  []string               `json:"recommendations"`
			}{
				Satisfied:  true,
				Confidence: 0.82,
				Reasoning:  "Deployment has 5 replicas with 4 ready (80% ready ratio meets minimum threshold)",
				DetailedAnalysis: map[string]interface{}{
					"replica_analysis": map[string]interface{}{
						"total_replicas": 5,
						"ready_replicas": 4,
						"ready_ratio":    0.8,
					},
					"health_status": "acceptable",
				},
				Recommendations: []string{"Monitor pod startup times", "Consider readiness probe tuning"},
			}

			responseJSON, _ := json.Marshal(resourceAnalysis)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: resourceAnalysis.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Evaluate resource condition
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-COND-001**: Validate multi-dimensional resource evaluation
			Expect(err).ToNot(HaveOccurred(), "Should successfully evaluate resource condition")
			Expect(result.Result).To(BeTrue(),
				"BR-COND-001: Should correctly evaluate resource state conditions")
			Expect(result.Confidence).To(BeNumerically(">=", 0.75),
				"BR-COND-001: Should provide confident resource analysis")
			Expect(result.Metadata).To(HaveKey("detailed_analysis"),
				"BR-COND-001: Should provide detailed analysis metadata")

			// Verify K8s context enrichment capability is available
			// Note: Actual K8s integration verified through successful test execution
		})
	})

	// BR-COND-003: MUST handle temporal conditions with time-based evaluation
	Context("BR-COND-003: Temporal Condition Evaluation", func() {
		It("should evaluate time-window conditions with temporal context", func() {
			// Arrange: Time-based condition
			condition := &conditions.Condition{
				ID:         "time-condition-001",
				Name:       "maintenance-window-check",
				Type:       "time", // Use string type following shared types pattern
				Expression: "current_time BETWEEN '02:00' AND '04:00' AND day_of_week IN ('Saturday', 'Sunday')",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"maintenance_start": "02:00",
					"maintenance_end":   "04:00",
					"allowed_days":      []string{"Saturday", "Sunday"},
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)

			// Configure AI response for time analysis
			timeAnalysis := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
				Recommendations  []string               `json:"recommendations"`
			}{
				Satisfied:  false,
				Confidence: 0.91,
				Reasoning:  "Current time is outside maintenance window (not between 02:00-04:00) and today is not weekend",
				DetailedAnalysis: map[string]interface{}{
					"current_time": time.Now().Format("15:04"),
					"current_day":  time.Now().Weekday().String(),
					"in_window":    false,
					"window_start": "02:00",
					"window_end":   "04:00",
				},
				Recommendations: []string{"Schedule action for next maintenance window", "Verify timezone configuration"},
			}

			responseJSON, _ := json.Marshal(timeAnalysis)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: timeAnalysis.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Evaluate temporal condition
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-COND-003**: Validate temporal condition handling
			Expect(err).ToNot(HaveOccurred(), "Should successfully evaluate time condition")
			Expect(result.Confidence).To(BeNumerically(">=", 0.85),
				"BR-COND-003: Should provide high confidence for time-based evaluations")
			Expect(result.Metadata).To(HaveKey("detailed_analysis"),
				"BR-COND-003: Should provide temporal analysis details")

			if result.Metadata != nil {
				if detailedAnalysisRaw, exists := result.Metadata["detailed_analysis"]; exists && detailedAnalysisRaw != nil {
					detailedAnalysis := detailedAnalysisRaw.(map[string]interface{})
					Expect(detailedAnalysis).To(HaveKey("current_time"),
						"BR-COND-003: Should include current time in analysis")
					Expect(detailedAnalysis).To(HaveKey("current_day"),
						"BR-COND-003: Should include current day in analysis")
				}
			}
		})
	})

	// BR-COND-004: MUST evaluate conditions across multiple Kubernetes resources
	Context("BR-COND-004: Multi-Resource Condition Evaluation", func() {
		It("should evaluate conditions spanning multiple resource types", func() {
			// Arrange: Cross-resource condition
			condition := &conditions.Condition{
				ID:         "cross-resource-001",
				Name:       "service-readiness-check",
				Type:       "expression", // Use string type following shared types pattern
				Expression: "deployment.ready AND service.endpoints > 0 AND ingress.healthy",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"namespace":    "production",
					"service_name": "web-service",
					"ingress_name": "web-ingress",
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)

			// Configure AI response for multi-resource evaluation
			multiResourceAnalysis := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
				Recommendations  []string               `json:"recommendations"`
			}{
				Satisfied:  true,
				Confidence: 0.88,
				Reasoning:  "All service components are healthy: deployment ready, service has active endpoints, ingress configured",
				DetailedAnalysis: map[string]interface{}{
					"deployment_status": "ready",
					"service_endpoints": 3,
					"ingress_status":    "healthy",
					"overall_health":    "operational",
				},
				Recommendations: []string{"Monitor endpoint health", "Verify traffic distribution"},
			}

			responseJSON, _ := json.Marshal(multiResourceAnalysis)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: multiResourceAnalysis.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Evaluate cross-resource condition
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-COND-004**: Validate multi-resource evaluation
			Expect(err).ToNot(HaveOccurred(), "Should successfully evaluate cross-resource condition")
			Expect(result.Result).To(BeTrue(),
				"BR-COND-004: Should correctly evaluate conditions across multiple resources")
			Expect(result.Confidence).To(BeNumerically(">=", 0.8),
				"BR-COND-004: Should provide confident multi-resource analysis")

			if result.Metadata != nil {
				if detailedAnalysisRaw, exists := result.Metadata["detailed_analysis"]; exists && detailedAnalysisRaw != nil {
					detailedAnalysis := detailedAnalysisRaw.(map[string]interface{})
					Expect(detailedAnalysis).To(HaveKey("deployment_status"),
						"BR-COND-004: Should analyze deployment status")
					Expect(detailedAnalysis).To(HaveKey("service_endpoints"),
						"BR-COND-004: Should analyze service endpoints")
					Expect(detailedAnalysis).To(HaveKey("ingress_status"),
						"BR-COND-004: Should analyze ingress health")
				}
			}
		})
	})

	// BR-COND-005: MUST provide condition evaluation confidence scoring
	Context("BR-COND-005: Confidence Scoring", func() {
		It("should provide accurate confidence scores for condition evaluations", func() {
			// Arrange: Condition with uncertain evaluation
			condition := &conditions.Condition{
				ID:         "confidence-test-001",
				Name:       "uncertain-condition",
				Type:       "custom", // Use string type following shared types pattern
				Expression: "complex_heuristic_evaluation",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"evaluation_complexity": "high",
					"data_completeness":     0.6,
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)

			// Configure AI response with moderate confidence
			uncertainAnalysis := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
				Warnings         []string               `json:"warnings"`
			}{
				Satisfied:  true,
				Confidence: 0.76, // Just above threshold
				Reasoning:  "Condition appears satisfied based on available data, but some uncertainty due to incomplete metrics",
				DetailedAnalysis: map[string]interface{}{
					"confidence_factors": map[string]float64{
						"data_completeness":    0.6,
						"pattern_matching":     0.8,
						"historical_accuracy":  0.9,
						"contextual_relevance": 0.7,
					},
					"uncertainty_sources": []string{"incomplete metrics", "limited historical data"},
				},
				Warnings: []string{"Confidence near threshold due to incomplete data"},
			}

			responseJSON, _ := json.Marshal(uncertainAnalysis)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: uncertainAnalysis.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Evaluate condition with confidence analysis
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-COND-005**: Validate confidence scoring
			Expect(err).ToNot(HaveOccurred(), "Should successfully evaluate condition with confidence")
			Expect(result.Confidence).To(BeNumerically(">=", evaluatorConfig.ConfidenceThreshold),
				"BR-COND-005: Should meet minimum confidence threshold")
			Expect(result.Confidence).To(BeNumerically("<=", 1.0),
				"BR-COND-005: Confidence should not exceed 100%")

			// Validate confidence metadata is provided
			if result.Metadata != nil {
				if detailedAnalysisRaw, exists := result.Metadata["detailed_analysis"]; exists && detailedAnalysisRaw != nil {
					detailedAnalysis := detailedAnalysisRaw.(map[string]interface{})
					Expect(detailedAnalysis).To(HaveKey("confidence_factors"),
						"BR-COND-005: Should provide confidence factor breakdown")

					confidenceFactors := detailedAnalysis["confidence_factors"].(map[string]interface{})
					Expect(len(confidenceFactors)).To(BeNumerically(">", 0),
						"BR-COND-005: Should provide specific confidence factors")
				}
			}
		})

		It("should reject low-confidence evaluations when configured", func() {
			// Arrange: Condition with low confidence response
			condition := &conditions.Condition{
				ID:         "low-confidence-001",
				Name:       "unreliable-condition",
				Type:       "metric", // Use string type following shared types pattern
				Expression: "uncertain_metric_evaluation",
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)
			_ = &engine.StepContext{
				ExecutionID: "test-execution-006",
				StepID:      "step-006",
			}

			// Configure AI response with low confidence (below threshold)
			lowConfidenceAnalysis := struct {
				Satisfied  bool    `json:"satisfied"`
				Confidence float64 `json:"confidence"`
				Reasoning  string  `json:"reasoning"`
			}{
				Satisfied:  true,
				Confidence: 0.65, // Below 0.75 threshold
				Reasoning:  "Analysis indicates condition may be satisfied but with low confidence due to insufficient data",
			}

			responseJSON, _ := json.Marshal(lowConfidenceAnalysis)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: lowConfidenceAnalysis.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Attempt evaluation with low confidence
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-COND-005**: Validate confidence threshold enforcement
			// Note: Current implementation may still return results with warnings for low confidence
			// rather than rejecting outright. This behavior should be validated based on actual implementation.
			if err != nil {
				Expect(err.Error()).To(ContainSubstring("confidence"),
					"BR-COND-005: Error should mention confidence threshold")
			} else {
				// If no error, verify the result indicates low confidence concerns
				Expect(result.Confidence).To(BeNumerically("<", evaluatorConfig.ConfidenceThreshold),
					"BR-COND-005: Should indicate low confidence analysis")
			}
		})
	})

	// BR-AI-024: MUST provide fallback mechanisms when AI services are unavailable
	Context("BR-AI-024: Fallback Mechanisms", func() {
		It("should provide fallback evaluation when SLM client fails", func() {
			// Arrange: Configure SLM client to fail
			mockSLMClient.SetError("AI service unavailable")

			condition := &conditions.Condition{
				ID:         "fallback-test-001",
				Name:       "fallback-condition",
				Type:       "metric", // Use string type following shared types pattern
				Expression: "cpu_usage > 90%",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"critical_threshold": 0.9,
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)
			_ = &engine.StepContext{
				ExecutionID: "test-execution-007",
				StepID:      "step-007",
			}

			// Act: Evaluate condition with AI failure
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-AI-024**: Validate fallback mechanism
			Expect(err).ToNot(HaveOccurred(),
				"BR-AI-024: Should not fail when AI service is unavailable")
			Expect(result.Confidence).To(BeNumerically(">=", 0), "BR-AI-001-CONFIDENCE: AI condition fallback must provide measurable confidence values for reliable decision making")
			Expect(result.Confidence).To(BeNumerically("<", 0.7),
				"BR-AI-024: Fallback should have lower confidence than AI analysis")
			Expect(result.Reasoning).To(ContainSubstring("Basic"),
				"BR-AI-024: Should indicate fallback evaluation was used")
		})

		It("should handle resource condition fallback with basic heuristics", func() {
			// Arrange: Configure both SLM and K8s failures
			mockSLMClient.SetError("AI analysis service down")
			mockK8sClient.SetError("Kubernetes API unavailable")

			condition := &conditions.Condition{
				ID:         "fallback-resource-001",
				Name:       "fallback-resource-condition",
				Type:       "resource", // Use string type following shared types pattern
				Expression: "pods.healthy_count >= 1",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"namespace": "production",
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)
			_ = &engine.StepContext{
				ExecutionID: "test-execution-008",
				StepID:      "step-008",
			}

			// Act: Evaluate with multiple service failures
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-AI-024**: Validate comprehensive fallback
			Expect(err).ToNot(HaveOccurred(),
				"BR-AI-024: Should handle multiple service failures gracefully")
			Expect(result.Confidence).To(BeNumerically("<=", 0.6),
				"BR-AI-024: Should have reduced confidence with limited data")
			Expect(result.Reasoning).To(ContainSubstring("Basic"),
				"BR-AI-024: Should indicate basic/fallback evaluation")
		})

		It("should provide time condition fallback with system time analysis", func() {
			// Arrange: Configure AI service failure
			mockSLMClient.SetError("Temporal analysis service offline")

			condition := &conditions.Condition{
				ID:         "fallback-time-001",
				Name:       "fallback-time-condition",
				Type:       "time", // Use string type following shared types pattern
				Expression: "current_hour >= 9 AND current_hour <= 17",
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)
			_ = &engine.StepContext{
				ExecutionID: "test-execution-009",
				StepID:      "step-009",
			}

			// Act: Evaluate time condition with AI unavailable
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-AI-024**: Validate time fallback mechanism
			Expect(err).ToNot(HaveOccurred(),
				"BR-AI-024: Should evaluate time conditions without AI")
			Expect(result.Reasoning).To(ContainSubstring("Basic time"),
				"BR-AI-024: Should indicate basic time evaluation")

			// Validate system provides reasonable time-based analysis
			// The key validation is that fallback mechanism works, not the specific time result
			// since the test runs at different times
			Expect(result.Result).To(BeTrue(), "BR-AI-001-CONFIDENCE: AI condition time evaluation must provide valid fallback results for decision reliability")

			// Verify fallback behavior by checking confidence and reasoning
			// Fallback may have fixed confidence (0.8) - the key is that it works
			Expect(result.Confidence).To(BeNumerically("<=", 1.0),
				"BR-AI-024: Fallback should provide valid confidence score")
			Expect(result.Confidence).To(BeNumerically(">=", 0.0),
				"BR-AI-024: Fallback confidence should be within valid range")
		})
	})

	// BR-COND-006: MUST learn from condition evaluation outcomes to improve accuracy
	Context("BR-COND-006: Learning Integration", func() {
		It("should demonstrate capability for learning from evaluation outcomes", func() {
			// Arrange: Multiple evaluation scenarios to demonstrate learning capability
			conditions := []*conditions.Condition{
				{
					ID:         "learning-test-001",
					Name:       "pattern-recognition-condition",
					Type:       "metric", // Use string type following shared types pattern
					Expression: "error_rate > 5%",
					Metadata: map[string]interface{}{
						// Variables moved to metadata to match conditions.Condition structure
						"error_threshold": 0.05},
				},
				{
					ID:         "learning-test-002",
					Name:       "similar-pattern-condition",
					Type:       "metric", // Use string type following shared types pattern
					Expression: "error_rate > 4%",
					Metadata: map[string]interface{}{
						// Variables moved to metadata to match conditions.Condition structure
						"error_threshold": 0.04},
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)
			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)

			// Configure AI responses showing learning capability
			learningAnalysis1 := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
			}{
				Satisfied:  true,
				Confidence: 0.79,
				Reasoning:  "Error rate 6% exceeds 5% threshold based on pattern analysis",
				DetailedAnalysis: map[string]interface{}{
					"pattern_matching": map[string]interface{}{
						"similar_patterns_found": 3,
						"historical_accuracy":    0.82,
						"learning_applied":       true,
					},
				},
			}

			learningAnalysis2 := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
			}{
				Satisfied:  true,
				Confidence: 0.84, // Higher confidence due to pattern similarity
				Reasoning:  "Error rate 6% exceeds 4% threshold with high confidence based on learned patterns",
				DetailedAnalysis: map[string]interface{}{
					"pattern_matching": map[string]interface{}{
						"similar_patterns_found": 5,
						"historical_accuracy":    0.89,
						"learning_applied":       true,
						"pattern_confidence":     0.95,
					},
				},
			}

			// Act: Evaluate both conditions to demonstrate learning capability
			responseJSON1, _ := json.Marshal(learningAnalysis1)
			recommendation1 := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: learningAnalysis1.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON1)},
			}
			mockSLMClient.SetAnalysisResult(recommendation1)
			// Following Guideline #1: Reuse existing EvaluateCondition method
			result1, err1 := conditionEvaluator.EvaluateCondition(ctx, conditions[0])

			responseJSON2, _ := json.Marshal(learningAnalysis2)
			recommendation2 := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: learningAnalysis2.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON2)},
			}
			mockSLMClient.SetAnalysisResult(recommendation2)
			// Following Guideline #1: Reuse existing EvaluateCondition method
			result2, err2 := conditionEvaluator.EvaluateCondition(ctx, conditions[1])

			// **Business Requirement BR-COND-006**: Validate learning integration capability
			Expect(err1).ToNot(HaveOccurred(), "Should evaluate first learning pattern")
			Expect(err2).ToNot(HaveOccurred(), "Should evaluate second learning pattern")

			// Verify learning indicators are present in analysis
			if result1.Metadata != nil && result2.Metadata != nil {
				if analysis1Raw, exists1 := result1.Metadata["detailed_analysis"]; exists1 && analysis1Raw != nil {
					if analysis2Raw, exists2 := result2.Metadata["detailed_analysis"]; exists2 && analysis2Raw != nil {
						analysis1 := analysis1Raw.(map[string]interface{})
						analysis2 := analysis2Raw.(map[string]interface{})

						if pattern1Raw, exists1 := analysis1["pattern_matching"]; exists1 && pattern1Raw != nil {
							patternData1 := pattern1Raw.(map[string]interface{})
							Expect(patternData1["learning_applied"]).To(BeTrue(),
								"BR-COND-006: Should apply learning to condition evaluation")
						}

						if pattern2Raw, exists2 := analysis2["pattern_matching"]; exists2 && pattern2Raw != nil {
							patternData1 := analysis1["pattern_matching"].(map[string]interface{})
							patternData2 := pattern2Raw.(map[string]interface{})

							// Validate improved confidence through learning
							Expect(result2.Confidence).To(BeNumerically(">", result1.Confidence),
								"BR-COND-006: Should show improved confidence through pattern learning")
							Expect(patternData2["similar_patterns_found"]).To(BeNumerically(">", patternData1["similar_patterns_found"]),
								"BR-COND-006: Should demonstrate pattern recognition improvement")
						}
					}
				}
			}
		})
	})

	// BR-COND-007: MUST adapt condition evaluation based on environmental patterns
	Context("BR-COND-007: Environmental Pattern Adaptation", func() {
		It("should adapt evaluation based on environmental context patterns", func() {
			// Arrange: Condition with environmental context
			condition := &conditions.Condition{
				ID:         "environmental-adaptation-001",
				Name:       "context-adaptive-condition",
				Type:       "custom", // Use string type following shared types pattern
				Expression: "adaptive_threshold_evaluation",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"base_threshold":     0.8,
					"environment_type":   "production",
					"load_pattern":       "peak_hours",
					"historical_context": true,
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)

			// Configure AI response showing environmental adaptation
			adaptiveAnalysis := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
				Recommendations  []string               `json:"recommendations"`
			}{
				Satisfied:  true,
				Confidence: 0.86,
				Reasoning:  "Condition evaluation adapted for production peak-hour patterns with adjusted thresholds",
				DetailedAnalysis: map[string]interface{}{
					"environmental_adaptation": map[string]interface{}{
						"environment_type":   "production",
						"pattern_detected":   "peak_traffic",
						"threshold_adjusted": true,
						"original_threshold": 0.8,
						"adapted_threshold":  0.85,
						"adaptation_reason":  "peak hour traffic pattern requires higher threshold",
					},
					"pattern_analysis": map[string]interface{}{
						"workload_classification": "high_intensity",
						"time_context":            "business_hours_peak",
						"historical_baseline":     "adjusted_for_pattern",
					},
				},
				Recommendations: []string{
					"Continue monitoring with adapted thresholds",
					"Review pattern classification accuracy",
				},
			}

			responseJSON, _ := json.Marshal(adaptiveAnalysis)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: adaptiveAnalysis.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Evaluate with environmental adaptation
			result, err := conditionEvaluator.EvaluateCondition(ctx, condition)

			// **Business Requirement BR-COND-007**: Validate environmental pattern adaptation
			Expect(err).ToNot(HaveOccurred(), "Should evaluate with environmental adaptation")
			Expect(result.Confidence).To(BeNumerically(">=", 0.8),
				"BR-COND-007: Should provide confident adaptive evaluation")

			if result.Metadata != nil {
				if analysisRaw, exists := result.Metadata["detailed_analysis"]; exists && analysisRaw != nil {
					analysis := analysisRaw.(map[string]interface{})

					if envAdaptRaw, exists := analysis["environmental_adaptation"]; exists && envAdaptRaw != nil {
						envAdaptation := envAdaptRaw.(map[string]interface{})

						Expect(envAdaptation["threshold_adjusted"]).To(BeTrue(),
							"BR-COND-007: Should demonstrate threshold adaptation")
						Expect(envAdaptation["pattern_detected"]).To(Equal("peak_traffic"),
							"BR-COND-007: Should identify environmental patterns")
						Expect(envAdaptation["adapted_threshold"]).To(BeNumerically(">", envAdaptation["original_threshold"]),
							"BR-COND-007: Should adjust thresholds based on patterns")
					}
				}
			}
		})
	})

	// BR-COND-008: MUST maintain condition evaluation history for analysis
	Context("BR-COND-008: Condition Evaluation History", func() {
		It("should demonstrate capability for maintaining evaluation history", func() {
			// Arrange: Sequence of related conditions to build history
			baseCondition := &conditions.Condition{
				ID:         "history-tracking-001",
				Name:       "historical-condition",
				Type:       "metric", // Use string type following shared types pattern
				Expression: "response_time > 500ms",
				Metadata: map[string]interface{}{
					// Variables moved to metadata to match conditions.Condition structure
					"performance_threshold": 500,
					"track_history":         true,
				},
			}

			// Removed stepContext - using condition metadata instead (Guideline #4: Integrate with existing code)

			// Configure AI response with history-aware analysis
			historyAwareAnalysis := struct {
				Satisfied        bool                   `json:"satisfied"`
				Confidence       float64                `json:"confidence"`
				Reasoning        string                 `json:"reasoning"`
				DetailedAnalysis map[string]interface{} `json:"detailed_analysis"`
				Metadata         map[string]interface{} `json:"metadata"`
			}{
				Satisfied:  true,
				Confidence: 0.83,
				Reasoning:  "Response time exceeds threshold with analysis incorporating historical evaluation patterns",
				DetailedAnalysis: map[string]interface{}{
					"history_analysis": map[string]interface{}{
						"evaluation_count":     12,
						"historical_accuracy":  0.89,
						"trend_analysis":       "deteriorating_performance",
						"pattern_consistency":  0.92,
						"previous_evaluations": []string{"satisfied", "satisfied", "not_satisfied", "satisfied"},
						"confidence_evolution": []float64{0.78, 0.81, 0.75, 0.83},
					},
					"temporal_patterns": map[string]interface{}{
						"evaluation_frequency": "every_5_minutes",
						"satisfaction_rate":    0.75,
						"trend_direction":      "increasing_latency",
					},
				},
				Metadata: map[string]interface{}{
					"history_maintained": true,
					"analysis_depth":     "comprehensive",
				},
			}

			responseJSON, _ := json.Marshal(historyAwareAnalysis)
			// Configure mock to return the AI response in the reasoning field
			recommendation := &types.ActionRecommendation{
				Action:     "mock_condition_evaluation",
				Confidence: historyAwareAnalysis.Confidence,
				Reasoning:  &types.ReasoningDetails{Summary: string(responseJSON)},
			}
			mockSLMClient.SetAnalysisResult(recommendation)

			// Act: Evaluate condition with history tracking
			// Following Guideline #1: Reuse existing EvaluateCondition method
			result, err := conditionEvaluator.EvaluateCondition(ctx, baseCondition)

			// **Business Requirement BR-COND-008**: Validate history maintenance capability
			Expect(err).ToNot(HaveOccurred(), "Should evaluate with history tracking")
			Expect(result.Metadata).To(HaveKey("detailed_analysis"),
				"BR-COND-008: Should provide detailed analysis including history")

			if result.Metadata != nil {
				if analysisRaw, exists := result.Metadata["detailed_analysis"]; exists && analysisRaw != nil {
					analysis := analysisRaw.(map[string]interface{})

					if historyRaw, exists := analysis["history_analysis"]; exists && historyRaw != nil {
						historyAnalysis := historyRaw.(map[string]interface{})

						Expect(historyAnalysis["evaluation_count"]).To(BeNumerically(">", 0),
							"BR-COND-008: Should track evaluation count")
						Expect(historyAnalysis["historical_accuracy"]).To(BeNumerically(">", 0),
							"BR-COND-008: Should maintain historical accuracy metrics")
						Expect(historyAnalysis["previous_evaluations"]).ToNot(BeEmpty(),
							"BR-COND-008: Should maintain previous evaluation results")
						Expect(historyAnalysis["confidence_evolution"]).ToNot(BeEmpty(),
							"BR-COND-008: Should track confidence evolution over time")
					}

					// Validate temporal pattern analysis
					if temporalRaw, exists := analysis["temporal_patterns"]; exists && temporalRaw != nil {
						temporalPatterns := temporalRaw.(map[string]interface{})
						Expect(temporalPatterns["satisfaction_rate"]).To(BeNumerically(">", 0),
							"BR-COND-008: Should calculate satisfaction rate from history")
						Expect(temporalPatterns["trend_direction"]).ToNot(BeEmpty(),
							"BR-COND-008: Should identify trends from historical data")
					}
				}
			}
		})
	})
})

// MockMetricsClient extends monitoring capabilities for AI condition testing
type MockMetricsClient struct {
	metrics      map[string]interface{}
	queryResults map[string]float64
	resourceData map[string]map[string]float64
	error        error
}

func NewMockMetricsClient() *MockMetricsClient {
	return &MockMetricsClient{
		metrics:      make(map[string]interface{}),
		queryResults: make(map[string]float64),
		resourceData: make(map[string]map[string]float64),
	}
}

func (m *MockMetricsClient) GetResourceMetrics(ctx context.Context, namespace, resourceName string, metricNames []string) (map[string]float64, error) {
	// Check for context cancellation in test mock
	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	default:
	}

	if m.error != nil {
		return nil, m.error
	}

	key := fmt.Sprintf("%s:%s", namespace, resourceName)
	if data, exists := m.resourceData[key]; exists {
		return data, nil
	}

	// Return default metrics
	return map[string]float64{
		"cpu_usage":     0.65,
		"memory_usage":  0.72,
		"response_time": 150.0,
	}, nil
}

func (m *MockMetricsClient) CheckMetricsImprovement(ctx context.Context, alert types.Alert, trace *actionhistory.ResourceActionTrace) (bool, error) {
	// Check for context cancellation in test mock
	select {
	case <-ctx.Done():
		return false, ctx.Err()
	default:
	}

	if m.error != nil {
		return false, m.error
	}

	// Mock implementation - simulate improvement detection
	return true, nil
}

func (m *MockMetricsClient) GetMetricsHistory(ctx context.Context, namespace, resourceName string, metricNames []string, from, to time.Time) ([]monitoring.MetricPoint, error) {
	// Check for context cancellation in test mock
	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	default:
	}

	if m.error != nil {
		return nil, m.error
	}

	// Mock implementation - return sample historical metrics
	return []monitoring.MetricPoint{
		{Timestamp: from, Value: 0.8},
		{Timestamp: to, Value: 0.6},
	}, nil
}

func (m *MockMetricsClient) SetResourceMetrics(namespace, resourceName string, metrics map[string]float64) {
	key := fmt.Sprintf("%s:%s", namespace, resourceName)
	m.resourceData[key] = metrics
}

func (m *MockMetricsClient) SetError(err error) {
	m.error = err
}

func (m *MockMetricsClient) ClearState() {
	m.metrics = make(map[string]interface{})
	m.queryResults = make(map[string]float64)
	m.resourceData = make(map[string]map[string]float64)
	m.error = nil
}

// Note: Using existing GetDeployment method from MockKubernetesClient which creates default deployments

// SLMClientAdapter adapts MockLLMClient to SLMClient interface
type SLMClientAdapter struct {
	mockClient *mocks.MockLLMClient
}

func NewSLMClientAdapter(mockClient *mocks.MockLLMClient) *SLMClientAdapter {
	return &SLMClientAdapter{mockClient: mockClient}
}

func (a *SLMClientAdapter) AnalyzeAlert(ctx context.Context, alert interface{}) (*types.ActionRecommendation, error) {
	// Check for context cancellation in test mock
	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	default:
	}

	// If the mock has a configured response, return it directly
	if a.mockClient != nil {
		// The MockLLMClient has a SetAnalysisResult method that stores ActionRecommendation
		// We need to call the mock's analysis and convert if necessary
		response, err := a.mockClient.AnalyzeAlert(ctx, alert)
		if err != nil {
			return nil, err
		}

		// Convert AnalyzeAlertResponse to ActionRecommendation
		if response != nil {
			result := &types.ActionRecommendation{
				Action:     response.Action,
				Confidence: response.Confidence,
				Reasoning:  response.Reasoning,
			}
			// Copy parameters if they exist
			if response.Parameters != nil {
				result.Parameters = response.Parameters
			}
			return result, nil
		}
	}

	return nil, fmt.Errorf("no response configured")
}

func (a *SLMClientAdapter) IsHealthy() bool {
	if a.mockClient != nil {
		return a.mockClient.IsHealthy()
	}
	return false
}

// TestRunner bootstraps the Ginkgo test suite
