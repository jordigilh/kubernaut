"""
Integration Test Infrastructure Configuration for HAPI

Business Requirement: BR-HAPI-250 - Workflow Catalog Search Tool
Design Decision: DD-TEST-001 v1.8 - Port Allocation
Pattern: DD-INTEGRATION-001 v2.0 - Go-Bootstrapped Infrastructure

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CORRECT ARCHITECTURE (December 29, 2025)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

HAPI is a Python service â†’ Tests should be in Python
Infrastructure bootstrap â†’ Uses shared Go library

Services Started By: test/infrastructure/holmesgpt_integration.go (Go)
Test Logic: holmesgpt-api/tests/integration/*.py (Python)

This conftest.py provides fixtures to discover Go-started services.
It does NOT start services itself (that's handled by Go infrastructure).

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Port Allocation (per DD-TEST-001 v1.8):
- PostgreSQL: 15439 (HAPI-specific, shared with Notification/WE)
- Redis: 16387 (HAPI-specific, shared with Notification/WE)
- Data Storage Service: 18098 (HAPI allocation)
- HolmesGPT API: 18120 (HAPI primary port)

Usage:
    def test_workflow_search(hapi_url, data_storage_url):
        # Services already running via Go infrastructure
        response = requests.post(f"{hapi_url}/api/v1/incident/analyze", ...)

Running Tests:
    # Start Go infrastructure first (in separate terminal or via script)
    cd /path/to/kubernaut
    ginkgo run ./test/integration/holmesgptapi/

    # Then run Python tests (they discover Go-started services)
    cd holmesgpt-api
    python -m pytest tests/integration/ -v

See: holmesgpt-api/tests/integration/PYTHON_TESTS_WITH_GO_INFRASTRUCTURE.md
"""

import os
import subprocess
import sys
from pathlib import Path
import pytest
import requests


# ========================================
# DD-HAPI-005: Auto-Regenerate OpenAPI Client
# ========================================
# Regenerate Python OpenAPI client before tests to prevent urllib3 version conflicts.
# This matches Go's `go generate` pattern: regenerate on-demand, never commit.
# ========================================

@pytest.fixture(scope="session")
def ensure_openapi_client(request):
    """
    Manually regenerate HAPI OpenAPI client (DD-HAPI-005).

    **NOTE**: This fixture is NOT auto-invoked. Client generation happens in:
    - Makefile: Step 1 runs `generate-client.sh` before pytest
    - Manual: Developer can call this fixture explicitly if needed

    Pattern: Same as Go's `go generate ./pkg/holmesgpt/client/`
    - Client regenerated from api/openapi.json
    - Never committed to git (in .gitignore)
    - Always compatible with current dependencies

    This prevents recurring urllib3 version conflicts.

    **Parallel Execution Safety**:
    - Makefile generates client ONCE before pytest starts
    - All workers (gw0-gw3) share the same generated client
    - No file system conflicts during parallel execution
    """
    client_path = Path(__file__).parent.parent / "clients" / "holmesgpt_api_client"

    # Check if client already exists (pre-generated by Makefile)
    if client_path.exists():
        print("\nâœ… DD-HAPI-005: OpenAPI client already generated")
        return

    # Generate client if missing (manual pytest invocation)
    print("\nðŸ”§ DD-HAPI-005: Regenerating Python OpenAPI client...")
    print(f"   Pattern: Auto-regenerate (like Go's `go generate`)")
    print(f"   Target: {client_path}")

    generate_script = Path(__file__).parent / "generate-client.sh"

    try:
        result = subprocess.run(
            [str(generate_script)],
            check=True,
            capture_output=True,
            text=True,
            timeout=120  # 2 minutes max for docker pull + generation
        )
        print(result.stdout)
        print("âœ… Client regeneration complete")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Client generation failed: {e.stderr}", file=sys.stderr)
        pytest.fail(f"DD-HAPI-005: Failed to regenerate OpenAPI client: {e.stderr}")
    except subprocess.TimeoutExpired:
        pytest.fail("DD-HAPI-005: Client generation timed out (>2 minutes)")


# ========================================
# DD-TEST-001 v1.8: Port Allocation
# ========================================
# Read from environment (set by Go infrastructure or CI)
# Defaults match test/infrastructure/holmesgpt_integration.go
DATA_STORAGE_PORT = int(os.getenv("DS_INTEGRATION_PORT", "18098"))
POSTGRES_PORT = int(os.getenv("PG_INTEGRATION_PORT", "15439"))
REDIS_PORT = int(os.getenv("REDIS_INTEGRATION_PORT", "16387"))

# Service URLs (use 127.0.0.1 to force IPv4, avoid IPv6 resolution issues in CI)
DATA_STORAGE_URL = os.getenv("DATA_STORAGE_URL", f"http://127.0.0.1:{DATA_STORAGE_PORT}")


# ========================================
# PYTEST FIXTURES
# ========================================
# Architecture Change (Jan 4, 2026):
# Integration tests call business logic DIRECTLY (no HTTP, no HAPI container)
# - Go pattern: controller.Reconcile(ctx, req)
# - Python pattern: analyze_incident(request_data)
# HTTP API testing deferred to E2E tests (future)
# ========================================

@pytest.fixture(scope="session")
def data_storage_url():
    """
    Data Storage service URL (started by Go infrastructure).

    Integration tests use this for audit event validation (external dependency).
    Tests call HAPI business logic DIRECTLY (no HTTP client needed).

    Returns:
        str: Data Storage base URL (e.g., http://127.0.0.1:18098)
    """
    return DATA_STORAGE_URL


@pytest.fixture(scope="session")
def audit_store(data_storage_url):
    """
    BufferedAuditStore instance for explicit flush() in tests.

    This fixture provides access to the HAPI audit store singleton,
    allowing tests to explicitly flush buffered events before querying
    Data Storage. This eliminates async race conditions in audit tests.

    Pattern (matching Go integration tests):
    1. Call business logic (emits audit events)
    2. audit_store.flush() - force write to Data Storage
    3. Query Data Storage API - events now visible
    4. Assert audit event content

    Returns:
        BufferedAuditStore: HAPI audit store singleton
    """
    # Import here to avoid circular dependencies
    sys.path.insert(0, str(Path(__file__).parent.parent.parent))
    from src.audit.factory import get_audit_store

    # Get singleton instance (initializes if needed)
    store = get_audit_store(data_storage_url=data_storage_url)

    print(f"\nâœ… Audit store singleton accessed for tests")
    print(f"   Data Storage URL: {data_storage_url}")
    print(f"   Flush support: Enabled (eliminates async race conditions)")

    yield store

    # Cleanup: flush and close at end of session
    # HAPI-SHUTDOWN-001: Flush audit store BEFORE infrastructure stops
    # This prevents "connection refused" errors during cleanup when the
    # background writer tries to flush buffered events after DataStorage is stopped.
    # See: docs/handoff/WE_DATASTORAGE_CONNECTION_REFUSED_TRIAGE_JAN_04_2026.md
    print(f"\nðŸ”„ Flushing audit store before infrastructure shutdown...")
    if store.flush(timeout=10.0):
        print(f"âœ… Audit store flushed (all buffered events written)")
    else:
        print(f"âš ï¸  Warning: Audit store flush timed out or failed")

    print(f"ðŸ”„ Closing audit store...")
    store.close()
    print(f"âœ… Audit store closed")


@pytest.fixture(scope="session")
def hapi_client():
    """
    FastAPI TestClient for HAPI service (in-process).

    NOTE (Jan 4, 2026): Integration tests NO LONGER use this fixture.
    Integration tests call business logic DIRECTLY (no HTTP client).
    This fixture is kept for future E2E tests or manual testing.

    Architecture Decision (Dec 29, 2025):
    - Integration tests: TestClient (in-process HAPI) âœ…
    - E2E tests: External HAPI in Kind cluster âœ…

    Benefits:
    - âœ… Faster (~3 min vs ~7 min, no Docker build)
    - âœ… Reliable audit persistence (in-process config)
    - âœ… Easier debugging (direct app access)
    - âœ… Consistent with FastAPI best practices

    Configuration:
    - config.yaml: holmesgpt-api/config.yaml
    - Data Storage: http://localhost:18098 (Go-started)
    - Mock LLM: enabled

    See: docs/shared/HAPI_INTEGRATION_TEST_ARCHITECTURE_FIX_DEC_29_2025.md
    """
    import os
    from fastapi.testclient import TestClient

    # Environment variables are set globally in pytest_configure
    # (CONFIG_FILE, MOCK_LLM_MODE, LLM_MODEL, LLM_ENDPOINT)

    # Import app (env vars already set by pytest_configure)
    from src.main import app

    client = TestClient(app)
    print(f"\nâœ… HAPI TestClient initialized (in-process)")
    print(f"   Data Storage URL: {DATA_STORAGE_URL}")
    print(f"   Config: config.yaml")
    print(f"   Mode: Mock LLM")

    return client


@pytest.fixture(scope="session")
def integration_infrastructure():
    """
    Integration infrastructure URLs (services started by Go infrastructure).

    This fixture provides service URLs for tests that need them.

    Note: HAPI integration tests use direct business logic calls (no HTTP),
    so hapi_url is not included.

    Returns:
        dict: Service URLs
            {
                "data_storage_url": "http://localhost:18098",
            }
    """
    return {
        "data_storage_url": DATA_STORAGE_URL,
    }


@pytest.fixture(scope="session", autouse=True)
def bootstrap_test_workflows(data_storage_url):
    """
    Bootstrap test workflows into Data Storage for integration tests.

    DD-TEST-011 v2.0 UPDATE (Jan 31, 2026):
    Workflow seeding now handled by Go suite setup (before Mock LLM starts).
    This fixture is kept as a no-op for backward compatibility but workflows
    are already seeded when Python tests execute.

    Pattern (matches AIAnalysis):
    1. Go: Seed workflows â†’ capture UUIDs
    2. Go: Write Mock LLM config file with UUIDs
    3. Go: Start Mock LLM with config file mounted
    4. Python: Tests run with workflows already available

    Workflows Seeded (by Go):
    - OOMKilled workflows (2)
    - CrashLoopBackOff workflows (1)
    - NodeNotReady workflows (1)
    - ImagePullBackOff workflows (1)

    Returns:
        dict: Bootstrap results (workflows already seeded by Go)
    """
    print(f"\nâœ… DD-TEST-011 v2.0: Workflows already seeded by Go suite setup")
    print(f"   Mock LLM configured with actual DataStorage UUIDs")
    print(f"   Data Storage URL: {data_storage_url}")
    
    # Return empty results (workflows already seeded)
    return {
        "created": [],
        "existing": [],
        "failed": [],
        "total": 0
    }


@pytest.fixture(scope="function")
def worker_id(request):
    """
    Get unique worker ID for parallel test execution.

    When running with pytest-xdist (-n flag), this returns a unique worker ID (gw0, gw1, etc.).
    When running sequentially, returns "master".

    This enables test isolation by creating unique test data per worker.

    Usage:
        def test_something(worker_id):
            unique_id = f"test-{worker_id}-{int(time.time())}"
            # Use unique_id for database records, API calls, etc.

    Returns:
        str: Worker ID (e.g., "gw0", "gw1", "master")
    """
    if hasattr(request.config, 'workerinput'):
        return request.config.workerinput['workerid']
    return "master"


@pytest.fixture(scope="function")
def unique_test_id(worker_id, request):
    """
    Generate a unique test ID for isolation in parallel execution.

    Format: {test_name}_{worker_id}_{timestamp}

    This ensures tests running in parallel don't conflict with each other
    when creating database records, API requests, etc.

    Usage:
        def test_workflow_search(unique_test_id):
            incident_id = f"inc-{unique_test_id}"
            # Use incident_id in requests

    Returns:
        str: Unique test identifier
    """
    import time
    test_name = request.node.name
    timestamp = int(time.time() * 1000)  # Millisecond precision
    return f"{test_name}_{worker_id}_{timestamp}"


# ========================================
# HELPER FUNCTIONS
# ========================================

def create_authenticated_datastorage_client(data_storage_url: str):
    """
    Create authenticated DataStorage API client with ServiceAccount token injection.
    
    DD-AUTH-014: All DataStorage clients MUST use ServiceAccount token authentication.
    This helper ensures consistent auth pattern across all test files.
    
    Args:
        data_storage_url: DataStorage service URL (e.g., "http://127.0.0.1:18098")
    
    Returns:
        Tuple of (ApiClient, api_instance) ready for use
        
    Example:
        api_client, search_api = create_authenticated_datastorage_client(data_storage_url)
        response = search_api.search_workflows(...)
    """
    # Import inside function to avoid module-level import errors when DS client not available
    try:
        from datastorage import Configuration, ApiClient
        from datastorage.api import WorkflowCatalogAPIApi  # Fixed typo: "apis" â†’ "api" (singular)
    except ImportError as e:
        raise ImportError(f"DataStorage client not available. Run 'make generate-datastorage-client' first: {e}")
    
    # Import pool manager for token injection
    sys.path.insert(0, str(Path(__file__).parent.parent / "src"))
    from clients.datastorage_pool_manager import get_shared_datastorage_pool_manager
    
    # Create client
    config = Configuration(host=data_storage_url)
    api_client = ApiClient(configuration=config)
    
    # DD-AUTH-014: Inject ServiceAccount token via shared pool manager
    auth_pool = get_shared_datastorage_pool_manager()
    api_client.rest_client.pool_manager = auth_pool
    
    # Return client and API instance
    search_api = WorkflowCatalogAPIApi(api_client)
    return api_client, search_api


def is_service_available(url: str, timeout: float = 2.0, max_retries: int = 5, retry_delay: float = 1.0) -> bool:
    """
    Check if a service is available at the given URL with retries.

    Args:
        url: Service URL to check (e.g., http://localhost:18120/health)
        timeout: Request timeout in seconds per attempt
        max_retries: Maximum number of retry attempts
        retry_delay: Delay in seconds between retries

    Returns:
        bool: True if service responds with 200 OK, False otherwise
    """
    import time

    for attempt in range(max_retries):
        try:
            response = requests.get(url, timeout=timeout)
            if response.status_code == 200:
                return True
        except (requests.RequestException, ConnectionError):
            pass

        if attempt < max_retries - 1:
            time.sleep(retry_delay)

    return False


def is_integration_infra_available() -> bool:
    """
    Check if HAPI integration infrastructure is available.

    Note: HAPI integration tests use direct business logic calls (no HTTP),
    so we only check Data Storage availability.

    Returns:
        bool: True if all required services are available
    """
    ds_available = is_service_available(f"{DATA_STORAGE_URL}/health")
    return ds_available


# ========================================
# PYTEST MARKERS AND GLOBAL CONFIGURATION
# ========================================

def pytest_configure(config):
    """
    Register custom pytest markers and set global environment variables.

    CRITICAL: Set LLM configuration env vars here (before test collection)
    to ensure they're available in all pytest workers (gw0-gw3).
    """
    import os

    # Set LLM configuration for all integration tests
    # These must be set BEFORE any test modules import src.main
    os.environ["LLM_MODEL"] = "gpt-4-turbo"
    # DD-TEST-001 v2.5: Mock LLM on port 18140 (HAPI integration tests)
    os.environ["LLM_ENDPOINT"] = "http://127.0.0.1:18140"
    os.environ["MOCK_LLM_MODE"] = "true"
    os.environ["CONFIG_FILE"] = "config.yaml"
    os.environ["OPENAI_API_KEY"] = "test-api-key-for-integration-tests"

    config.addinivalue_line(
        "markers",
        "requires_data_storage: mark test as requiring Data Storage service"
    )
    config.addinivalue_line(
        "markers",
        "requires_hapi: mark test as requiring HAPI service"
    )


def pytest_collection_modifyitems(config, items):
    """
    Auto-skip tests if required infrastructure is not available.

    This is a safety mechanism. In practice, tests should fail (not skip)
    if infrastructure is missing, per TESTING_GUIDELINES.md.

    Note: HAPI runs in-process with tests (FastAPI TestClient), so we only
    check DataStorage availability for @pytest.mark.requires_data_storage tests.
    """
    if config.getoption("--collect-only"):
        return

    # Check infrastructure availability based on test requirements
    ds_available = is_service_available(f"{DATA_STORAGE_URL}/health")
    # NOTE: HAPI tests use direct business logic calls (no HTTP service), so no availability check needed

    for item in items:
        # Skip tests that require DataStorage if it's not available
        if "requires_data_storage" in item.keywords and not ds_available:
            skip_ds = pytest.mark.skip(reason="Data Storage not available (start via Go: ginkgo run ./test/integration/holmesgptapi/)")
            item.add_marker(skip_ds)

        # NOTE: "requires_hapi" marker no longer needed - HAPI tests use direct business logic calls (in-process)


def pytest_collection_modifyitems(config, items):
    """
    Auto-skip tests if required infrastructure is not available.

    This is a safety mechanism. In practice, tests should fail (not skip)
    if infrastructure is missing, per TESTING_GUIDELINES.md.

    Note: HAPI runs in-process with tests (FastAPI TestClient), so we only
    check DataStorage availability for @pytest.mark.requires_data_storage tests.
    """
    if config.getoption("--collect-only"):
        return

    # Check infrastructure availability based on test requirements
    ds_available = is_service_available(f"{DATA_STORAGE_URL}/health")
    # NOTE: HAPI tests use direct business logic calls (no HTTP service), so no availability check needed

    for item in items:
        # Skip tests that require DataStorage if it's not available
        if "requires_data_storage" in item.keywords and not ds_available:
            skip_ds = pytest.mark.skip(reason="Data Storage not available (start via Go: ginkgo run ./test/integration/holmesgptapi/)")
            item.add_marker(skip_ds)

        # NOTE: "requires_hapi" marker no longer needed - HAPI tests use direct business logic calls (in-process)
