# Gateway Chaos Testing Scenarios

**Test Tier**: Chaos/E2E Testing
**Purpose**: Validate Gateway resilience under infrastructure failures
**Status**: ‚è≥ **PENDING IMPLEMENTATION** (Infrastructure Not Yet Built)
**Created**: 2025-10-27

---

## üéØ **Purpose**

Chaos tests validate that the Gateway service remains resilient and maintains data consistency when infrastructure components fail unexpectedly. These tests simulate real-world failure scenarios that cannot be easily replicated in integration tests.

---

## üö® **Why Chaos Testing?**

### **Production Risks Mitigated**

1. **Network Failures**: Transient network issues during Redis/K8s operations
2. **Partial Failures**: Mid-operation failures that could corrupt state
3. **Cascading Failures**: One component failure triggering others
4. **Resource Exhaustion**: Infrastructure running out of resources under load
5. **Timing Issues**: Race conditions exposed only under failure scenarios

### **Business Value**

- ‚úÖ **Prevent Data Loss**: Ensure no CRDs are lost during infrastructure failures
- ‚úÖ **Maintain Consistency**: Redis and K8s state remain consistent despite failures
- ‚úÖ **Graceful Degradation**: System fails safely without corrupting data
- ‚úÖ **Fast Recovery**: System recovers quickly after infrastructure is restored

---

## üìã **Chaos Test Scenarios**

### **Scenario 1: Redis Pipeline Command Failures** ‚è≥ **PENDING**

**Status**: Moved from integration tier (2025-10-27)
**Original Location**: `test/integration/gateway/redis_integration_test.go:307`
**New Location**: `test/e2e/gateway/chaos/redis_failure_test.go`

#### **Business Requirement**

BR-GATEWAY-008: Redis operations must handle partial failures without corrupting state

#### **Failure Scenario**

Redis pipeline commands fail mid-batch due to network issues, causing some commands to succeed and others to fail.

#### **Test Steps**

1. **Setup**: Start Gateway with Redis connection
2. **Act 1**: Send 20 alerts (batch 1) ‚Üí All succeed
3. **Inject Failure**: Simulate Redis pipeline failure (network partition, connection drop)
4. **Act 2**: Send 20 more alerts (batch 2) ‚Üí Some fail
5. **Verify**: State remains consistent (no partial writes, no corruption)

#### **Expected Outcomes**

- ‚úÖ **Batch 1**: All 20 alerts processed successfully
- ‚úÖ **Batch 2**: Requests fail gracefully (503 Service Unavailable)
- ‚úÖ **State Consistency**: Redis fingerprint count matches K8s CRD count
- ‚úÖ **No Corruption**: No partial writes or orphaned data

#### **Production Risk**

**High** - Network issues during batch operations are common in distributed systems

#### **Implementation Requirements**

1. **Failure Injection**: Mechanism to simulate Redis pipeline failures
2. **Network Chaos**: Tools to inject network partitions (e.g., Toxiproxy, Chaos Mesh)
3. **State Verification**: Comprehensive checks for data consistency
4. **Recovery Testing**: Verify system recovers after failure is resolved

#### **Estimated Effort**

2-3 hours (includes chaos infrastructure setup)

---

### **Scenario 2: Redis Connection Failure During Processing** ‚è≥ **PENDING**

**Status**: New scenario (identified 2025-10-27)
**Location**: `test/e2e/gateway/chaos/redis_failure_test.go`

#### **Business Requirement**

BR-GATEWAY-008: Gateway must fail fast when Redis is unavailable

#### **Failure Scenario**

Redis connection drops mid-request while Gateway is processing a webhook.

#### **Test Steps**

1. **Setup**: Start Gateway with Redis connection
2. **Act 1**: Send alert ‚Üí Processing starts
3. **Inject Failure**: Drop Redis connection mid-processing
4. **Act 2**: Complete request processing
5. **Verify**: Request fails gracefully (503), no partial state

#### **Expected Outcomes**

- ‚úÖ **Request Failure**: Returns 503 Service Unavailable
- ‚úÖ **No Partial State**: No CRD created if Redis write fails
- ‚úÖ **Fast Failure**: Request fails within timeout (5 seconds)
- ‚úÖ **Error Logging**: Failure logged with context

#### **Production Risk**

**Medium** - Redis connection drops are less common but high impact

#### **Implementation Requirements**

1. **Connection Chaos**: Mechanism to drop Redis connections mid-request
2. **Timing Control**: Inject failure at specific points in request lifecycle
3. **Atomicity Verification**: Ensure no partial state on failure

#### **Estimated Effort**

1-2 hours

---

### **Scenario 3: K8s API Failure During CRD Creation** ‚è≥ **PENDING**

**Status**: New scenario (identified 2025-10-27)
**Location**: `test/e2e/gateway/chaos/k8s_failure_test.go`

#### **Business Requirement**

BR-GATEWAY-010: Gateway must handle K8s API failures gracefully

#### **Failure Scenario**

K8s API becomes unavailable while Gateway is creating a RemediationRequest CRD.

#### **Test Steps**

1. **Setup**: Start Gateway with K8s API connection
2. **Act 1**: Send alert ‚Üí CRD creation starts
3. **Inject Failure**: Make K8s API unavailable (network partition, API server down)
4. **Act 2**: Complete CRD creation attempt
5. **Verify**: Request fails gracefully, Redis state cleaned up

#### **Expected Outcomes**

- ‚úÖ **Request Failure**: Returns 503 Service Unavailable
- ‚úÖ **Redis Cleanup**: Fingerprint removed from Redis (rollback)
- ‚úÖ **Retry Logic**: Client can retry request after K8s API recovers
- ‚úÖ **No Orphaned Data**: No fingerprints without corresponding CRDs

#### **Production Risk**

**Medium** - K8s API throttling/unavailability happens during cluster upgrades

#### **Implementation Requirements**

1. **K8s API Chaos**: Mechanism to make K8s API unavailable
2. **Rollback Testing**: Verify Redis state is cleaned up on K8s failure
3. **Retry Testing**: Verify requests can be retried after recovery

#### **Estimated Effort**

2-3 hours

---

### **Scenario 4: Cascading Failures (Redis + K8s API)** ‚è≥ **PENDING**

**Status**: New scenario (identified 2025-10-27)
**Location**: `test/e2e/gateway/chaos/cascading_failure_test.go`

#### **Business Requirement**

BR-GATEWAY-008, BR-GATEWAY-010: Gateway must handle multiple simultaneous failures

#### **Failure Scenario**

Both Redis and K8s API become unavailable simultaneously.

#### **Test Steps**

1. **Setup**: Start Gateway with Redis and K8s API connections
2. **Act 1**: Send alert ‚Üí Processing starts
3. **Inject Failure**: Make both Redis and K8s API unavailable
4. **Act 2**: Complete request processing
5. **Verify**: Request fails gracefully, no partial state

#### **Expected Outcomes**

- ‚úÖ **Request Failure**: Returns 503 Service Unavailable
- ‚úÖ **Fast Failure**: Fails within timeout (5 seconds)
- ‚úÖ **No Partial State**: No writes to either Redis or K8s
- ‚úÖ **Recovery**: System recovers when dependencies are restored

#### **Production Risk**

**Low** - Simultaneous failures are rare but catastrophic

#### **Implementation Requirements**

1. **Multi-Component Chaos**: Inject failures in multiple components simultaneously
2. **Dependency Ordering**: Test different failure orders (Redis first, K8s first, simultaneous)
3. **Recovery Testing**: Verify system recovers when dependencies are restored

#### **Estimated Effort**

2-3 hours

---

### **Scenario 5: Network Latency Injection** ‚è≥ **PENDING**

**Status**: New scenario (identified 2025-10-27)
**Location**: `test/e2e/gateway/chaos/latency_test.go`

#### **Business Requirement**

BR-GATEWAY-011: Gateway must handle slow dependencies gracefully

#### **Failure Scenario**

Redis or K8s API responses are delayed significantly (e.g., 10+ seconds).

#### **Test Steps**

1. **Setup**: Start Gateway with Redis and K8s API connections
2. **Act 1**: Send alert ‚Üí Processing starts
3. **Inject Latency**: Add 10-second delay to Redis/K8s API responses
4. **Act 2**: Complete request processing
5. **Verify**: Request times out gracefully, no hanging requests

#### **Expected Outcomes**

- ‚úÖ **Timeout Handling**: Request times out within configured limit (15 seconds)
- ‚úÖ **No Hanging Requests**: Request doesn't hang indefinitely
- ‚úÖ **Error Response**: Returns 504 Gateway Timeout
- ‚úÖ **Resource Cleanup**: No goroutine leaks or connection leaks

#### **Production Risk**

**Medium** - Network latency spikes happen during infrastructure issues

#### **Implementation Requirements**

1. **Latency Injection**: Mechanism to add delays to Redis/K8s API calls
2. **Timeout Testing**: Verify timeouts are enforced correctly
3. **Resource Monitoring**: Verify no leaks during timeout scenarios

#### **Estimated Effort**

1-2 hours

---

### **Scenario 6: Redis Memory Exhaustion (OOM)** ‚è≥ **PENDING**

**Status**: New scenario (identified 2025-10-27)
**Location**: `test/e2e/gateway/chaos/redis_oom_test.go`

#### **Business Requirement**

BR-GATEWAY-008: Gateway must handle Redis OOM errors gracefully

#### **Failure Scenario**

Redis runs out of memory and starts rejecting commands with OOM errors.

#### **Test Steps**

1. **Setup**: Start Gateway with Redis (limited memory)
2. **Act 1**: Send alerts until Redis memory is full
3. **Inject Failure**: Redis returns "OOM command not allowed"
4. **Act 2**: Send more alerts
5. **Verify**: Requests fail gracefully, no data corruption

#### **Expected Outcomes**

- ‚úÖ **Request Failure**: Returns 503 Service Unavailable
- ‚úÖ **Error Logging**: OOM error logged with context
- ‚úÖ **No Corruption**: Existing data remains consistent
- ‚úÖ **Recovery**: System recovers when Redis memory is freed

#### **Production Risk**

**Medium** - Redis OOM happens during alert storms or memory leaks

#### **Implementation Requirements**

1. **Memory Limit**: Configure Redis with limited memory
2. **OOM Injection**: Fill Redis memory to trigger OOM errors
3. **Recovery Testing**: Verify system recovers after memory is freed

#### **Estimated Effort**

1-2 hours

---

## üõ†Ô∏è **Chaos Engineering Infrastructure**

### **Required Tools**

#### **Option 1: Toxiproxy** (Recommended for v1.0)

**Pros**:
- ‚úÖ Simple to set up and use
- ‚úÖ Supports network failures, latency, timeouts
- ‚úÖ HTTP API for programmatic control
- ‚úÖ Works with any TCP service (Redis, K8s API)

**Cons**:
- ‚ö†Ô∏è Requires proxy setup (adds complexity)
- ‚ö†Ô∏è Limited to network-level failures

**Estimated Setup Time**: 2-3 hours

---

#### **Option 2: Chaos Mesh** (Recommended for v2.0+)

**Pros**:
- ‚úÖ Kubernetes-native chaos engineering
- ‚úÖ Supports pod failures, network chaos, stress testing
- ‚úÖ Declarative YAML configuration
- ‚úÖ Built-in observability and monitoring

**Cons**:
- ‚ö†Ô∏è Requires Kubernetes cluster
- ‚ö†Ô∏è More complex setup
- ‚ö†Ô∏è Heavier weight

**Estimated Setup Time**: 4-6 hours

---

#### **Option 3: Manual Failure Injection** (Quick Start)

**Pros**:
- ‚úÖ No additional tools required
- ‚úÖ Simple to implement
- ‚úÖ Good for initial testing

**Cons**:
- ‚ö†Ô∏è Limited failure scenarios
- ‚ö†Ô∏è Not production-realistic
- ‚ö†Ô∏è Manual test execution

**Estimated Setup Time**: 1-2 hours

---

### **Recommended Approach**

**Phase 1 (v1.0)**: Manual failure injection for basic scenarios
**Phase 2 (v1.5)**: Toxiproxy for network-level failures
**Phase 3 (v2.0)**: Chaos Mesh for comprehensive chaos testing

---

## üìä **Test Coverage**

| Scenario | Priority | Effort | Status |
|----------|----------|--------|--------|
| **Redis Pipeline Failures** | High | 2-3h | ‚è≥ Pending |
| **Redis Connection Failure** | High | 1-2h | ‚è≥ Pending |
| **K8s API Failure** | Medium | 2-3h | ‚è≥ Pending |
| **Cascading Failures** | Low | 2-3h | ‚è≥ Pending |
| **Network Latency** | Medium | 1-2h | ‚è≥ Pending |
| **Redis OOM** | Medium | 1-2h | ‚è≥ Pending |
| **Total** | - | 10-16h | ‚è≥ Pending |

---

## üéØ **Success Criteria**

### **Functional**

- ‚úÖ All chaos scenarios pass (100% pass rate)
- ‚úÖ No data loss during failures
- ‚úÖ State consistency maintained (Redis ‚Üî K8s)
- ‚úÖ Graceful degradation (503 errors, not panics)

### **Non-Functional**

- ‚úÖ Fast failure (<5 seconds timeout)
- ‚úÖ No resource leaks (goroutines, connections)
- ‚úÖ Comprehensive error logging
- ‚úÖ Recovery within 30 seconds after failure resolved

---

## üìù **Implementation Plan**

### **Phase 1: Infrastructure Setup** (4-6 hours)

1. Choose chaos engineering tool (Toxiproxy recommended)
2. Set up chaos testing environment
3. Create failure injection mechanisms
4. Implement state verification helpers

### **Phase 2: Scenario Implementation** (10-16 hours)

1. Implement Redis pipeline failures test (2-3h)
2. Implement Redis connection failure test (1-2h)
3. Implement K8s API failure test (2-3h)
4. Implement cascading failures test (2-3h)
5. Implement network latency test (1-2h)
6. Implement Redis OOM test (1-2h)

### **Phase 3: Documentation & CI/CD** (2-3 hours)

1. Document chaos test execution
2. Create chaos test scripts
3. Integrate with CI/CD pipeline (optional)

**Total Estimated Effort**: 16-25 hours

---

## üîó **Related Documentation**

- **Integration Tests**: `test/integration/gateway/README.md`
- **Load Tests**: `test/load/gateway/README.md`
- **Test Tier Classification**: `test/integration/gateway/TEST_TIER_CLASSIFICATION_ASSESSMENT.md`
- **Testing Strategy**: `.cursor/rules/03-testing-strategy.mdc`

---

## üìã **Next Steps**

### **When Ready to Implement**

1. **Review this document** and prioritize scenarios
2. **Choose chaos engineering tool** (Toxiproxy recommended)
3. **Set up chaos testing environment**
4. **Implement high-priority scenarios** (Redis pipeline failures, Redis connection failure)
5. **Integrate with CI/CD pipeline**

### **Prerequisites**

- ‚úÖ Integration tests passing (100% pass rate)
- ‚úÖ Load tests implemented and passing
- ‚úÖ Dedicated chaos testing environment available
- ‚úÖ Monitoring and observability in place

---

**Status**: ‚è≥ **PENDING IMPLEMENTATION**
**Priority**: **Medium** (after integration and load tests are stable)
**Next Review**: When ready to start chaos testing implementation


