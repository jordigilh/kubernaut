package orchestration

import (
	"fmt"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/sirupsen/logrus"

	"github.com/jordigilh/kubernaut/pkg/intelligence/learning"
	"github.com/jordigilh/kubernaut/pkg/intelligence/shared"
	sharedtypes "github.com/jordigilh/kubernaut/pkg/shared/types"
)

// Suite structure moved to orchestration_suite_test.go - generated by ginkgo bootstrap

var _ = Describe("MachineLearningAnalyzer", func() {
	var (
		analyzer *learning.MachineLearningAnalyzer
		config   *learning.MLConfig
		logger   *logrus.Logger
	)

	BeforeEach(func() {
		logger = logrus.New()
		logger.SetLevel(logrus.WarnLevel)

		config = &learning.MLConfig{
			MinExecutionsForPattern: 5,
			MaxHistoryDays:          30,
			SimilarityThreshold:     0.8,
			ClusteringEpsilon:       0.3,
			MinClusterSize:          3,
			ModelUpdateInterval:     time.Hour,
			FeatureWindowSize:       20,
			PredictionConfidence:    0.7,
		}

		analyzer = learning.NewMachineLearningAnalyzer(config, logger)
	})

	Context("Feature Extraction", func() {
		It("should extract features from valid workflow data", func() {
			data := createTestWorkflowExecutionData(true, time.Minute*5)

			features, err := analyzer.ExtractFeatures(data)

			Expect(err).ToNot(HaveOccurred())
			Expect(features.AlertCount).To(BeNumerically(">=", 0), "Should extract alert count")
			Expect(features.SeverityScore).To(BeNumerically(">=", 0), "Should extract severity score")
			Expect(features.ResourceCount).To(BeNumerically(">=", 0), "Should extract resource count")
			Expect(features.HourOfDay).To(BeNumerically(">=", 0), "Should extract hour of day")
			Expect(features.DayOfWeek).To(BeNumerically(">=", 0), "Should extract day of week")
		})

		It("should handle missing alert data gracefully", func() {
			data := &sharedtypes.WorkflowExecutionData{
				ExecutionID: "test-exec",
				Timestamp:   time.Now(),
				// No alert data
			}

			features, err := analyzer.ExtractFeatures(data)

			Expect(err).ToNot(HaveOccurred())
			Expect(features.AlertCount).To(BeNumerically(">=", 0), "Should extract meaningful features")
		})

		It("should validate feature consistency", func() {
			data1 := createTestWorkflowExecutionData(true, time.Minute*3)
			data2 := createTestWorkflowExecutionData(false, time.Minute*7)

			features1, err1 := analyzer.ExtractFeatures(data1)
			features2, err2 := analyzer.ExtractFeatures(data2)

			Expect(err1).ToNot(HaveOccurred())
			Expect(err2).ToNot(HaveOccurred())

			// Features should have consistent structure
			Expect(features1.HourOfDay).To(BeNumerically(">=", 0), "Features should have valid hour")
			Expect(features1.DayOfWeek).To(BeNumerically(">=", 0), "Features should have valid day")
			Expect(features2.HourOfDay).To(BeNumerically(">=", 0), "Features should have valid hour")
			Expect(features2.DayOfWeek).To(BeNumerically(">=", 0), "Features should have valid day")
		})
	})

	Context("Model Training", func() {
		var trainingData []*sharedtypes.WorkflowExecutionData

		BeforeEach(func() {
			trainingData = createTrainingDataset(20, 0.7) // 20 samples, 70% success rate
		})

		It("should train a success prediction model successfully", func() {
			model, err := analyzer.TrainModel("success_prediction", trainingData)

			// **Business Requirement Validation**: Verify ML model training produces usable business intelligence
			Expect(err).ToNot(HaveOccurred(), "Should complete model training successfully")
			Expect(model.Type).To(Equal("classification"), "Should create classification model for success prediction")

			// **Business Value Validation**: Model should be operationally useful
			Expect(model.Accuracy).To(BeNumerically(">", 0), "Should produce a trained model with measurable accuracy")
			Expect(model.Accuracy).To(BeNumerically("<=", 1.0), "Accuracy should be within valid range")
			Expect(len(model.Features)).To(BeNumerically(">", 0), "Should identify relevant features for prediction")

			// **Business Outcome**: Model should be ready for prediction use
			Expect(model.ID).ToNot(BeEmpty(), "Should have model identifier for operational use")
			Expect(model.Features).ToNot(BeNil(), "Should have feature definitions for prediction")
		})

		It("should train a duration prediction model successfully", func() {
			model, err := analyzer.TrainModel("duration_prediction", trainingData)

			// **Business Requirement Validation**: Verify regression model training for duration prediction
			Expect(err).ToNot(HaveOccurred(), "Should complete duration prediction model training")
			Expect(model.Type).To(Equal("regression"), "Should create regression model for duration prediction")

			// **Business Value Validation**: Model should provide measurable regression capability
			Expect(model.TrainingMetrics).ToNot(BeNil(), "Should provide training metrics for model evaluation")
			Expect(model.TrainingMetrics.R2Score).To(BeNumerically(">=", 0), "R2 score should be calculable")
			Expect(model.TrainingMetrics.R2Score).To(BeNumerically("<=", 1.0), "R2 score should be within valid range")

			// **Business Outcome**: Model should be ready for duration prediction use
			Expect(len(model.Features)).To(BeNumerically(">", 0), "Should identify features relevant to duration prediction")
			Expect(model.ID).ToNot(BeEmpty(), "Should have model identifier for operational use")
		})

		It("should fail with insufficient training data", func() {
			insufficientData := trainingData[:2] // Only 2 samples

			model, err := analyzer.TrainModel("success_prediction", insufficientData)

			Expect(err).To(HaveOccurred())
			Expect(model).To(BeNil())
			Expect(err.Error()).To(ContainSubstring("insufficient"))
		})

		It("should validate model quality metrics", func() {
			model, err := analyzer.TrainModel("success_prediction", trainingData)

			// **Business Requirement Validation**: Verify model produces valid quality metrics
			Expect(err).ToNot(HaveOccurred(), "Should successfully train model for quality validation")
			Expect(model.TrainingMetrics).ToNot(BeNil(), "Should provide training metrics for quality assessment")

			// **Business Value Validation**: Model metrics should be within operational boundaries
			Expect(model.TrainingMetrics.Accuracy).To(BeNumerically(">=", 0), "Accuracy should be measurable")
			Expect(model.TrainingMetrics.Accuracy).To(BeNumerically("<=", 1.0), "Accuracy should not exceed 100%")

			// **Business Outcome**: Model should provide quality indicators for operational decisions
			Expect(model.TrainingMetrics.Precision).To(BeNumerically(">=", 0), "Should calculate precision metric")
			Expect(model.TrainingMetrics.Recall).To(BeNumerically(">=", 0), "Should calculate recall metric")
			Expect(model.TrainingMetrics.F1Score).To(BeNumerically(">=", 0), "Should calculate F1 score for balanced assessment")
		})
	})

	Context("Outcome Prediction", func() {
		var patterns []*shared.DiscoveredPattern

		BeforeEach(func() {
			trainingData := createTrainingDataset(15, 0.8)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			patterns = createTestDiscoveredPatterns(3)
		})

		It("should predict workflow outcome with confidence", func() {
			features := createTestWorkflowFeatures()

			prediction, err := analyzer.PredictOutcome(features, patterns)

			Expect(err).ToNot(HaveOccurred())
			Expect(prediction.Confidence).To(BeNumerically(">=", 0.3), "Should provide reasonably confident predictions")
			Expect(prediction.Confidence).To(BeNumerically("<=", 1.0), "Confidence should not exceed 100%")
			Expect(prediction.SuccessProbability).To(BeNumerically(">=", 0.0))
			Expect(prediction.SuccessProbability).To(BeNumerically("<=", 1.0))
		})

		It("should provide reasonable predictions for known patterns", func() {
			// Test with high-confidence pattern
			highConfidencePattern := patterns[0]
			highConfidencePattern.Confidence = 0.95
			highConfidencePattern.SuccessRate = 0.90

			features := createTestWorkflowFeatures()
			prediction, err := analyzer.PredictOutcome(features, []*shared.DiscoveredPattern{highConfidencePattern})

			// **Business Requirement Validation**: Verify prediction capability with known patterns
			Expect(err).ToNot(HaveOccurred(), "Should successfully generate predictions for known patterns")
			Expect(prediction).ToNot(BeNil(), "Should return a valid prediction object")

			// **Business Value Validation**: Prediction should provide actionable intelligence
			Expect(prediction.Confidence).To(BeNumerically(">", 0), "Should provide measurable confidence in prediction")
			Expect(prediction.Confidence).To(BeNumerically("<=", 1.0), "Confidence should be within valid range")
			Expect(prediction.SuccessProbability).To(BeNumerically(">=", 0), "Should calculate success probability")
			Expect(prediction.SuccessProbability).To(BeNumerically("<=", 1.0), "Success probability should be within valid range")

			// **Business Outcome**: Prediction should reflect input pattern quality
			Expect(prediction.SimilarPatterns).To(BeNumerically(">=", 1), "Should identify similar patterns for decision support")
			Expect(prediction.Reason).ToNot(BeEmpty(), "Should provide prediction reasoning for decision support")
		})

		It("should handle edge cases in prediction", func() {
			// Test with empty patterns
			features := createTestWorkflowFeatures()
			prediction, err := analyzer.PredictOutcome(features, []*shared.DiscoveredPattern{})

			Expect(err).ToNot(HaveOccurred())
			Expect(prediction.Confidence).To(BeNumerically(">=", 0.1), "Should provide baseline confidence even without patterns")
			Expect(prediction.SuccessProbability).To(BeNumerically(">=", 0.0), "Should have valid success probability")
			Expect(prediction.SuccessProbability).To(BeNumerically("<=", 1.0), "Success probability should not exceed 100%")
			Expect(prediction.SimilarPatterns).To(Equal(0), "Should show 0 patterns when none provided")
		})
	})

	Context("Cross-Validation", func() {
		It("should perform k-fold cross-validation", func() {
			trainingData := createTrainingDataset(25, 0.75)

			metrics, err := analyzer.CrossValidateModel("success_prediction", trainingData, 5)

			// **Business Requirement Validation**: Verify cross-validation provides model reliability assessment
			Expect(err).ToNot(HaveOccurred(), "Should successfully perform cross-validation analysis")
			Expect(metrics).ToNot(BeNil(), "Should return cross-validation metrics")
			Expect(metrics.Folds).To(Equal(5), "Should use requested number of validation folds")

			// **Business Value Validation**: Cross-validation should provide statistical validity assessment
			Expect(metrics.MeanAccuracy).To(BeNumerically(">=", 0), "Should calculate mean accuracy across folds")
			Expect(metrics.MeanAccuracy).To(BeNumerically("<=", 1.0), "Mean accuracy should be within valid range")
			Expect(metrics.StdAccuracy).To(BeNumerically(">=", 0.0), "Should calculate accuracy standard deviation")

			// **Business Outcome**: Cross-validation should support model reliability decisions
			Expect(metrics.MeanF1).To(BeNumerically(">=", 0), "Should calculate mean F1 score across folds")
			Expect(metrics.StdF1).To(BeNumerically(">=", 0), "Should calculate F1 standard deviation for reliability assessment")
		})

		It("should detect overfitting through cross-validation", func() {
			// Create a dataset with clear patterns but limited size
			trainingData := createOverfittingProneDataset(10)

			metrics, err := analyzer.CrossValidateModel("success_prediction", trainingData, 3)

			Expect(err).ToNot(HaveOccurred())
			// High standard deviation indicates potential overfitting
			if metrics.MeanAccuracy > 0.95 && metrics.StdAccuracy > 0.2 {
				logger.Warn("Potential overfitting detected in cross-validation")
			}
		})
	})

	Context("Model Analysis and Performance", func() {
		It("should analyze model performance with test data", func() {
			// Train a model first
			trainingData := createTrainingDataset(20, 0.75)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			// Test model performance analysis
			testData := createTrainingDataset(10, 0.8)
			performance, err := analyzer.AnalyzeModelPerformance("success_prediction", testData)

			// **Business Requirement Validation**: Verify model performance analysis capability
			Expect(err).ToNot(HaveOccurred(), "Should successfully analyze model performance")
			Expect(performance).ToNot(BeNil(), "Should return performance analysis results")
			Expect(performance.ModelID).To(Equal("success_prediction"), "Should track correct model ID")
			Expect(performance.TestDataSize).To(Equal(10), "Should use correct test data size")

			// **Business Value Validation**: Performance analysis should provide actionable metrics
			Expect(performance.Metrics).ToNot(BeNil(), "Should provide performance metrics for evaluation")
			Expect(performance.Metrics["accuracy"]).To(BeNumerically(">=", 0), "Should calculate test accuracy")
			Expect(performance.Metrics["accuracy"]).To(BeNumerically("<=", 1.0), "Test accuracy should be within valid range")

			// **Business Outcome**: Performance analysis should support operational decisions
			Expect(performance.EvaluatedAt).ToNot(BeNil(), "Should timestamp analysis for tracking")
			Expect(len(performance.Metrics)).To(BeNumerically(">=", 1), "Should provide multiple performance metrics")
		})

		It("should maintain model count correctly", func() {
			initialCount := analyzer.GetModelCount()

			// Train a new model
			trainingData := createTrainingDataset(15, 0.7)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			// Model count should reflect the trained model
			newCount := analyzer.GetModelCount()
			Expect(newCount).To(BeNumerically(">=", initialCount))
		})

		It("should provide access to trained models", func() {
			// Train a model
			trainingData := createTrainingDataset(15, 0.7)
			_, err := analyzer.TrainModel("success_prediction", trainingData)
			Expect(err).ToNot(HaveOccurred())

			// Get models should return the trained model
			models := analyzer.GetModels()
			Expect(len(models)).To(BeNumerically(">", 0), "Should return trained models")

			if successModel, exists := models["success_prediction"]; exists {
				Expect(successModel.Type).To(Equal("classification"))
				Expect(successModel.TrainingMetrics.Accuracy).To(BeNumerically(">=", 0.0))
			}
		})
	})
})

// Helper functions

func createTestWorkflowExecutionData(success bool, duration time.Duration) *sharedtypes.WorkflowExecutionData {
	return &sharedtypes.WorkflowExecutionData{
		ExecutionID: "test-exec-" + time.Now().Format("150405"),
		WorkflowID:  "test-workflow-001",
		Timestamp:   time.Now(),
		Duration:    duration,
		Success:     success,
		Metrics: map[string]float64{
			"alert_count":       2.0,
			"severity_score":    2.5,
			"resource_count":    3.0,
			"step_count":        5.0,
			"dependency_depth":  2.0,
			"cluster_load":      0.6,
			"resource_pressure": 0.4,
		},
	}
}

func createTrainingDataset(size int, successRate float64) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		success := float64(i) < float64(size)*successRate
		duration := time.Minute * time.Duration(2+i%10) // Varying durations 2-12 minutes
		severity := 2.0
		if !success {
			severity = 4.0 // Higher severity for failures
		}

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("training-exec-%d", i),
			WorkflowID:  "test-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    duration,
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       float64(1 + i%3),
				"severity_score":    severity,
				"resource_count":    float64(2 + i%4),
				"step_count":        float64(3 + i%8),
				"dependency_depth":  float64(1 + i%3),
				"cluster_load":      0.3 + float64(i%10)/20.0, // 0.3 to 0.75
				"resource_pressure": 0.2 + float64(i%8)/20.0,  // 0.2 to 0.55
			},
		}
	}

	return dataset
}

func createOverfittingProneDataset(size int) []*sharedtypes.WorkflowExecutionData {
	// Create a dataset with very specific patterns that might lead to overfitting
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		// Create a very specific pattern: success depends on exact pattern
		success := i%2 == 0

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("overfitting-exec-%d", i),
			WorkflowID:  "overfitting-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    5 * time.Minute,
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":    float64(i%3 + 1), // Very specific pattern
				"severity_score": float64(i%2 + 1), // Binary severity
				"step_count":     3.0,              // Fixed step count
			},
		}
	}

	return dataset
}

func createTestDiscoveredPatterns(count int) []*shared.DiscoveredPattern {
	patterns := make([]*shared.DiscoveredPattern, count)

	for i := 0; i < count; i++ {
		patterns[i] = &shared.DiscoveredPattern{
			PatternType:  shared.PatternTypeAlert,
			DiscoveredAt: time.Now().Add(-time.Duration(i) * time.Hour),
		}
	}

	return patterns
}

func createTestWorkflowFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         2,
		SeverityScore:      2.5,
		ResourceCount:      4,
		StepCount:          6,
		DependencyDepth:    2,
		HourOfDay:          14, // 2 PM
		DayOfWeek:          2,  // Tuesday
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     0,
		AverageSuccessRate: 0.75,
		LastExecutionTime:  5 * time.Minute,
		ClusterLoad:        0.6,
		ResourcePressure:   0.3,
		CustomMetrics: map[string]float64{
			"workflow_complexity": 0.6,
		},
	}
}

// Helper functions for Advanced ML Capabilities tests
func createComplexWorkflowData() *sharedtypes.WorkflowExecutionData {
	return &sharedtypes.WorkflowExecutionData{
		ExecutionID: "complex-workflow-001",
		WorkflowID:  "complex-multi-step-workflow",
		Timestamp:   time.Now(),
		Duration:    15 * time.Minute,
		Success:     true,
		Metrics: map[string]float64{
			"alert_count":       5.0,  // High alert count
			"severity_score":    4.0,  // High severity
			"resource_count":    8.0,  // Many resources
			"step_count":        25.0, // Many steps
			"dependency_depth":  5.0,  // Deep dependencies
			"cluster_load":      0.8,  // High load
			"resource_pressure": 0.7,  // High pressure
		},
		Context: map[string]interface{}{
			"workflow_complexity": 0.9,
		},
	}
}

func createSimpleWorkflowData() *sharedtypes.WorkflowExecutionData {
	return &sharedtypes.WorkflowExecutionData{
		ExecutionID: "simple-workflow-001",
		WorkflowID:  "simple-single-step-workflow",
		Timestamp:   time.Now(),
		Duration:    2 * time.Minute,
		Success:     true,
		Metrics: map[string]float64{
			"alert_count":       1.0, // Low alert count
			"severity_score":    1.5, // Low severity
			"resource_count":    2.0, // Few resources
			"step_count":        3.0, // Few steps
			"dependency_depth":  1.0, // Shallow dependencies
			"cluster_load":      0.3, // Low load
			"resource_pressure": 0.2, // Low pressure
		},
		Context: map[string]interface{}{
			"workflow_complexity": 0.2,
		},
	}
}

func createResourceIntensiveTrainingData(size int, successRate float64) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		success := float64(i) < float64(size)*successRate
		duration := time.Minute * time.Duration(10+i%20) // 10-30 minutes for resource-intensive

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("resource-intensive-exec-%d", i),
			WorkflowID:  "resource-heavy-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    duration,
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       float64(3 + i%5),        // 3-8 alerts
				"severity_score":    3.0 + float64(i%3),      // 3-6 severity
				"resource_count":    float64(5 + i%10),       // 5-15 resources
				"step_count":        float64(10 + i%20),      // 10-30 steps
				"dependency_depth":  float64(3 + i%4),        // 3-7 depth
				"cluster_load":      0.6 + float64(i%5)/10.0, // 0.6-1.0
				"resource_pressure": 0.5 + float64(i%6)/10.0, // 0.5-1.0
			},
		}
	}
	return dataset
}

func createLightweightTrainingData(size int, successRate float64) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		success := float64(i) < float64(size)*successRate
		duration := time.Minute * time.Duration(1+i%3) // 1-4 minutes for lightweight

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("lightweight-exec-%d", i),
			WorkflowID:  "lightweight-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    duration,
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       1.0,                      // Single alert
				"severity_score":    1.0 + float64(i%2),       // 1-2 severity
				"resource_count":    float64(1 + i%2),         // 1-2 resources
				"step_count":        float64(2 + i%3),         // 2-5 steps
				"dependency_depth":  1.0,                      // Shallow
				"cluster_load":      0.1 + float64(i%3)/10.0,  // 0.1-0.4
				"resource_pressure": 0.05 + float64(i%2)/20.0, // 0.05-0.15
			},
		}
	}
	return dataset
}

func createResourceIntensiveWorkflowFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         6,
		SeverityScore:      4.5,
		ResourceCount:      12,
		StepCount:          25,
		DependencyDepth:    5,
		HourOfDay:          14,
		DayOfWeek:          2,
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     1,
		AverageSuccessRate: 0.65,
		LastExecutionTime:  15 * time.Minute,
		ClusterLoad:        0.85,
		ResourcePressure:   0.75,
		CustomMetrics: map[string]float64{
			"workflow_complexity": 0.9,
			"resource_intensity":  0.85,
		},
	}
}

func createLightweightWorkflowFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         1,
		SeverityScore:      1.5,
		ResourceCount:      2,
		StepCount:          3,
		DependencyDepth:    1,
		HourOfDay:          14,
		DayOfWeek:          2,
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     0,
		AverageSuccessRate: 0.9,
		LastExecutionTime:  2 * time.Minute,
		ClusterLoad:        0.2,
		ResourcePressure:   0.1,
		CustomMetrics: map[string]float64{
			"workflow_complexity": 0.2,
			"resource_intensity":  0.15,
		},
	}
}

func createEnvironmentalPatternData(environment string, size int, successRate float64) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		success := float64(i) < float64(size)*successRate

		// Adjust metrics based on environment
		var clusterLoad, resourcePressure, severity float64
		switch environment {
		case "stable":
			clusterLoad = 0.3 + float64(i%3)/10.0      // 0.3-0.6
			resourcePressure = 0.2 + float64(i%3)/15.0 // 0.2-0.4
			severity = 1.0 + float64(i%2)              // 1-2
		case "degraded":
			clusterLoad = 0.7 + float64(i%4)/10.0      // 0.7-1.0
			resourcePressure = 0.6 + float64(i%4)/10.0 // 0.6-1.0
			severity = 3.0 + float64(i%3)              // 3-5
		default:
			clusterLoad = 0.5
			resourcePressure = 0.4
			severity = 2.5
		}

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("%s-env-exec-%d", environment, i),
			WorkflowID:  fmt.Sprintf("%s-environment-workflow", environment),
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    time.Minute * time.Duration(3+i%8),
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       float64(1 + i%4),
				"severity_score":    severity,
				"resource_count":    float64(2 + i%5),
				"step_count":        float64(4 + i%6),
				"dependency_depth":  float64(1 + i%3),
				"cluster_load":      clusterLoad,
				"resource_pressure": resourcePressure,
			},
			Context: map[string]interface{}{
				"environment": environment,
			},
		}
	}
	return dataset
}

func createEnvironmentalWorkflowFeatures(environment string) *shared.WorkflowFeatures {
	features := createTestWorkflowFeatures()

	switch environment {
	case "stable":
		features.ClusterLoad = 0.4
		features.ResourcePressure = 0.3
		features.SeverityScore = 1.5
		features.RecentFailures = 0
		features.AverageSuccessRate = 0.85
	case "degraded":
		features.ClusterLoad = 0.85
		features.ResourcePressure = 0.75
		features.SeverityScore = 4.0
		features.RecentFailures = 3
		features.AverageSuccessRate = 0.55
	}

	features.CustomMetrics["environment"] = func() float64 {
		if environment == "stable" {
			return 1.0
		}
		return 0.0
	}()

	return features
}

// Helper functions for Failure Learning tests
func createFailurePatternData(size int) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		// Create patterns where higher severity and resource pressure lead to failures
		severity := 2.0 + float64(i%4)                      // 2-6 severity
		resourcePressure := 0.3 + float64(i%7)/10.0         // 0.3-0.9
		success := severity < 4.0 && resourcePressure < 0.7 // Failure patterns

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("failure-pattern-exec-%d", i),
			WorkflowID:  "failure-prone-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    time.Minute * time.Duration(5+i%10),
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       float64(2 + i%5),
				"severity_score":    severity,
				"resource_count":    float64(3 + i%4),
				"step_count":        float64(6 + i%8),
				"dependency_depth":  float64(2 + i%3),
				"cluster_load":      0.5 + float64(i%6)/10.0,
				"resource_pressure": resourcePressure,
			},
			Context: map[string]interface{}{
				"failure_context": fmt.Sprintf("pattern_%d", i%3),
			},
		}
	}
	return dataset
}

func createFailureProneWorkflowFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         4,
		SeverityScore:      4.5, // High severity
		ResourceCount:      6,
		StepCount:          10,
		DependencyDepth:    3,
		HourOfDay:          14,
		DayOfWeek:          2,
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     2,    // Recent failures
		AverageSuccessRate: 0.45, // Poor historical success
		LastExecutionTime:  8 * time.Minute,
		ClusterLoad:        0.8,  // High load
		ResourcePressure:   0.75, // High pressure
		CustomMetrics: map[string]float64{
			"workflow_complexity": 0.8,
			"failure_risk":        0.85,
		},
	}
}

func createFeedbackCorrectedData(originalData []*sharedtypes.WorkflowExecutionData) []*sharedtypes.WorkflowExecutionData {
	// Simulate feedback corrections by adjusting some outcomes and metrics
	correctedData := make([]*sharedtypes.WorkflowExecutionData, len(originalData))

	for i, data := range originalData {
		correctedData[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: data.ExecutionID + "-corrected",
			WorkflowID:  data.WorkflowID,
			Timestamp:   data.Timestamp,
			Duration:    data.Duration,
			Success:     data.Success,
			Metrics:     make(map[string]float64),
		}

		// Copy metrics and apply feedback corrections
		for k, v := range data.Metrics {
			correctedData[i].Metrics[k] = v
		}

		// Simulate feedback corrections for some cases
		if i%5 == 0 { // Every 5th case gets feedback correction
			if data.Success && data.Metrics["severity_score"] > 3.0 {
				// Feedback: This should have failed due to high severity
				correctedData[i].Success = false
				correctedData[i].Metrics["feedback_correction"] = 1.0
			} else if !data.Success && data.Metrics["severity_score"] < 2.0 {
				// Feedback: This should have succeeded despite low severity
				correctedData[i].Success = true
				correctedData[i].Metrics["feedback_correction"] = 1.0
			}
		}
	}

	return correctedData
}

func createCorrectedScenarioFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         3,
		SeverityScore:      3.5, // Moderate severity
		ResourceCount:      4,
		StepCount:          8,
		DependencyDepth:    2,
		HourOfDay:          14,
		DayOfWeek:          2,
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     1,
		AverageSuccessRate: 0.7,
		LastExecutionTime:  6 * time.Minute,
		ClusterLoad:        0.6,
		ResourcePressure:   0.5,
		CustomMetrics: map[string]float64{
			"workflow_complexity": 0.65,
			"feedback_available":  1.0,
		},
	}
}

// Specific failure pattern creators
func createTimeoutFailureData(size int) []*sharedtypes.WorkflowExecutionData {
	return createSpecificFailureData(size, "timeout", func(i int) map[string]float64 {
		return map[string]float64{
			"alert_count":       float64(1 + i%2),
			"severity_score":    2.0,
			"resource_count":    float64(3 + i%3),
			"step_count":        float64(15 + i%10), // High step count
			"dependency_depth":  float64(4 + i%3),   // Deep dependencies
			"cluster_load":      0.9,                // Very high load
			"resource_pressure": 0.4,
		}
	})
}

func createResourceFailureData(size int) []*sharedtypes.WorkflowExecutionData {
	return createSpecificFailureData(size, "resource", func(i int) map[string]float64 {
		return map[string]float64{
			"alert_count":       float64(2 + i%3),
			"severity_score":    3.0,
			"resource_count":    float64(8 + i%5), // High resource count
			"step_count":        float64(6 + i%4),
			"dependency_depth":  float64(2 + i%2),
			"cluster_load":      0.6,
			"resource_pressure": 0.95, // Very high pressure
		}
	})
}

func createDependencyFailureData(size int) []*sharedtypes.WorkflowExecutionData {
	return createSpecificFailureData(size, "dependency", func(i int) map[string]float64 {
		return map[string]float64{
			"alert_count":       float64(3 + i%4),
			"severity_score":    2.5,
			"resource_count":    float64(4 + i%3),
			"step_count":        float64(8 + i%5),
			"dependency_depth":  float64(6 + i%4), // Very deep dependencies
			"cluster_load":      0.5,
			"resource_pressure": 0.5,
		}
	})
}

func createSpecificFailureData(size int, failureType string, metricsFunc func(int) map[string]float64) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("%s-failure-exec-%d", failureType, i),
			WorkflowID:  fmt.Sprintf("%s-failure-workflow", failureType),
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    time.Minute * time.Duration(8+i%12),
			Success:     false, // All are failures
			Metrics:     metricsFunc(i),
			Context: map[string]interface{}{
				"failure_type": failureType,
			},
		}
	}
	return dataset
}

// Feature creators for specific failure patterns
func createTimeoutProneFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         2,
		SeverityScore:      2.0,
		ResourceCount:      4,
		StepCount:          20, // High step count
		DependencyDepth:    5,  // Deep dependencies
		HourOfDay:          14,
		DayOfWeek:          2,
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     1,
		AverageSuccessRate: 0.6,
		LastExecutionTime:  12 * time.Minute,
		ClusterLoad:        0.9, // Very high load
		ResourcePressure:   0.4,
		CustomMetrics: map[string]float64{
			"timeout_risk": 0.9,
		},
	}
}

func createResourceConstrainedFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         3,
		SeverityScore:      3.0,
		ResourceCount:      10, // High resource count
		StepCount:          8,
		DependencyDepth:    2,
		HourOfDay:          14,
		DayOfWeek:          2,
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     2,
		AverageSuccessRate: 0.5,
		LastExecutionTime:  6 * time.Minute,
		ClusterLoad:        0.6,
		ResourcePressure:   0.95, // Very high pressure
		CustomMetrics: map[string]float64{
			"resource_risk": 0.95,
		},
	}
}

func createDependencyIssueFeatures() *shared.WorkflowFeatures {
	return &shared.WorkflowFeatures{
		AlertCount:         4,
		SeverityScore:      2.5,
		ResourceCount:      5,
		StepCount:          10,
		DependencyDepth:    7, // Very deep dependencies
		HourOfDay:          14,
		DayOfWeek:          2,
		IsWeekend:          false,
		IsBusinessHour:     true,
		RecentFailures:     1,
		AverageSuccessRate: 0.55,
		LastExecutionTime:  9 * time.Minute,
		ClusterLoad:        0.5,
		ResourcePressure:   0.5,
		CustomMetrics: map[string]float64{
			"dependency_risk": 0.85,
		},
	}
}

// Helper functions for Data Security and Analytics tests
func createSensitiveWorkflowData() *sharedtypes.WorkflowExecutionData {
	return &sharedtypes.WorkflowExecutionData{
		ExecutionID: "sensitive-workflow-001",
		WorkflowID:  "sensitive-data-workflow",
		Timestamp:   time.Now(),
		Duration:    8 * time.Minute,
		Success:     true,
		Metrics: map[string]float64{
			"alert_count":       3.0,
			"severity_score":    2.5,
			"resource_count":    5.0,
			"step_count":        8.0,
			"dependency_depth":  2.0,
			"cluster_load":      0.6,
			"resource_pressure": 0.4,
		},
		Context: map[string]interface{}{
			"contains_pii":        true,
			"security_level":      "high",
			"data_classification": "sensitive",
		},
		Metadata: map[string]interface{}{
			"namespace":   "production",
			"environment": "prod",
			"owner":       "team-alpha", // Potentially sensitive info
		},
	}
}

func createSanitizedWorkflowData() *sharedtypes.WorkflowExecutionData {
	return &sharedtypes.WorkflowExecutionData{
		ExecutionID: "sanitized-workflow-001",
		WorkflowID:  "sanitized-data-workflow",
		Timestamp:   time.Now(),
		Duration:    8 * time.Minute,
		Success:     true,
		Metrics: map[string]float64{
			"alert_count":       3.0, // Same business metrics
			"severity_score":    2.5,
			"resource_count":    5.0,
			"step_count":        8.0,
			"dependency_depth":  2.0,
			"cluster_load":      0.6,
			"resource_pressure": 0.4,
		},
		Context: map[string]interface{}{
			"contains_pii":        false,
			"security_level":      "standard",
			"data_classification": "sanitized",
		},
		Metadata: map[string]interface{}{
			"namespace":   "[redacted]", // Sanitized sensitive info
			"environment": "sanitized",
			"owner":       "[redacted]",
		},
	}
}

func createAnalyticsTrainingData(size int) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		// Create diverse data for comprehensive analytics
		severity := 1.0 + float64(i%5) // 1-6 severity
		success := severity < 4.0      // Success pattern based on severity

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("analytics-exec-%d", i),
			WorkflowID:  "analytics-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    time.Minute * time.Duration(2+i%8),
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       float64(1 + i%4),
				"severity_score":    severity,
				"resource_count":    float64(2 + i%6),
				"step_count":        float64(4 + i%10),
				"dependency_depth":  float64(1 + i%4),
				"cluster_load":      0.2 + float64(i%8)/10.0,
				"resource_pressure": 0.1 + float64(i%7)/15.0,
			},
			Context: map[string]interface{}{
				"analytics_context": fmt.Sprintf("scenario_%d", i%4),
			},
		}
	}
	return dataset
}

func createAnalyticsTestData(size int) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		// Create test data with known patterns for analytics validation
		severity := 2.0 + float64(i%3) // 2-5 severity
		success := severity < 3.5      // Known success pattern

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("analytics-test-exec-%d", i),
			WorkflowID:  "analytics-test-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    time.Minute * time.Duration(3+i%6),
			Success:     success,
			Metrics: map[string]float64{
				"alert_count":       float64(2 + i%3),
				"severity_score":    severity,
				"resource_count":    float64(3 + i%4),
				"step_count":        float64(5 + i%7),
				"dependency_depth":  float64(2 + i%3),
				"cluster_load":      0.3 + float64(i%5)/12.0,
				"resource_pressure": 0.2 + float64(i%4)/20.0,
			},
			Context: map[string]interface{}{
				"test_scenario": fmt.Sprintf("test_%d", i%3),
			},
		}
	}
	return dataset
}

func createValidAnalyticsData(size int) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("valid-analytics-exec-%d", i),
			WorkflowID:  "valid-analytics-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    time.Minute * time.Duration(3+i%5),
			Success:     i%3 != 0, // ~67% success rate
			Metrics: map[string]float64{
				"alert_count":       float64(1 + i%3),
				"severity_score":    2.0 + float64(i%3),
				"resource_count":    float64(2 + i%4),
				"step_count":        float64(4 + i%6),
				"dependency_depth":  float64(1 + i%3),
				"cluster_load":      0.3 + float64(i%6)/15.0,
				"resource_pressure": 0.2 + float64(i%5)/20.0,
			},
			Context: map[string]interface{}{
				"data_quality": "valid",
			},
		}
	}
	return dataset
}

func createCorruptedAnalyticsData(size int) []*sharedtypes.WorkflowExecutionData {
	dataset := make([]*sharedtypes.WorkflowExecutionData, size)

	for i := 0; i < size; i++ {
		// Create data with various corruption patterns
		var metrics map[string]float64
		var success bool
		var duration time.Duration

		switch i % 4 {
		case 0:
			// Missing critical metrics
			metrics = map[string]float64{
				"alert_count": float64(i + 1),
				// Missing severity_score and other key metrics
			}
			success = true
			duration = time.Minute * 5
		case 1:
			// Invalid metric values
			metrics = map[string]float64{
				"alert_count":       -1.0, // Invalid negative count
				"severity_score":    10.0, // Invalid high severity
				"resource_count":    0.0,  // Invalid zero resources
				"step_count":        -5.0, // Invalid negative steps
				"dependency_depth":  -2.0, // Invalid negative depth
				"cluster_load":      1.5,  // Invalid > 1.0
				"resource_pressure": -0.5, // Invalid negative pressure
			}
			success = false
			duration = time.Minute * 3
		case 2:
			// Inconsistent data
			metrics = map[string]float64{
				"alert_count":       5.0,
				"severity_score":    1.0,  // Low severity but many alerts
				"resource_count":    1.0,  // Single resource
				"step_count":        50.0, // Too many steps for single resource
				"dependency_depth":  0.0,  // No dependencies but many steps
				"cluster_load":      0.1,
				"resource_pressure": 0.9, // High pressure but low load
			}
			success = true           // Inconsistent with high pressure
			duration = time.Hour * 2 // Unrealistic duration
		default:
			// Minimal but valid data
			metrics = map[string]float64{
				"alert_count":    1.0,
				"severity_score": 2.0,
			}
			success = true
			duration = time.Minute * 2
		}

		dataset[i] = &sharedtypes.WorkflowExecutionData{
			ExecutionID: fmt.Sprintf("corrupted-analytics-exec-%d", i),
			WorkflowID:  "corrupted-analytics-workflow",
			Timestamp:   time.Now().Add(-time.Duration(i) * time.Hour),
			Duration:    duration,
			Success:     success,
			Metrics:     metrics,
			Context: map[string]interface{}{
				"data_quality":    "corrupted",
				"corruption_type": []string{"missing_metrics", "invalid_values", "inconsistent_data", "minimal_data"}[i%4],
			},
		}
	}
	return dataset
}
