# Configuration for local LLM instance at 192.168.1.169:8080
# This configuration enables AI features using your LocalAI/Compatible instance

app:
  name: "prometheus-alerts-slm"
  version: "1.0.0"

server:
  webhook_port: "8080"
  metrics_port: "9090"
  health_port: "8081"

logging:
  level: "info"
  format: "json"

# AI Services Configuration - Direct HolmesGPT Integration (Phase 2)
ai_services:
  # General-purpose LLM for workflow generation
  llm:
    endpoint: "http://192.168.1.169:8080"
    provider: "localai"              # LocalAI compatible provider
    model: "gpt-oss:20b"             # Default model
    api_key: ""                      # Leave empty if no API key required
    timeout: 30s
    retry_count: 3
    temperature: 0.3                 # Conservative for reliable decisions
    max_tokens: 500                  # Reasonable response size
    max_context_size: 2000           # Good balance for decision quality

  # HolmesGPT for specialized alert investigation
  holmesgpt:
    enabled: true                    # Enable HolmesGPT integration
    mode: "development"              # Development vs production mode
    endpoint: "http://localhost:8090" # Local container endpoint
    timeout: 60s                     # Longer timeout for investigations
    retry_count: 3
    toolsets:                        # Available investigation toolsets
      - "kubernetes"
      - "prometheus"
      - "internet"
    priority: 100                    # Higher priority than LLM for investigations

  # Context API for HolmesGPT dynamic context orchestration (Phase 2)
  context_api:
    enabled: true                    # Enable Context API server
    host: "0.0.0.0"                  # Listen on all interfaces
    port: 8091                       # Context API port (avoids conflicts with 8080=main, 8090=HolmesGPT)
    timeout: 30s                     # Context gathering timeout

# Backward compatibility - will be migrated to ai_services.llm automatically
slm:
  endpoint: "http://192.168.1.169:8080"
  provider: "localai"
  model: "gpt-oss:20b"
  timeout: 30s
  retry_count: 3
  temperature: 0.3
  max_tokens: 500
  max_context_size: 2000

kubernetes:
  context: ""                      # Use default context
  namespace: "default"
  service_account: "prometheus-alerts-slm"

actions:
  dry_run: false                   # Set to true for testing
  max_concurrent: 5
  cooldown_period: "5m"

# Database configuration (optional for testing)
database:
  enabled: true                    # Enable for AI learning features
  host: "localhost"
  port: "5432"
  database: "action_history"
  username: "slm_user"
  password: "slm_password"
  ssl_mode: "disable"
  max_open_conns: 10
  max_idle_conns: 5
  conn_max_lifetime_minutes: 5

# Vector Database - Using PostgreSQL with pgvector (âœ… FULLY IMPLEMENTED)
vectordb:
  enabled: true
  backend: "postgresql"            # Use PostgreSQL with pgvector extension
  postgresql:
    use_main_db: true              # Use same DB as action history
    host: "localhost"              # Only needed if use_main_db: false
    port: "5432"
    database: "action_history"     # Same database as main application
    username: "slm_user"
    password: "slm_password"
    index_lists: 100               # IVFFlat index parameter for up to 100K vectors
  embedding:
    provider: "local"              # Use local embedding generation
    model: "all-MiniLM-L6-v2"     # 384-dimensional embeddings (matches migration)
    dimensions: 384

monitoring:
  use_production_clients: true     # Enable real monitoring integration
  prometheus_endpoint: "http://localhost:9090"
  alertmanager_endpoint: "http://localhost:9093"

webhook:
  port: "8080"
  path: "/alerts"
  auth:
    type: "bearer"
    token: ""                      # Set via environment variable
