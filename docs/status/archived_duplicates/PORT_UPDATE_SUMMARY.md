# HolmesGPT Port Update Summary - 8080 â†’ 8090

**Date**: January 2025
**Reason**: Avoid port conflict with ramalama default port 8080

---

## âœ… **Updated References - HolmesGPT Service Port 8080 â†’ 8090**

### **Configuration Files**
- âœ… `config/local-llm.yaml` - holmesgpt endpoint
- âœ… `config/production-holmesgpt.yaml` - holmesgpt endpoint
- âœ… `deploy/holmesgpt-e2e-values.yaml` - service port, target port, API port, environment variables

### **Scripts**
- âœ… `scripts/test-holmesgpt-integration.sh` - HOLMES_URL and health check
- âœ… `scripts/deploy-holmesgpt-e2e.sh` - service definition, port forward, health checks
- âœ… `scripts/run-holmesgpt-local.sh` - API port, service URLs, health checks

### **Documentation**
- âœ… `docs/development/HOLMESGPT_QUICKSTART.md` - URLs and port forwards
- âœ… `docs/development/HOLMESGPT_DIRECT_INTEGRATION.md` - endpoints and examples
- âœ… `README.md` - service URLs and metrics
- âœ… `PHASE_2_MIGRATION_SUMMARY.md` - configuration example

---

## âœ… **Preserved References - LLM Service Port (Stays 8080)**

### **LLM Endpoints (Correctly Unchanged)**
- âœ… `192.168.1.169:8080` - ramalama/LocalAI service endpoint
- âœ… `http://192.168.1.169:8080/v1` - LLM API endpoints
- âœ… Network policy egress port 8080 for LLM connectivity

### **Kubernaut Service (Correctly Unchanged)**
- âœ… `webhook_port: "8080"` - Go service webhook port
- âœ… `server.webhook_port` - Main application ports

---

## ðŸŽ¯ **Port Allocation Summary**

| Service | Port | Purpose |
|---------|------|---------|
| **Ramalama/LocalAI** | 8080 | LLM inference service |
| **Kubernaut Go Service** | 8080 | Webhook receiver, health endpoints |
| **HolmesGPT** | **8090** | AI investigation service (**Updated**) |
| **Prometheus Metrics** | 9090 | Metrics collection |
| **Health Check** | 8081 | Health monitoring |

---

## ðŸ§ª **Updated Testing Commands**

### **Local Development**
```bash
# Start HolmesGPT (now on port 8090)
./scripts/run-holmesgpt-local.sh

# Test HolmesGPT health
curl http://localhost:8090/health

# Test HolmesGPT API docs
curl http://localhost:8090/docs

# Test LLM (still on port 8080)
curl http://192.168.1.169:8080/v1/models
```

### **Kubernetes Deployment**
```bash
# Deploy HolmesGPT
./scripts/deploy-holmesgpt-e2e.sh

# Port forward (updated port)
kubectl port-forward -n kubernaut-system svc/holmesgpt-e2e 8090:8090

# Test in cluster
kubectl run debug --image=curlimages/curl -it --rm -- \
  curl http://holmesgpt-e2e.kubernaut-system.svc.cluster.local:8090/health
```

---

## âœ… **Configuration Examples Updated**

### **Development Config**
```yaml
ai_services:
  holmesgpt:
    enabled: true
    endpoint: "http://localhost:8090"  # Updated
    mode: "development"
```

### **Production Config**
```yaml
ai_services:
  holmesgpt:
    enabled: true
    endpoint: "http://holmesgpt-e2e.kubernaut-system.svc.cluster.local:8090"  # Updated
    mode: "production"
```

---

**ðŸŽ‰ Port conflict resolved! HolmesGPT now runs on 8090, ramalama stays on 8080.**
