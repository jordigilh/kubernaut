# HAPI E2E Complete Triage - All 14 Failures Analyzed
**Date**: January 29, 2026  
**Test Run**: 26/40 passing (65%) | 14 failures  
**Must-Gather**: `/tmp/holmesgpt-api-e2e-logs-20260202-192052/`

---

## üéØ EXECUTIVE SUMMARY

**ROOT CAUSES**:
1. **Mock LLM Not Returning Structured Responses** (11 failures)
2. **OpenAPI Client Bug** (1 failure) - ‚úÖ **FIXED**
3. **Expected V1.1 Debt** (3 failures)

**CRITICAL INSIGHT**: The primary issue is that Mock LLM is returning responses, but HAPI's result parser **cannot extract structured fields** because Mock LLM's response format doesn't match what the parser expects.

---

## üìã CATEGORY A: Mock LLM Response Structure Issues (11 failures)

### **A1. Parser Cannot Extract `recovery_analysis` Field**

**Affected**: E2E-HAPI-023, 024

**Evidence** (test-recovery-023):
```json
{
  "event": "recovery_analysis_constructed",
  "incident_id": "test-recovery-023",
  "reason": "LLM response did not include recovery_analysis field"
}
```

**LLM Response Structure**:
```
# root_cause_analysis
{'summary': 'Problem self-resolved...', 'signal_type': 'MOCK_PROBLEM_RESOLVED', ...}

# recovery_analysis
{'previous_attempt_assessment': {...}, ...}
```

**Problem**: Mock LLM returns Python dict format with `# section_headers`, but HAPI's parser expects either:
- JSON codeblock format: ` ```json\n{...}\n``` `
- Or properly structured section headers that the parser can extract

**Result**:
- Parser falls back to constructing default `recovery_analysis`
- Default logic doesn't include `investigation_outcome="resolved"` check
- `can_recover` and `needs_human_review` use default logic

**Fix Needed**: Update Mock LLM to return properly formatted responses OR update HAPI parser to handle Python dict format

---

### **A2. LLM Returning Non-Existent Workflows**

**Affected**: E2E-HAPI-003, 004, 005, 013, 014, 026, 027

**Evidence** (test-happy-004):
```json
{
  "event": "workflow_validation_exhausted",
  "incident_id": "test-happy-004",
  "signal_type": "OOMKilled",
  "total_attempts": 3,
  "all_errors": [["Workflow 'wait-for-heal-v1' not found in catalog"]],
  "needs_human_review": true
}
```

**Expected**: Mock LLM should return `oomkill-increase-memory-v1` for `OOMKilled` signals  
**Actual**: Returns `wait-for-heal-v1` (wrong workflow, doesn't exist in test catalog)

**Analysis**:
1. HAPI calls LLM with `signal_type="OOMKilled"`
2. LLM returns `wait-for-heal-v1` (3 times across retries)
3. HAPI validates workflow against DataStorage catalog
4. Workflow not found ‚Üí retry with error feedback
5. After 3 retries ‚Üí `needs_human_review=true`, `human_review_reason=workflow_not_found`

**Root Cause**: Either:
- Mock LLM scenario detection failing (not matching `OOMKilled` to `oomkilled` scenario)
- Mock LLM returning hardcoded wrong workflow ID
- HAPI not configured to use Mock LLM (using real LLM instead)

**Investigation Needed**:
```bash
# Check if HAPI is using Mock LLM
kubectl get configmap holmesgpt-api-config -n holmesgpt-api-e2e -o yaml | grep LLM

# Check Mock LLM logs for requests
grep "OOMKilled\|test-happy-004" /tmp/.../mock-llm/0.log
```

---

### **A3. Missing Alternative Workflows**

**Affected**: E2E-HAPI-002

**Evidence**:
```json
{
  "event": "incident_analysis_completed",
  "incident_id": "test-edge-002",
  "signal_type": "MOCK_LOW_CONFIDENCE",
  "has_workflow": true,
  "needs_human_review": false
}
```

**Failure**: `alternative_workflows` array is empty

**Test Expectation**:
```go
Expect(incidentResp.AlternativeWorkflows).ToNot(BeEmpty(),
    "alternative_workflows help AIAnalysis when confidence is low")
```

**Root Cause**: Mock LLM's `low_confidence` scenario doesn't populate `alternative_workflows` in response

**Fix**: Update Mock LLM `server.py` to add alternatives for `low_confidence` scenario (see Phase 2.3 in fix plan)

---

## üìã CATEGORY B: OpenAPI Client Bug (1 failure) - ‚úÖ FIXED

### **B1. `.Set=true` for `null` Values**

**Affected**: E2E-HAPI-001

**Evidence**:
- HAPI response: `{"selected_workflow": null}` ‚úÖ Correct
- Client state: `incidentResp.SelectedWorkflow.Set = true` ‚ùå Wrong

**Root Cause**: `ogen` client sets `.Set=true` even for JSON `null` values

**Fix Applied**:
```go
// OLD (test/e2e/holmesgpt-api/incident_analysis_test.go line 78)
Expect(incidentResp.SelectedWorkflow.Set).To(BeFalse())

// NEW (‚úÖ FIXED)
Expect(incidentResp.SelectedWorkflow.Value).To(BeNil())
```

**Status**: ‚úÖ **FIXED** - Compiles successfully

---

## üìã CATEGORY C: Expected V1.1 Debt (3 failures)

**Affected**: E2E-HAPI-007, 008, 018

**Status**: Server-side input validation missing (accepted technical debt per user agreement)

**No action required** - defer to V1.1

---

## üî¨ DETAILED FAILURE ANALYSIS

### **E2E-HAPI-001**: No workflow found returns human review ‚úÖ FIXED
- **Root Cause**: OpenAPI client `.Set` bug
- **Fix**: Changed to `.Value == nil` check
- **Status**: Ready for testing

---

### **E2E-HAPI-002**: Low confidence returns alternatives
- **Root Cause**: Mock LLM doesn't return `alternative_workflows`
- **HAPI Log**: `has_workflow=true`, `needs_human_review=false` ‚úÖ BR compliant
- **Missing**: `alternative_workflows` array
- **Fix**: Update Mock LLM `low_confidence` scenario

---

### **E2E-HAPI-003**: Max retries exhausted
- **Root Cause**: Mock LLM returns `wait-for-heal-v1` (wrong workflow)
- **Expected**: Should trigger LLM parsing error after max retries
- **Actual**: Returns non-existent workflow ‚Üí validation failure ‚Üí `human_review_reason=workflow_not_found`
- **Test Expectation**: `human_review_reason=llm_parsing_error`
- **Issue**: Test expects parsing error, but Mock LLM is returning valid format with wrong workflow

---

### **E2E-HAPI-004**: Normal incident analysis
- **Root Cause**: Mock LLM returns `wait-for-heal-v1` for `OOMKilled` signal
- **Expected Workflow**: `oomkill-increase-memory-v1`
- **Actual Workflow**: `wait-for-heal-v1` (doesn't exist)
- **Retries**: 3 attempts, all return same wrong workflow
- **Result**: `needs_human_review=true`, `human_review_reason=workflow_not_found`

**Critical**: This is a **happy path test** that should pass with high confidence. Mock LLM scenario detection is broken.

---

### **E2E-HAPI-005**: Response structure validation
- **Same Root Cause**: Mock LLM wrong workflow (E2E-HAPI-004)
- **Expected**: Confident response with workflow
- **Actual**: Human review required due to workflow validation failure

---

### **E2E-HAPI-007, 008, 018**: Server validation
- **Status**: Expected failures (V1.1 debt)
- **No action**: Defer

---

### **E2E-HAPI-013**: Recovery analysis (unknown)
- **Needs Investigation**: Likely same Mock LLM wrong workflow issue
- **Test ID**: `test-recovery-013`
- **Log**: `'confidence': 0.7` - suggests workflow found
- **Status**: Need to check failure message

---

### **E2E-HAPI-014**: Recovery analysis (unknown)
- **Needs Investigation**: Likely same pattern as E2E-HAPI-013
- **Log**: `'confidence': 0.7` - suggests workflow found

---

### **E2E-HAPI-023**: Signal not reproducible
- **Root Cause**: Parser cannot extract `recovery_analysis` field
- **Signal Type**: `MOCK_PROBLEM_RESOLVED`
- **Parser Log**: `"LLM response did not include recovery_analysis field"`
- **Result**: Default logic used ‚Üí `can_recover=true` (wrong)
- **Expected**: `can_recover=false`, `needs_human_review=false`

**My Fix Status**: Code changes applied to `result_parser.py`, but parser isn't reaching that code because it can't extract the structured response

---

### **E2E-HAPI-024**: No recovery workflow found
- **Root Cause**: Same as E2E-HAPI-023 - parser issue
- **Signal Type**: `MOCK_NO_WORKFLOW_FOUND`
- **Parser Log**: `"LLM response did not include recovery_analysis field"`
- **Result**: `strategy_count=0`, `confidence=0.0`
- **Test Failure**: Type mismatch (might be Gomega display issue, not actual bug)

---

### **E2E-HAPI-026**: Normal recovery analysis
- **Root Cause**: Confidence value mismatch
- **Expected**: `0.85` (Mock LLM)
- **Actual**: `0.7` (DataStorage workflow confidence)
- **Analysis**: HAPI may prefer DataStorage confidence over LLM recommendation

---

### **E2E-HAPI-027**: Recovery response structure
- **Needs Investigation**: Likely confidence mismatch (same as E2E-HAPI-026)

---

## üîç KEY INSIGHTS

### **Insight #1: Mock LLM Response Format Problem**

Mock LLM returns Python dict format:
```
# root_cause_analysis
{'summary': '...', 'signal_type': '...'}

# recovery_analysis
{'previous_attempt_assessment': {...}}
```

HAPI parser expects either:
1. JSON codeblock: ` ```json\n{...}\n``` `
2. Properly structured sections it can parse

**Result**: Parser falls back to default logic, bypassing custom scenarios

---

### **Insight #2: Mock LLM Scenario Detection Likely Broken**

Evidence:
- `OOMKilled` signal ‚Üí Returns `wait-for-heal-v1` (should be `oomkill-increase-memory-v1`)
- Happens consistently across 3 retries
- Affects multiple tests (003, 004, 005, 013, 014, 026, 027)

**Hypothesis**: Mock LLM scenario matching logic is case-sensitive or pattern-based and not matching properly

---

### **Insight #3: BR-HAPI-197 Fixes Are Working**

HAPI correctly:
- Does NOT enforce confidence thresholds
- Sets `needs_human_review=false` for low confidence (E2E-HAPI-002)
- Uses validation failure logic for `needs_human_review=true` (E2E-HAPI-004)

---

### **Insight #4: My Recovery Logic Fixes Not Reaching Execution**

My `result_parser.py` changes ARE in the codebase, but:
- Parser can't extract `recovery_analysis` field from Mock LLM response
- Falls back to default construction logic
- Never reaches my `investigation_outcome="resolved"` check

---

## üöÄ RECOMMENDED FIX PRIORITY

### **Priority 1: Investigate Mock LLM Configuration (2 hours) - Fixes 11 tests**

**Action Items**:
1. Verify HAPI is configured to use Mock LLM endpoint
   ```bash
   kubectl get configmap holmesgpt-api-config -n holmesgpt-api-e2e -o yaml
   ```

2. Check Mock LLM logs for request receipt
   ```bash
   grep -E "test-happy-004|OOMKilled|Request received" /tmp/.../mock-llm/0.log
   ```

3. If Mock LLM not receiving requests:
   - Check HAPI `OPENAI_BASE_URL` environment variable
   - Check HAPI LLM client initialization logs
   - Fix ConfigMap or deployment to point to `http://mock-llm:8080/v1`

4. If Mock LLM receiving requests but returning wrong workflows:
   - Check scenario detection logic in `server.py` (line ~615-625)
   - Verify case-sensitivity: `"oomkilled" in content` vs `"OOMKilled"`
   - Add debug logging to scenario matcher

5. If Mock LLM response format is the issue:
   - Update Mock LLM to return JSON codeblock format
   - OR update HAPI parser to handle Python dict format
   - Ensure `recovery_analysis` field is properly structured

**Expected Impact**: ‚úÖ Fixes E2E-HAPI-002, 003, 004, 005, 013, 014, 023, 024, 026, 027 (10 tests)

---

### **Priority 2: Quick Win Already Achieved (Immediate) - Fixes 1 test**

**E2E-HAPI-001**: ‚úÖ FIXED - Changed `.Set` to `.Value == nil`

**Expected Impact**: ‚úÖ +1 test passing (27/40 = 68%)

---

### **Priority 3: Verify Remaining Unknowns (30 min) - Verify 0 tests**

**E2E-HAPI-007, 008, 018**: Confirmed V1.1 debt - no action

---

## üìä EXPECTED RESULTS AFTER FIXES

| Priority | Action | Tests Fixed | New Pass Rate |
|----------|--------|-------------|---------------|
| **Current** | - | - | 26/40 (65%) |
| **Priority 2** | E2E-HAPI-001 fix | +1 | 27/40 (68%) |
| **Priority 1** | Mock LLM fixes | +10 | 37/40 (93%) |
| **Priority 3** | V1.1 debt | 0 (defer) | 37/40 (93%) |

**Target**: 37/40 passing (93%) excluding V1.1 debt

---

## ‚úÖ VALIDATION CHECKLIST

After applying Priority 1 fixes:

**Mock LLM Configuration**:
- [ ] HAPI `OPENAI_BASE_URL` points to Mock LLM
- [ ] Mock LLM receives requests for test scenarios
- [ ] Mock LLM scenario detection works (case-insensitive)

**Mock LLM Response Structure**:
- [ ] `oomkilled` scenario returns `oomkill-increase-memory-v1`
- [ ] `low_confidence` scenario includes `alternative_workflows`
- [ ] `problem_resolved` scenario includes `investigation_outcome="resolved"`
- [ ] Response format parsable by HAPI (JSON codeblock or structured sections)

**Test Results**:
- [ ] E2E-HAPI-001: `selected_workflow.Value == nil` ‚úÖ
- [ ] E2E-HAPI-002: `alternative_workflows` not empty
- [ ] E2E-HAPI-004: Mock LLM returns correct workflow, `needs_human_review=false`
- [ ] E2E-HAPI-023: `can_recover=false` for problem resolved
- [ ] E2E-HAPI-024: No type mismatch error
- [ ] All workflow catalog tests pass (30-48)

---

## üîë NEXT ACTIONS

1. **Verify E2E-HAPI-001 fix works** (run single test)
2. **Check HAPI ConfigMap** for Mock LLM endpoint configuration
3. **Check Mock LLM logs** for request receipt and scenario matching
4. **Based on findings**:
   - If Mock LLM not configured: Fix ConfigMap
   - If scenario detection broken: Fix `server.py` matching logic
   - If response format issue: Update Mock LLM or HAPI parser

---

**Status**: Ready for Priority 1 investigation - Mock LLM configuration and scenario detection
