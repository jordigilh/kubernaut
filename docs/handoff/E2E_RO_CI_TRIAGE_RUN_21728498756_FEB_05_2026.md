# E2E (remediationorchestrator) CI Job Triage – Run 21728498756

**Date**: February 5, 2026  
**Workflow Run**: https://github.com/jordigilh/kubernaut/actions/runs/21728498756  
**Failing Job**: https://github.com/jordigilh/kubernaut/actions/runs/21728498756/job/62677186029  
**Branch/PR**: fix(ci): Coverage reporting issues in Test Suite Summary (#206)

---

## 1. Failure summary

| Item | Value |
|------|--------|
| **Job name** | E2E (remediationorchestrator) |
| **Failing step** | Run remediationorchestrator E2E tests |
| **Exit code** | 2 |
| **Duration** | 5m 37s (~337 s) |
| **Annotation** | Process completed with exit code 2 |

---

## 2. Exit code interpretation

- **0**: All tests passed.
- **1**: One or more tests failed (assertions).
- **2**: Interrupted run (e.g. suite/setup failure, timeout, or panic in Ginkgo).

Exit code **2** indicates the run was interrupted, not a normal assertion failure. Common causes:

- Failure or timeout in **BeforeSuite** (setup aborts the suite).
- **Ginkgo** or runner timeout (less likely here: 5m37s vs 30m suite timeout).
- **Panic** in setup or early in the suite.
- Process **killed** (OOM, runner limit).

---

## 3. Likely failure location

Evidence from code and previous handoffs:

### 3.1 BeforeSuite timeout (most likely)

- **Location**: `test/e2e/remediationorchestrator/suite_test.go` around **line 137**  
  `Expect(err).ToNot(HaveOccurred())` after `SetupROInfrastructureHybridWithCoverage(...)`.
- **Previous similar runs**: E2E_FAILURES_COMPLETE_ANALYSIS_FEB_04_2026.md reported “Generic timeout in BeforeSuite” at the same line, duration ~382 s (6.4 min). This run failed at ~337 s (5m 37s), so same class of failure, possibly earlier in the setup.
- **Implication**: `SetupROInfrastructureHybridWithCoverage` is returning an error (or timing out), which fails BeforeSuite and can produce exit code 2.

### 3.2 Where setup can fail (inside `SetupROInfrastructureHybridWithCoverage`)

1. **Phase 1 (image “build”)**  
   In CI, `IMAGE_REGISTRY` + `IMAGE_TAG` are set → no local build; registry image refs are returned. Unlikely to fail here.

2. **Phase 2**  
   Kind cluster create, CRD install, namespace create. Possible but less often the bottleneck.

3. **Phase 3**  
   Load images: in CI, LoadImageToKind **skips** load for registry images (Kind pulls on-demand). So no load step.

4. **Phase 4**  
   Deploy PostgreSQL, Redis, migrations, DataStorage, AuthWebhook, RemediationOrchestrator.  
   Then **wait for RO controller ready**: 3-minute loop (`remediationorchestrator_e2e_hybrid.go` ~652–661).  
   If the RO pod never becomes Ready (e.g. **ImagePullBackOff** from ghcr.io, **CrashLoopBackOff**, or dependency not ready), this returns:  
   `RemediationOrchestrator not ready within timeout` → setup fails → BeforeSuite fails → exit 2.

5. **After setup, in suite_test.go**  
   - `CreateE2EServiceAccountWithDataStorageAccess`  
   - `GetServiceAccountToken`  
   If token creation/retrieval fails or times out, the same `Expect(err).ToNot(HaveOccurred())` pattern would fail BeforeSuite and can yield exit 2.

### 3.3 CI-specific factors

- **Registry pull**: Images are pulled by Kind from ghcr.io when pods start. Slower or failed pulls (auth, network, tag) → pods not ready in time.
- **Runner load**: Shared runner load can slow pod startup and image pull, making a 3-minute RO wait tight.
- **Coverage PR**: This run is on a coverage-reporting PR; no E2E code change is required for the failure—likely environment/timing.

---

## 4. Recommended next steps

### 4.1 Get exact failure from logs (high priority)

From the failing job run 21728498756, job 62677186029:

1. Open the **“Run remediationorchestrator E2E tests”** step log.
2. Search for:
   - `❌` or `failed` or `Error`
   - `RemediationOrchestrator not ready within timeout`
   - `ImagePullBackOff` / `ErrImagePull`
   - `BeforeSuite` or panic stack
   - Last 50–100 lines before the step exits (to see which phase was running).

That will confirm whether the failure is RO not ready, image pull, SA/token, or something else.

### 4.2 Reproduce locally (optional)

```bash
# With registry (simulate CI)
IMAGE_REGISTRY=ghcr.io/$GITHUB_REPOSITORY_OWNER/kubernaut IMAGE_TAG=pr-206 make test-e2e-remediationorchestrator

# Without registry (full local build)
make test-e2e-remediationorchestrator
```

Compare behavior and timing. If it passes locally but fails in CI, treat as CI environment/timing.

### 4.3 Mitigations (if RO / image pull is confirmed)

- **Increase RO wait**: In `test/infrastructure/remediationorchestrator_e2e_hybrid.go`, consider increasing the controller wait from 3 minutes to 5 minutes (e.g. `deadline := time.Now().Add(5 * time.Minute)`) for CI only or globally.
- **E2E job timeout**: Job timeout is 40 min; no change needed unless logs show the step being killed by the runner.
- **Preserve cluster on failure**: Ensure must-gather/cleanup still runs so that on the next failure you get pod describe/logs (see E2E_FAILURES_COMPLETE_ANALYSIS_FEB_04_2026.md – “Must-Gather Collection BROKEN”); avoid deleting the cluster before artifact collection.

### 4.4 Unrelated to this job

- **Test Suite Summary** (coverage table, bash syntax): Addressed in the same PR (#206). This triage is only for the **E2E (remediationorchestrator)** job exit code 2.

---

## 5. References

- E2E_FAILURES_COMPLETE_ANALYSIS_FEB_04_2026.md – Failure #4: RO E2E BeforeSuite timeout at suite_test.go:137.  
- E2E_FAILURES_RUN_21684279480_FEB_04_2026.md – RO cleanup (podman rmi in CI) and audit EventType filter fixes.  
- `test/e2e/remediationorchestrator/suite_test.go` – BeforeSuite (lines 109–164).  
- `test/infrastructure/remediationorchestrator_e2e_hybrid.go` – SetupROInfrastructureHybridWithCoverage, waitForROControllerReady (3 min), waitForROServicesReady.

---

## 6. Summary

| Question | Answer |
|----------|--------|
| What failed? | Step “Run remediationorchestrator E2E tests” exited with code 2. |
| Where? | Most likely BeforeSuite: `SetupROInfrastructureHybridWithCoverage` or E2E SA/token, around suite_test.go:137. |
| Why exit 2? | Setup failed or was interrupted (timeout/panic), so Ginkgo did not complete normally (would be 1 for test failures). |
| Next action | Inspect the failing step’s log for the exact error message and phase; then consider increasing RO wait or improving diagnostics on failure. |
