# Data Storage Service - V5.6 Changelog

**Version**: 5.6
**Date**: 2025-11-08
**Purpose**: Add unified audit table implementation (ADR-034)
**Status**: â¸ï¸ Deferred to post-current-branch

---

## ğŸ“‹ Changes

### New Work: Day 21 - Unified Audit Table Implementation

**Context**: Approved ADR-034 for unified audit table design using industry-standard event sourcing pattern.

**Scope**: Implement unified audit infrastructure in Data Storage Service to support audit traces for all services (Gateway, Context API, AI Analysis, Workflow, Execution, etc.).

**Estimated Effort**: 20 hours

---

## ğŸ¯ Day 21: Unified Audit Table Implementation (20 hours)

### **Phase 1: Core Schema** (4 hours)

**Objective**: Create `audit_events` table with industry-standard event sourcing pattern

**Tasks**:
1. Create `audit_events` table schema
   - Structured columns (event_id, event_timestamp, event_type, etc.)
   - JSONB column for flexible event_data
   - Indexes (timestamp, correlation_id, resource, event_type, actor, outcome)
   - GIN index for JSONB queries

2. Create monthly partitions
   - Current month + next 3 months
   - Automated partition management script

3. Test schema with sample data
   - Insert sample events from all services
   - Verify indexes are used
   - Test query performance

**Deliverables**:
- [ ] `migrations/XXX_create_audit_events_table.up.sql`
- [ ] `migrations/XXX_create_audit_events_table.down.sql`
- [ ] `scripts/create_audit_partitions.sh`
- [ ] Sample data test script

**Success Criteria**:
- âœ… Table created with all columns and indexes
- âœ… Partitions created for current + 3 months
- âœ… Sample data inserts successfully
- âœ… Query performance <100ms for correlation_id queries

---

### **Phase 2: Event Data Format & Helpers** (4 hours)

**Objective**: Implement hybrid event_data format (common envelope + service-specific payload)

**Tasks**:
1. Define common envelope structure
   - `version`, `service`, `operation`, `status`
   - Optional: `attempt`, `retry_reason`, `source_payload`

2. Implement helper functions
   - `NewEventData()` - Create event with common envelope
   - `WithSourcePayload()` - Add original signal
   - `WithRetry()` - Add retry information
   - `ToJSON()` - Serialize to JSONB
   - `FromJSON()` - Deserialize from JSONB

3. Create per-service payload schemas documentation
   - Gateway payload schema
   - Context API payload schema
   - AI Analysis payload schema
   - Workflow payload schema
   - Data Storage payload schema
   - Execution payload schema

4. JSON schema validation (optional but recommended)
   - Common envelope JSON schema
   - Validation middleware

**Deliverables**:
- [ ] `pkg/datastorage/audit/event_data.go`
- [ ] `pkg/datastorage/audit/event_data_test.go`
- [ ] `docs/services/stateless/data-storage/schemas/event_data/common_envelope.md`
- [ ] `docs/services/stateless/data-storage/schemas/event_data/gateway_payload.md`
- [ ] `docs/services/stateless/data-storage/schemas/event_data/context_api_payload.md`
- [ ] `docs/services/stateless/data-storage/schemas/event_data/ai_analysis_payload.md`
- [ ] `docs/services/stateless/data-storage/schemas/event_data/workflow_payload.md`
- [ ] `docs/services/stateless/data-storage/schemas/event_data/data_storage_payload.md`
- [ ] `docs/services/stateless/data-storage/schemas/event_data/execution_payload.md`

**Success Criteria**:
- âœ… Helper functions create valid JSONB
- âœ… All per-service schemas documented
- âœ… Unit tests pass (100% coverage)

---

### **Phase 3: Signal Source Adapters** (6 hours)

**Objective**: Implement adapters for heterogeneous signal sources

**Tasks**:
1. Generic signal handler
   - Accepts any JSON payload
   - Routes to appropriate adapter
   - Falls back to pass-through for unknown sources

2. K8s Prometheus adapter
   - Parse Prometheus AlertManager webhook
   - Extract alert_name, fingerprint, labels, annotations
   - Create standardized event_data

3. AWS CloudWatch adapter
   - Parse AWS CloudWatch alarm webhook
   - Extract alarm_name, metric_name, dimensions
   - Create standardized event_data

4. GCP Monitoring adapter
   - Parse GCP Monitoring alert webhook
   - Extract alert_policy_name, resource_type, metric
   - Create standardized event_data

5. Custom webhook adapter (pass-through)
   - Accept any JSON structure
   - Store as-is in source_payload
   - Minimal processing in payload

**Deliverables**:
- [ ] `pkg/datastorage/audit/signal_handler.go`
- [ ] `pkg/datastorage/audit/adapters/prometheus.go`
- [ ] `pkg/datastorage/audit/adapters/aws_cloudwatch.go`
- [ ] `pkg/datastorage/audit/adapters/gcp_monitoring.go`
- [ ] `pkg/datastorage/audit/adapters/custom_webhook.go`
- [ ] `test/unit/datastorage/audit/signal_handler_test.go`
- [ ] `test/unit/datastorage/audit/adapters/prometheus_test.go`
- [ ] `test/unit/datastorage/audit/adapters/aws_cloudwatch_test.go`
- [ ] `test/unit/datastorage/audit/adapters/gcp_monitoring_test.go`
- [ ] `test/unit/datastorage/audit/adapters/custom_webhook_test.go`

**Success Criteria**:
- âœ… All adapters parse signals correctly
- âœ… Event_data follows common envelope format
- âœ… Unit tests pass (100% coverage)
- âœ… Unknown sources fall back to pass-through

---

### **Phase 4: Query API** (4 hours)

**Objective**: Implement REST API for audit queries

**Tasks**:
1. Query by correlation_id (remediation timeline)
   - `GET /api/v1/audit/events?correlation_id=rr-2025-001`
   - Returns all events for a remediation in chronological order

2. Query by event_type (filter by signal source)
   - `GET /api/v1/audit/events?event_type=gateway.signal.received`
   - Returns all events of a specific type

3. Query by time range + filters
   - `GET /api/v1/audit/events?since=24h&actor_id=gateway&outcome=failure`
   - Supports multiple filters (actor_id, outcome, severity, etc.)

4. Pagination support
   - Limit/offset pagination
   - Cursor-based pagination for large result sets

**Deliverables**:
- [ ] `pkg/datastorage/audit/query_api.go`
- [ ] `pkg/datastorage/audit/query_api_test.go`
- [ ] `test/integration/datastorage/audit_query_api_test.go`
- [ ] API documentation

**Success Criteria**:
- âœ… All query endpoints return correct results
- âœ… Query performance <100ms for correlation_id
- âœ… Query performance <500ms for JSONB path queries
- âœ… Pagination works correctly
- âœ… Integration tests pass

---

### **Phase 5: Observability & Testing** (2 hours)

**Objective**: Add metrics, dashboards, and comprehensive testing

**Tasks**:
1. Prometheus metrics
   - `audit_events_total` (by service, operation, outcome)
   - `audit_write_duration_seconds` (histogram)
   - `audit_query_duration_seconds` (histogram)

2. Grafana dashboard
   - Event volume by service
   - Success rate by service
   - Top 10 event types
   - Query performance

3. Alerting rules
   - Audit write failures
   - High error rate (>5%)
   - Slow queries (>1s)

4. E2E tests
   - Full signal ingestion to query flow
   - Multi-service correlation
   - Performance test (1000 events/sec)

**Deliverables**:
- [ ] `pkg/datastorage/audit/metrics.go`
- [ ] `deploy/grafana/dashboards/audit_events.json`
- [ ] `deploy/prometheus/alerts/audit_events.yaml`
- [ ] `test/e2e/datastorage/audit_e2e_test.go`
- [ ] `test/e2e/datastorage/audit_performance_test.go`

**Success Criteria**:
- âœ… Metrics exposed and scraped by Prometheus
- âœ… Dashboard displays correctly in Grafana
- âœ… Alerts fire on error conditions
- âœ… E2E tests pass
- âœ… Performance test achieves 1000 events/sec

---

## ğŸ“Š Expected Outcomes

### Before Day 21
- **Audit Infrastructure**: None (services use per-service audit tables)
- **Cross-Service Correlation**: Manual (no unified view)
- **Signal Source Support**: K8s only
- **Query Flexibility**: Limited (per-service schemas)
- **Compliance Readiness**: Partial (no unified audit trail)

### After Day 21
- **Audit Infrastructure**: âœ… Unified audit table (event sourcing pattern)
- **Cross-Service Correlation**: âœ… Automatic (correlation_id)
- **Signal Source Support**: âœ… K8s, AWS, GCP, Azure, OpenTelemetry, custom
- **Query Flexibility**: âœ… SQL + JSONB queries (structured + flexible)
- **Compliance Readiness**: âœ… SOC 2, ISO 27001, GDPR ready

---

## ğŸ¯ Success Criteria

### Functional Requirements
- [ ] Audit events table created with partitions
- [ ] Event_data format follows common envelope pattern
- [ ] Signal source adapters handle K8s, AWS, GCP, custom webhooks
- [ ] Query API returns correct results for all query types
- [ ] Pagination works correctly

### Non-Functional Requirements
- [ ] Write performance: 1000 events/sec
- [ ] Query performance: <100ms for correlation_id queries
- [ ] Query performance: <500ms for JSONB path queries
- [ ] Storage: ~1KB per event (compressed: ~300 bytes)
- [ ] Retention: 90 days to 7 years (configurable)

### Testing Requirements
- [ ] Unit tests: 100% coverage for helpers and adapters
- [ ] Integration tests: PostgreSQL roundtrip, JSONB queries
- [ ] E2E tests: Full signal ingestion to query flow
- [ ] Performance tests: 1000 events/sec sustained write throughput

### Documentation Requirements
- [ ] ADR-034 created and approved
- [ ] Per-service payload schemas documented
- [ ] API documentation complete
- [ ] Migration guide for existing services

---

## ğŸ”— Related Work

### Gateway Service Integration (Day 22, 6 hours)
**Scope**: Implement audit traces for Gateway operations

**Tasks**:
1. Define audit events (signal_received, deduplicated, storm_detected, etc.)
2. Implement audit functions
3. Integration with Data Storage Service audit API
4. Testing (unit, integration, E2E)

**Deliverables**:
- [ ] `pkg/gateway/audit/audit.go`
- [ ] `pkg/gateway/audit/audit_test.go`
- [ ] `test/integration/gateway/audit_integration_test.go`
- [ ] `test/e2e/gateway/audit_e2e_test.go`

**See**: Gateway implementation plan update (separate task)

---

## ğŸ“‹ Rationale for Deferral

**Why Defer to Post-Current-Branch**:
1. âœ… **Current branch focus**: BR documentation for Data Storage, AI/ML, Workflow services
2. âœ… **Non-blocking**: Existing per-service audit tables continue to work
3. âœ… **Foundation work**: Requires Data Storage Service to be production-ready first
4. âœ… **Cross-service coordination**: Gateway, Context API, AI Analysis all need updates
5. âœ… **Testing complexity**: Requires comprehensive E2E testing across services

**Implementation Timeline**:
- **Current Branch**: Complete BR documentation (Data Storage, AI/ML, Workflow)
- **Next Branch**: Implement unified audit table (Day 21 + Day 22)
- **Estimated Start**: After current branch merge

---

## ğŸ“š References

- **ADR-034**: Unified Audit Table Design with Event Sourcing Pattern
- **Analysis Documents**:
  - `GATEWAY_AUDIT_VS_LOGGING_ANALYSIS.md` (95% confidence)
  - `INDUSTRY_STANDARD_AUDIT_TABLE_DESIGN.md` (90% confidence)
  - `KUBERNAUT_EVENT_DATA_FORMAT_DESIGN.md` (95% confidence)
  - `EXTENSIBILITY_VALIDATION_NEW_SERVICES.md` (98% confidence)
  - `INDUSTRY_BEST_PRACTICES_AUDIT_STORAGE.md` (95% confidence)

---

**Status**: â¸ï¸ **DEFERRED TO POST-CURRENT-BRANCH**
**Priority**: High (foundational for compliance and observability)
**Estimated Effort**: 20 hours (Data Storage) + 6 hours (Gateway) = 26 hours total

